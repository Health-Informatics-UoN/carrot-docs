{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Important Notice</p> <p>This site may contain outdated information about Carrot.</p> <p>The official documentation site of Carrot Mapper and Carrot Transform is now: https://carrot.ac.uk/documentation. Contributions are welcomed at https://github.com/Health-Informatics-UoN/carrot.</p>"},{"location":"standards/","title":"Data Standards","text":""},{"location":"standards/#converting-to-csv-files","title":"Converting to CSV files","text":"<p>Before using White Rabbit to generate a Scan Report File, data must be converted to a set of CSV files.  This may require some restructuring of the dataset.</p> <p>The CSV files must fulfil the following:</p> <ul> <li>One CSV file that represents the demographic data of individuals. This should contain the anonymised ID for the individual (following the CO-CONNECT specification), Sex and Date of Birth (obfuscated as required) as a minimum, and Ethnicity where possible. In cases of multiple demographic records per individual, please include only the most recent record in the demographics file.</li> <li>One or more CSV files that contain event data, such as questionnaire responses or clinical events/measurements. As a minimum each CSV file must have a column that contains the anonymised ID for the individual (matching an entry in the CSV file above), the date of a particular event (one per file) and one or more columns that capture the data relevant to that date. You can have as many CSV files as needed, but each CSV file must only contain information effective on the specified event date.</li> <li>All measurements in the metric system. </li> <li>All dates and datetimes in the ISO-8601 format: YYYY-MM-DD HH:MM:SS.ffffff</li> <li>All numbers without any digit grouping symbols (e.g. 1000, not 1,000). </li> <li>All decimal numbers rounded to two decimal places after the decimal symbol \u201c.\u201d. </li> <li>All CSV files encoded with UTF-8 and Unix/Linux line ending. </li> <li>All CSV file names limited to 30 characters.</li> </ul>"},{"location":"standards/#using-white-rabbit","title":"Using White Rabbit","text":""},{"location":"standards/#minimum-count-threshold","title":"Minimum count threshold","text":"<p>WhiteRabbit contains an option to automatically remove values with frequency values under a certain threshold,  for instance if a dataset contains a field called \u201ccondition\u201d, and there are only 2 patients with a very rare condition,  this can cause security concerns.  Consider whether you should set the \u201cMinimum Cell Count\u201d threshold in WhiteRabbit to avoid identifying information being included in the Scan Report File.</p>"},{"location":"standards/#reviewing-scan-report-file-contents","title":"Reviewing Scan Report File contents","text":"<p>Warning</p> <p>Ensure all Scan Report Files are thoroughly reviewed for any identifying information before extracting them from their secure location.</p> <p>Please remove any data values which could be deemed as confidential or sensitive from the scan report,  such as the anonymised ID of the individual, date of birth, and their frequencies. </p> <p>When removing these values and frequencies from the Scan Report, please refrain from removing the entire  column from the spreadsheet. For instance, please maintain the headers and remove the values, as in the example below: </p>"},{"location":"standards/#ids-and-date-values","title":"IDs and date values","text":"<p>Tip</p> <p>Note that in almost all scenarios, patient ID (pseudonymised or other) and date values are not required to be included in the Scan Report File, since they will not be mapped to an OMOP concept. These IDs and dates will still be transformed appropriately at the ETL stage without their inclusion in the Scan  Report File, so long as their columns are still present as above, and so we advise the column values are removed.  This will also speed up the processing of the Scan Report.</p>"},{"location":"standards/#data-dictionary-file","title":"Data Dictionary File","text":"<p>The optionally-supplied Data Dictionary can be used to supply </p> <ol> <li>descriptions of values</li> <li>vocabulary labelling for fields</li> </ol> <p>Please provide this information in one CSV file, with the following columns:  <code>csv_file_name</code>, <code>field_name</code>, <code>code</code>, and <code>value</code>.  Here is an example to demonstrate the expected format. </p> Table viewCSV view csv_file_name field_name code value questionnaire gender 0 Male questionnaire gender 1 Female results Q1 0 Fever results Q1 1 Cough results Q1 2 Wheeze results Q2 0 Yes results Q2 1 No results Q3 SNOMED results Q4 LOINC <pre><code>csv_file_name,field_name,code,value\nquestionnaire,gender,0,Male\nquestionnaire,gender,1,Female\nresults,Q1,0,Fever\nresults,Q1,1,Cough\nresults,Q1,2,Wheeze\nresults,Q2,0,Yes\nresults,Q2,1,No\nresults,Q3,SNOMED,\nresults,Q4,LOINC,\n</code></pre> <p>Tip</p> <p>The CSV file must have 4 columns.   Rows that indicate a vocabulary must still contain an empty column at the end of the row in the csv file. In the above, the last two rows contain a comma after the vocabulary name to achieve this.</p> <p>This will instruct Carrot-Mapper to perform the following:</p> <ol> <li>Rows 1-7 will add Value Descriptions to the Carrot-Mapper interface at the respective locations. For example, the first row will ensure the text \"Male\" is displayed next to the value \"0\" in the \"gender\" field in the  \"questionnaire table\". This means a human user of the interface can quickly identify the meaning of the value \"0\", rather than having to look up the value from the data dictionary themselves. Note that this mechanism does not perform any automated mapping, but is a helpful feature for a human user.</li> <li>Rows 8 and 9 indicate that the 2 fields \"Q3\" and \"Q4\" in the \"results\" table should be automatically mapped from the supplied vocabulary to the OMOP standard. Any values in this field that Carrot-Mapper recognises as valid  codes from the supplied vocabulary will be automatically converted to OMOP.</li> </ol>"},{"location":"AzureFunctions/NlpQueue/","title":"NlpQueue","text":"<p>Note</p> <p>The NLP Processing feature is not currently supported, and may be deprecated in future.</p>"},{"location":"AzureFunctions/NlpQueue/#introduction","title":"Introduction","text":"<p>'NLP Processing' is the general term for converting Field/Value text strings from Scan Reports to standard and valid OMOP CDM conceptIDs using the Microsoft Health API. For example:</p> <p>\"The patient has a cough\" </p> <p>can--via several intermediate stages detailed below--be converted to the valid and standard conceptID of 254761. The NLP service works by picking out what it thinks is the most pertinent information in a string (in this case, the symptom 'cough') and then returning concept codes for the various vocabularies in its database. We then convert concept codes to conceptIDs by looking them up with our deployed version of the OMOP CDM and save the results to the model <code>ScanReportConcept</code>.</p> <p>This page is not intended to be a highly detailed dive into the exact workings of the code. Rather, it is meant to provide a reasonably detailed account of the overall NLP process. Reference is made to key functionality within the code base which can be read if more detail is required.</p> <p>We are currently using Microsoft Health text API version 3.1 Preview 3.</p>"},{"location":"AzureFunctions/NlpQueue/#workflow","title":"Workflow","text":"<p>When a user clicks the 'Run NLP' button, the code performs a number of checks to determine what text string data to send to the NLP service. The final decision on what text to send for processing is determined by a combination of user input and what field/value data is available for analysis. The general rule is: if field/value metadata is available which describes the field/value in detail, then preferentially use the field/value descriptions over field/value names. This is because the NLP service stands a better chance of finding a match if it has more information to work with (up to a point).</p> <p> Figure 1 Decision tree for sending text data to the NLP Service</p>"},{"location":"AzureFunctions/NlpQueue/#decision-tree-workflow","title":"Decision Tree Workflow","text":"<p>After a user clicks the 'Run NLP' button:</p> <ol> <li> <p>Check whether the field is 'Pass From Source'. This is a user-defined flag set at the field level. If 'Pass from source' is set to 'Yes' then the code only processes data at the field level. In other words, we do not process any values associated with that field. Examples of fields which are typically set to 'pass from source' are continuous variables such as age, height and weight. We do not typically map individual heights, so there is no need to send n heights to the NLP service for analysis; we are only interested in mapping the field itself (in this example, 'height').</p> </li> <li> <p>If 'pass from source' is True, we check whether meta data is available for a given field. Here, meta data is defined as supplementary data describing the field. For some fields, the field name itself will be sufficient to attempt mapping with NLP (e.g. Field name of 'Was the patient administered ibuprofen'). However, some datasets have coded fields and the field name itself contains no useful information for NLP to work with. For example, a field name could be 'AH12345'. Such a field name can only be processed if there is some accompanying text describing the field. If metadata is available, we send the field description. If meta data is unavailable, we send the field name.</p> </li> </ol> <p>By the end of the decision tree, there are two different combinations for fields:</p> <ul> <li>Field description</li> <li>Field name</li> </ul> <p>If processing a value (when 'Pass From Source' is set to 'No'):</p> <ol> <li>Check whether the value is a 'negative assertion'. Negative assertions are currently set at the level of the scan report and a user must tell the system what values in the scan report are negative values. For example, in Dataset 1, negative values may be 'N' and '0' whereas in Dataset 2, negative values may be 'No' and 'Negative'. At the point, the code skips the value if it has been assigned a negative assertion. If the value is positive then:</li> <li>Check whether a field description is available for that value. If so, use the field description. If not, use the field name.</li> <li>Check whether a value description is available for that value. If so, use the value descirption. If not, use the value name. Concatenate the text strings from steps 4 and 5 for the final input into NLP.</li> </ol> <p>By the end of the decision tree, there are four different combinations for values:</p> <ul> <li>Field description + value description</li> <li>Field name + value name</li> <li>Field description + value name</li> <li>Field name + value description</li> </ul>"},{"location":"AzureFunctions/NlpQueue/#sendingrecieving-data-tofrom-the-nlp-service","title":"Sending/recieving data to/from the NLP service","text":"<p>After creating the various combinations of field/value names and descriptions, the data are sent as JSON to the NLP service using a <code>POST</code> request via the <code>requests</code> library. During testing, the time taken for the NLP service to process requests has varied substantially. Sometimes it can be a few seconds, other times it can take 30-40 seconds to process a single query (perhaps a function of the NLP service still being in preview.)</p> <p>When attempting to <code>GET</code> data back from the NLP service, the code contains a short <code>while</code> loop (in the function <code>get_data_from_nlp</code>). This is to give the API time to receive the job, queue it and process it. Only when the job status is 'complete' can a successful <code>GET</code> request take place. Data are returned as JSON. If successful, results are appended to a list for later conversion to conceptIDs.</p> <p>Official documentation for the NLP API can be found here.</p>"},{"location":"AzureFunctions/NlpQueue/#processing-the-return-from-nlp","title":"Processing the return from NLP","text":"<p>The functions <code>process_nlp_response</code> and <code>concept_code_to_id</code> (both in <code>services_nlp.py</code>) are used to drill down into the returned JSON, pick out the required data, save it to a list and then look up the conceptID from the concept code. At present, we return concept code data from the following vocabularies: ICD9CM, ICD10CM, RXNORM and SNOMEDCT_US. Note that the NLP service has slightly different names/cases for some vocabularies compared to our working version of the OMOP CDM. For example: NLP = SNOWMEDCT_US/OMOP = SNOMED, NLP = RXNORM/OMOP = RxNorm. These subtle differences are taken into consideration in <code>get_concept_from_concept_code</code> in <code>services.py</code> where we convert the vocabulary names for a successful concept code lookup.</p> <p>Because we return data from 4 different vocabularies, it is possible to get four different concept codes for the same input string. For example, the input string \"The patient has a fever\" could return 4 subtly different concepts (e.g. SNOMED = \"fever\", ICD9 = \"temperature\", ICD10 = \"high fever\", RXNORM = \"mild fever\"). In such cases, the code saves all 4 concepts back to <code>ScanReportConcept</code> - the user must then delete the unwanted concepts. However, where all vocaublaries converge to the same standard and valid conceptID, the app returns only the SNOMED valid and standard conceptID. This functionality is designed to save user input when all concept codes converge to the same conceptID.</p>"},{"location":"AzureFunctions/Rules/","title":"Rules","text":""},{"location":"AzureFunctions/Rules/#introduction","title":"Introduction","text":"<p>The Rules Azure Functions refers to a series of Durable functions, that are reponsible for generating the Rules for a given Scan Report Table.</p> Function Type Purpose RulesTrigger HttpTrigger Consumes the HTTP request from the web app, adds to the queue. RulesQueue Queue Reads the message from the queue, passes to the Orchestrator. RulesOrchestrator Orchestrator Orchestrates the activity, generating Concepts, and parallelising Rules Generation. RulesConceptsActivity Activity Generates the Scan Report Concepts. RulesGenerationActivity Activity Generates the Scan Report Rules."},{"location":"AzureFunctions/Rules/#converting-concept-codes-to-conceptids","title":"Converting Concept Codes to ConceptIDs","text":""},{"location":"AzureFunctions/Rules/#introduction_1","title":"Introduction","text":"<p>Some scan reports (most likely from the larger data providers) will be supplied with concept codes as values. In effect the mapping has already been done for us-- we just need to make sure we return valid and standard OMOP conceptIDs. In instances where concept codes are provided, they will be coded within a given vocabulary e.g. SNOMED or RxNorm. The ProcessQueue Azure function has therefore been updated with the ability to look up concept codes and convert them to standard and valid OMOP codes. This removes the requirement from the data team of manually looking up concept codes and finding the relevant conceptID.</p> <p>The Data Team have specified a list of vocabularies that are most likely to be used. </p>"},{"location":"AzureFunctions/Rules/#operation","title":"Operation","text":"<p>The concept code to conceptID lookup occurs as each scan report value is processed in <code>__init__.py</code>. Immediately after a value description is looked up, the code checks to see whether the dictionary contains any information on a vocabulary coding system. A dictionary may look something like:</p> csv_file_name field_name code value questionnaire sex 0 male survey question1 SNOMED <p>In this example, all of the values within the field <code>question1</code> will be encoded as SNOMED concept codes. For example, the question might be: \"Main symptom\" and the answer of 'cough' would be encoded as \"49727002\" under the SNOMED vocabulary.</p> <p>For a given value, we determine whether it falls within a field defined as a vocabulary field with:</p> <p><pre><code>code = next(\n    (\n        row[\"code\"]\n        for row in data_dictionary\n        if str(row[\"field_name\"]) == str(name)\n    ),\n    None,\n)\n</code></pre> If the value of <code>code</code> is within our list of vocabularies, we use <code>omop_helpers.get_concept_from_concept_code()</code> to look up the standard and valid OMOP conceptID with the following:</p> <pre><code># If 'code' is in our vocab list, try and convert the ScanReportValue (concept code) to conceptID\n# If there's a faulty concept code for the vocab, fail gracefully and set concept_id to default (-1)\nif code in vocabs:\n    try:\n        concept_id = omop_helpers.get_concept_from_concept_code(\n            concept_code=value,\n            vocabulary_id=code,\n            no_source_concept=True,\n        )\n        concept_id = concept_id[\"concept_id\"]\n    except:\n        concept_id = -1\nelse:\n    concept_id = -1\n</code></pre> <p>Where <code>value</code> is the scan report value (i.e. the concept code) in that loop's iteration and <code>code</code> is the name of the vocabulary from the data dictionary's 'code' column (see here for a standard data dictionary template.) If the supplied scan report value (concept code) is incorrect for any reason, we will simply default to setting <code>concept_id</code> to the default of -1.</p> <p>Here is an example if you wanted to manually look up a concept code:</p> <p><pre><code>concept_id = omop_helpers.get_concept_from_concept_code(\n                                concept_code=49727002,\n                                vocabulary_id=\"SNOMED,\n                                no_source_concept=True,\n                            )\n</code></pre> The argument <code>no_source_concept</code> tells the function to not return the non-standard conceptID. It will only return the standard and valid conceptID when this is set to <code>True</code>.</p> <p>With a conceptID in hand, we construct a dictionary called <code>scan_report_value_entry</code> with all the necessary details and append it to a list for a single <code>POST</code> to the database:</p> <pre><code>data = []\n\nscan_report_value_entry = {\n    \"created_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n    \"updated_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n    \"value\": value,\n    \"frequency\": int(frequency),\n    \"conceptID\": concept_id,\n    \"value_description\": val_desc,\n    \"scan_report_field\": names_x_ids[name],\n}\n\ndata.append(scan_report_value_entry)\n</code></pre>"},{"location":"AzureFunctions/Rules/#updating-scanreportconcept","title":"Updating ScanReportConcept","text":"<p>By this point, we have not associated conceptIDs with scan report values using the <code>ScanReportConcept</code> model. In effect, any conceptID information from the scan report is--at present--held in the model <code>ScanReportValue</code>. We therefore need to 'move' the conceptID data from the <code>conceptID</code> field and create a record for it in <code>ScanReportConcept</code>. </p> <p>Important</p> <p>The reason we have to do this is--prior to POSTing scan report values to <code>ScanReportValue</code>--we have no primary keys for any scan report values. Without the PKs, we can't establish a relationship between <code>ScanReportValue</code> and <code>ScanReportConcept</code>. After the <code>POST</code> request we have the PKs required to associate a <code>ScanReportConcept</code> to a <code>ScanReportValue</code>.</p> <p>After POSTing the scan report values to the database, we must then return all <code>ScanReportValues</code> where the conceptID != -1. In other words, we want to return all records where we performed a concept code -&gt; conceptID operation. </p> <p>We created a custom <code>ViewSet</code> called <code>ScanReportValuePKViewSet</code> which takes a Scan Report ID and returns all scan report values where there is a conceptID in the <code>conceptID</code> field. We then iterate over these model objects and create legitimate concept entries in <code>ScanReportConcept</code></p>"},{"location":"AzureFunctions/RunningLocally/","title":"RunningLocally","text":"<p>Note</p> <p>This page is for developers of Azure Functions - not users</p>"},{"location":"AzureFunctions/RunningLocally/#introduction","title":"Introduction","text":"<p>Running Azure Functions locally is a little involved; this guide has been written to get you up and running  with Azure CLI and Functions. Note that this guide assumes you're using VSCode. All of the functionality  described here can be replicated outside of VSCode (in Azure CLI) but instructions for doing so are not  detailed in this guide.</p> <p>Note that this guide is not intended to demonstrate how to create an Azure Function from scratch.  Its purpose is to show you how to run pre-coded Functions.</p> <p>To follow this guide you need to install Azure CLI.</p>"},{"location":"AzureFunctions/RunningLocally/#azure-functions-basics","title":"Azure Functions Basics","text":"<p>Conceptually, queue-based Azure Functions are simple to follow. A message is posted to a message queue.  A Function then executes on messages within a message queue. In Carrot-Mapper, a message is posted to a message  queue as JSON but other formats are possible (e.g. XML). Here's an example message destined for the <code>scanreports</code> queue:</p> <pre><code>{\n  \"scan_report_id\": 105, \n  \"blob_name\": \"GOSH_CoStar_WhiteRabbit_MetaData_V24_T29kygl.xlsx\"\n}\n</code></pre> <p>And here is an example message destined for the <code>nlpqueue</code> queue:</p> <pre><code>{\n  \"documents\": \n  [{\n    \"language\": \"en\", \n    \"id\": \"17730_field\", \n    \"text\": \"The patient has a fever\"\n  }]\n}\n</code></pre> <p>An Azure function is made up of three key files. They are kept in the function's directory (<code>app/workers/...</code>) and define what and how the function should run:</p> <ol> <li><code>init.py</code> - This contains the function's logic. The method <code>main()</code> should be present for the function to execute.</li> <li><code>function.json</code> - This tells Azure what type of function you're developing and also what queue from which to pick messages up from.</li> </ol> <p>Another file is kept outside of the function's directory and should not be commited to Git:</p> <ol> <li><code>local.settings.json</code> - Contains environment variables for your function. It must be present to run Azure functions locally. Contact a member of the development team to get all keys and connection strings for this file. For reference, a local.settings.json file looks like this:</li> </ol> <p><pre><code>{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"AzureWebJobsStorage\": \"REDACTED,\n    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n    \"STORAGE_CONN_STRING\": \"REDACTED\",\n    \"NLP_API_KEY\": \"REDACTED\",\n    \"APP_URL\": \"REDACTED\",\n    \"AZ_FUNCTION_KEY\": \"REDACTED\",\n    \"SCAN_REPORT_QUEUE_NAME\": \"scanreports-local\",\n    \"NLP_QUEUE_NAME\": \"nlpqueue-local\"\n\n  }\n}\n</code></pre> You only need to have 1 <code>local.settings.json</code> file for all functions within the Carrot-Mapper project. Note,  however, that it should be updated with extra queue names if/when the project requires them.</p>"},{"location":"AzureFunctions/RunningLocally/#azure-functions-in-carrot-mapper","title":"Azure Functions in Carrot-Mapper","text":"<p>The Carrot-Mapper project has several different Azure Functions: one for processing scan reports (called  <code>ProcessQueue</code> which posts to the message queue called <code>scanreports</code>), another to run the NLP service  (called <code>NLPQueue</code> which posts to the message queue called <code>nlpqueue</code>), and one to generate mapping rules for a scan report. The code for these functions lives  in the <code>/workers</code> directory.</p>"},{"location":"AzureFunctions/RunningLocally/#azure-functions-queues","title":"Azure Functions Queues","text":"<p>For each process (NLP and Scan Reports), we have two queues each - a 'local' queue and a 'live' queue  (for a total of 4 unique queues.) As environment variables, they are encoded as:</p>"},{"location":"AzureFunctions/RunningLocally/#local-queues","title":"Local queues","text":"<pre><code>NLP_QUEUE_NAME=nlpqueue-local\nSCAN_REPORT_QUEUE_NAME=scanreports-local\n</code></pre>"},{"location":"AzureFunctions/RunningLocally/#deployed-queues","title":"Deployed queues","text":"<pre><code>NLP_QUEUE_NAME=nlpqueue\nSCAN_REPORT_QUEUE_NAME=scanreports\n</code></pre> <p>The purpose of the local queues is that, when developing locally, messages are sent only to a 'local' queue. </p> <p>These environment variables which control where messages are sent must be maintained in the following locations:</p> <ol> <li><code>local.settings.json</code> - A local file to hold Azure Function environment variables. Should point to 'local'  (e.g. <code>NLP_QUEUE_NAME=nlpqueue-local</code>). </li> <li>Within the Carrot webapp, the <code>.env</code> file must include variables for 'local' on your local machine and set to  'live' variables on App Service.</li> </ol>"},{"location":"AzureFunctions/RunningLocally/#running-azure-functions-locally","title":"Running Azure Functions Locally","text":"<p>We use Azurite to enable running the functions locally, emulating the Azure Function services.</p> <p>To start debugging locally in VSCode you must first ensure that Carrot is up and running locally  (see here for building and running the Carrot-Mapper Docker image ).  Once your local server is running, you can start Azure Function's debugging mode by clicking: </p> <p>Run (top toolbar) -&gt; Start Debugging</p> <p>After a few seconds, you'll see something like the following printed to the debugging terminal that's  started when you run debugging:</p> <p><pre><code>&gt; Executing task: . .venv/bin/activate &amp;&amp; func host start &lt;\n\nFound Python version 3.9.5 (python3).\n\nAzure Functions Core Tools\nCore Tools Version:       3.0.3477 Commit hash: 5fbb9a76fc00e4168f2cc90d6ff0afe5373afc6d  (64-bit)\nFunction Runtime Version: 3.0.15584.0\n\n[2021-07-02T09:34:28.929Z] Cannot create directory for shared memory usage: /dev/shm/AzureFunctions\n[2021-07-02T09:34:28.929Z] System.IO.FileSystem: Access to the path '/dev/shm/AzureFunctions' is denied. Operation not permitted.\n\nFunctions:\n\n        NLPQueue: queueTrigger\n        ProcessQueue: queueTrigger\n\nFor detailed output, run func with --verbose flag.\n[2021-07-02T09:34:32.711Z] Traceback (most recent call last):\n[2021-07-02T09:34:32.711Z]   File \"/usr/local/Cellar/azure-functions-core-tools@3/3.0.3477/workers/python/3.9/OSX/X64/azure_functions_worker/bindings/shared_memory_data_transfer/file_accessor_unix.py\", line 127, in _get_valid_mem_map_dirs\n[2021-07-02T09:34:32.711Z]     os.makedirs(dir_path)\n[2021-07-02T09:34:32.711Z]   File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 215, in makedirs\n[2021-07-02T09:34:32.711Z]     makedirs(head, exist_ok=exist_ok)\n[2021-07-02T09:34:32.711Z]   File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 225, in makedirs\n[2021-07-02T09:34:32.711Z]     mkdir(name, mode)\n[2021-07-02T09:34:32.711Z] PermissionError: [Errno 1] Operation not permitted: '/dev/shm'\n[2021-07-02T09:34:32.812Z] Worker process started and initialized.\n[2021-07-02T09:34:36.731Z] Host lock lease acquired by instance ID '000000000000000000000000DC4198B8'.\n</code></pre> You'll likely see a few errors/warnings about directories and permissions. At the time of writing this guide, it's safe to ignore these! Note that two functions are listed as you start debugging mode: <code>NLPQueue</code> and <code>ProcessQueue</code>.  These are the names of the directories which hold the function's code, not the name of the  message queue which holds the function's messages. The text after the function name (queueTrigger)  specifies the type of function it is (defined in <code>function.json</code>). For more information on Function types, click here.</p> <p>With the final console output saying 'Host lock lease acquired', you're ready to run your first Azure Functions job!</p>"},{"location":"AzureFunctions/RunningLocally/#running-an-nlp-task","title":"Running an NLP task","text":"<p>The remainder of this guide will show what to expect to see in the debugging console if you run NLP at the field level.</p> <ol> <li>Navigate to a field within a scan report.</li> <li>Ensure the field you've chosen has a meaningful description (e.g. \"Patient has a cold\"). If it doesn't have one,  click 'Edit Field' and manually put one in.</li> <li>Ensure that <code>pass_from_source</code> is set to <code>True</code> within the field's settings. This will cause CCOM to process the field only.</li> <li>Click 'Run NLP'</li> <li>After a short wait a green message banner will appear to say NLP is now running.</li> </ol> <p>Hop back to VSCode. After a few seconds, you should see the debugging console updating to say that it has  received the message from the message queue: <pre><code>Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355)\nTrigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00\n</code></pre> If you have configured <code>.env</code> and <code>local.settings.json</code> correctly, you will see that the job was posted to the queue <code>nlpqueue-local</code>.</p> <p>After some time (around 15-30 seconds), you should notice further updates to the debugging console. Due to various <code>print()</code> statements in the init.py file (remember, this holds the function's logic), the console will start logging what's occurring as the function processes the message in the message queue. First, the console shows you what message was sent to the queue: <pre><code>MESSAGE &gt;&gt;&gt;  {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'}\n</code></pre> After job submission, the <code>NLPQueue</code> function works through the stages detailed in NLP Processing. Having received concept codes from the NLP API, the function looks up a standard and valid conceptID using the CCOM API. Sometimes, a concept code will be returned which doesn't have a standard and valid conceptID. In such instances you will see the following in the debugging console: <pre><code>Concept Code C34500 not found!\n</code></pre> However, if a standard and valid conceptID is found, the Azure function then saves the data to the <code>ScanReportConcept</code> model. This is shown to you in the debugging console with the <code>PAYLOAD</code> print statement like so: <pre><code>SAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\nSAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\nSAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\n</code></pre> Here, three conceptIDs were found for three different vocabularies (ICD10CM, ICD9CM and SNOWMEDCT_US). Therefore, three records are saved to <code>ScanReportConcepts</code> (hence the three <code>SAVING TO FIELD LEVEL...</code> statements)</p> <p>After successful execution, the debug console will finally tell you that the job has finished: <pre><code>Executed 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms)\n</code></pre></p> <p>Here is a full interrupted trace of the above process: <pre><code>Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355)\nTrigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00\nMESSAGE &gt;&gt;&gt;  {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'}\nConcept Code C34500 not found!\nSAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\nSAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\nSAVING TO FIELD LEVEL...\nPAYLOAD &gt;&gt;&gt; {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15}\nExecuted 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms)\n</code></pre></p>"},{"location":"AzureFunctions/RunningLocally/#stopping-debugging","title":"Stopping Debugging","text":"<p>Debugging can be stopped by clicking:</p> <p>Run (top menu) -&gt; Stop Debugging</p>"},{"location":"AzureFunctions/UploadQueue/","title":"UploadQueue","text":"<p>Note</p> <p>This page is for developers of Azure Functions - not users</p>"},{"location":"AzureFunctions/UploadQueue/#introduction","title":"Introduction","text":"<p>UploadQueue refers to an Azure Queue Trigger function which processes and saves a scan report and data dictionary to the Carrot database. It is also the name of the directory in the webapp's root directory which houses the Azure Function code. The function is triggered when a new message is added to the queue. Messages are added to the queue when a user uploads a Scan Report and Data Dictionary to the webapp.</p> <p>This guide documents the process when a scan report and and a data dictionary are uploaded to the system (i.e. the most complex scenario). Note, however, that a user can choose to upload a scan report without a data dictionary. The code snippets below have been extracted from <code>ScanReportFormView</code> in <code>views.py</code>. The snippets contain only the key code to illustrate a point/functionality, rather than offering the full FormView class code required for successful execution. Please refer to <code>api/views.py</code> for unabridged code.</p>"},{"location":"AzureFunctions/UploadQueue/#triggering-the-function","title":"Triggering the function","text":"<p>The function is triggered when a White Rabbit Scan Report file and a data dictionary are uploaded on the webapp. The code in <code>ScanReportFormView</code> carries out the following steps after form submission and during form validation:</p> <ol> <li>Creates a new object in the <code>ScanReport</code> model</li> <li>Connects to our Azure account</li> <li>Creates unique file names for the scan report and data dictionary</li> <li>Creates a dictionary holding the ScanReport model object and file names from Step 3 called <code>azure_dict</code></li> <li>Uploads data to our Azure blob storage accounts (<code>scan-reports</code> and <code>data-dictionaries</code>) using the connecting string from Step 2</li> <li>Submits <code>azure_dict</code> to the message queue so the Azure function using the connection string from Step 2. This messages is requires so the function ca get the correct files from Azure blob storage</li> </ol> <p>After  uploading the raw data and submitting a message to the queue, views.py doesn't carry out any further code execution - the online Azure function takes over, processes the scan report and dictionary (more on this below) and then POSTs the data back to the webapp via the Carrot API using the <code>requests</code> library. </p>"},{"location":"AzureFunctions/UploadQueue/#creating-a-scanreport-object","title":"Creating a <code>ScanReport</code> object","text":"<p>We create a model object in <code>ScanReport</code> so that tables, fields and values can later be associated with the correct scan report (as the scan report/dictionary is processed within the Azure Function and POST'd back to the webapp.)</p> <pre><code>scan_report = ScanReport.objects.create(\n    data_partner=form.cleaned_data[\"data_partner\"],\n    dataset=form.cleaned_data[\"dataset\"]\n)\n</code></pre>"},{"location":"AzureFunctions/UploadQueue/#connect-to-azure-account","title":"Connect to Azure Account","text":"<p>We connect to our Azure account with the following:</p> <pre><code>blob_service_client = BlobServiceClient.from_connection_string(os.getenv('STORAGE_CONN_STRING'))\n</code></pre> <p>Note that <code>STORAGE_CONN_STRING</code> has to be set either in your .env file when developing locally or within the the Azure Portal. See Sam/Vas for more details on where environment variables are kept online.</p>"},{"location":"AzureFunctions/UploadQueue/#create-unique-file-names","title":"Create unique file names","text":"<p>We create unique datetime and random alphanumeric strings to append to the file names of the scan report and data dictionary:</p> <p><pre><code>rand = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\ndt = '{:%Y%m%d-%H%M%S_}'.format(datetime.datetime.now())\n</code></pre> We do this so that if we ever need to cross-reference a particular scan report with a particular data dictionary, we can easily see (1) when the files were upload (specifed with <code>dt</code>) and (2) what dictionary belongs to which scan report (unique alphanumeric string defined in <code>rand</code>)</p>"},{"location":"AzureFunctions/UploadQueue/#create-dictionary","title":"Create dictionary","text":"<p>We then create a dictionary containing: 1. The newly created scan report ID  2. Unique scan report file name 3. Unique data dictionary nam <pre><code>azure_dict={\n    \"scan_report_id\":scan_report.id,\n    \"scan_report_blob\":os.path.splitext(str(form.cleaned_data.get('scan_report_file')))[0]+\"_\"+dt+rand+\".xlsx\",\n    \"data_dictionary_blob\":os.path.splitext(str(form.cleaned_data.get('data_dictionary_file')))[0]+\"_\"+dt+rand+\".csv\",\n}\n</code></pre></p>"},{"location":"AzureFunctions/UploadQueue/#upload-data-to-azure-blob-storage","title":"Upload data to Azure blob storage","text":"<p>We then upload our data to Azure blob storage. Scan reports go to the container called <code>scan-reports</code> and data dictionaries go to the container called <code>data-dictionaries</code>. </p> <pre><code>blob_client = blob_service_client.get_blob_client(container=\"scan-reports\", blob=os.path.splitext(str(form.cleaned_data.get('scan_report_file')))[0]+\"_\"+dt+rand+\".xlsx\")\nblob_client.upload_blob(form.cleaned_data.get('scan_report_file').open())\nblob_client = blob_service_client.get_blob_client(container=\"data-dictionaries\", blob=os.path.splitext(str(form.cleaned_data.get('data_dictionary_file')))[0]+\"_\"+dt+rand+\".csv\")\nblob_client.upload_blob(form.cleaned_data.get('data_dictionary_file').open())\n</code></pre>"},{"location":"AzureFunctions/UploadQueue/#submit-message-to-azure-queue","title":"Submit message to Azure queue","text":"<p>We then encode and submit the message defined in <code>azure_dict</code> to the Azure message queue: <pre><code>queue_message=json.dumps(azure_dict)\nmessage_bytes = queue_message.encode('ascii')\nbase64_bytes = base64.b64encode(message_bytes)\nbase64_message = base64_bytes.decode('ascii')\n\n...\n...\n...\n\nqueue = QueueClient.from_connection_string(\n  conn_str=os.environ.get(\"STORAGE_CONN_STRING\"),\n  queue_name=os.environ.get(\"SCAN_REPORT_QUEUE_NAME\")\n  )\nqueue.send_message(base64_message)\n</code></pre> Once the message has been sent, the Azure Function will automatically pick up the message and execute the code in <code>__init__.py</code>. Using the data in the message, the function will know what files to grab from blob storage for processing. By defining <code>scan_report_id</code> the function knows how to relate newly created tables, fields and values to the correct scan report.</p>"},{"location":"AzureFunctions/UploadQueue/#azure-function-operation","title":"Azure Function Operation","text":""},{"location":"AzureFunctions/UploadQueue/#accessing-the-file","title":"Accessing the File","text":"<p>As soon as the ProcessQueue is triggered it uses <code>scan_report_blob</code> and <code>data_dictionary_blob</code> from the message body to download the correct files from Azure Blob Storage into a BytesIO() object.</p> <p>The message body is accessed via: <pre><code># Get message from queue\nmessage = json.dumps(\n    {\n        \"id\": msg.id,\n        \"body\": msg.get_body().decode(\"utf-8\"),\n        \"expiration_time\": (\n            msg.expiration_time.isoformat() if msg.expiration_time else None\n        ),\n        \"insertion_time\": (\n            msg.insertion_time.isoformat() if msg.insertion_time else None\n        ),\n        \"time_next_visible\": (\n            msg.time_next_visible.isoformat() if msg.time_next_visible else None\n        ),\n        \"pop_receipt\": msg.pop_receipt,\n        \"dequeue_count\": msg.dequeue_count,\n    }\n)\n\n# Grab message body from storage queues, \n# extract filenames for scan reports and dictionaries\nmessage = json.loads(message)\nbody = ast.literal_eval(message[\"body\"])\nscan_report_blob = body[\"scan_report_blob\"]\ndata_dictionary_blob = body[\"data_dictionary_blob\"]\n</code></pre> The files are accessed from blob storage with the following code: <pre><code># Grab scan report data from blob\ncontainer_client = blob_service_client.get_container_client(\"scan-reports\")\nblob_scanreport_client = container_client.get_blob_client(scan_report_blob)\nstreamdownloader = blob_scanreport_client.download_blob()\nscanreport = BytesIO(streamdownloader.readall())\n\n# Grab data dictionary data from blob\ncontainer_client = blob_service_client.get_container_client(\"data-dictionaries\")\nblob_dict_client = container_client.get_blob_client(data_dictionary_blob)\nstreamdownloader = blob_dict_client.download_blob()\ndata_dictionary = list(csv.DictReader(streamdownloader.readall().decode('utf-8').splitlines()))\n</code></pre></p>"},{"location":"AzureFunctions/UploadQueue/#processing","title":"Processing","text":"<p>The next stage is to step through the Excel workbook and create model entries for each ScanReportTable, ScanReportField and ScanReportValue present in the file.</p>"},{"location":"AzureFunctions/UploadQueue/#scan-report-table","title":"Scan Report Table","text":"<p>The Field Overview sheet in a White Rabbit Scan Report has a 'Table' column which seperates different table names using a blank row. ProcessQueue main method iterates through the rows in the 'Table' column and appends each unique table name to the list <code>table_names</code>.  For each table in <code>table_names</code> a new <code>ScanReportTable</code> entry is generated. The table entries are linked to the <code>ScanReport</code> model using the <code>scan_report_id</code> from the queue message body. The table entries are then appended to a JSON array <code>json_data</code> which forms the input send in a POST request to the API. The API automatically generates the <code>id</code> for each table that was created. These ids are extracted from the POST request response and saved in a list (<code>table_ids</code>)</p>"},{"location":"AzureFunctions/UploadQueue/#scan-report-field","title":"Scan Report Field","text":"<p>Field Overview contains all the information on the fields including the name, description and other useful columns. ProcessQueue iterates through the rows in the sheet and generates a new <code>ScanReportField</code> entry. Once it detects an empty line (end of a table) it saves the fields found in that table. This is done by appending the field entries to a JSON array <code>json_data</code>, which forms the input to a POST request made to the API. From the response it saves the <code>field_ids</code> and <code>field_names</code> and stores them into a dictionary as key value pairs e.g \"Field Name\": Field ID.</p>"},{"location":"AzureFunctions/UploadQueue/#scan-report-value","title":"Scan Report Value","text":"<p>Scan report values are stored in sheets named after their corresponding table. Once the fields in a table are saved, it moves to the corresponding sheet (where the sheet is the same as the table_name) and the <code>process_scan_report_sheet_table()</code> function is called on that worksheet. The function extracts the data in the following format:</p> <p>Input</p> ID Frequency Date Frequency 1 20 02/12/2020 5 2 3 12/11/2020 37 <p>Output</p> <p>(ID, 1, 20) (Date, 02/12/2020, 5) (ID, 2, 3) (Date, 12/11/2020, 37)</p> <p>A ScanReportValue entry is generated by iterating through the output of <code>process_scan_report_sheet_table()</code>. The value entries are linked to the <code>ScanReportField</code> model using the function output and the dictionary with all the field names and ids. Same as with the other two models the value entries are appended to a JSON array <code>json_data</code>, and form the input to a POST request made to the API.</p>"},{"location":"AzureFunctions/UploadQueue/#data-dictionary-creation","title":"Data Dictionary Creation","text":"<p>If a user uploads a data dictionary, there is a small subroutine during the <code>ScanReportValue</code> code which matches the scan report value name against the supplied data dictionary. If there's a match, the code saves the dictionary's value description to the <code>value_description</code> field in the <code>ScanReportValue</code> model:</p> <pre><code>val_desc = next((row['value_description'] for row in data_dictionary if str(row['field_name']) == str(name) and str(row['code']) == str(value)), None)\n</code></pre>"},{"location":"BCLink/TestLink/ssh/","title":"Ssh","text":""},{"location":"BCLink/TestLink/ssh/#setup","title":"Setup","text":"<p>ssh to the test link machine: <pre><code>ssh 52.XXX.XXX.XXX\n</code></pre> This is with the configuration: <pre><code>$ cat ~/.ssh/config\nHost 52.XXX.XXX.XXX\nHostName 52.XXX.XXX.XXX\nPreferredAuthentications publickey\nIdentityFile &lt;path to private key identity file &gt;\n</code></pre> Otherwise you can do something like: <pre><code>ssh -i &lt;path to private key identity file &gt; 52.XXX.XXX.XXX\n</code></pre> If you ssh keys were generated without a /with a different user, you may need to do: <pre><code>ssh &lt;user&gt;@52.XXX.XXX.XXX\n</code></pre></p> <p>To take <code>root</code> control: <pre><code>sudo -s\n</code></pre></p> <p>To then use the user <code>bscos_srv</code> that has permissions to upload to BCLink use: <pre><code>su - bcos_srv\n</code></pre></p>"},{"location":"BCLink/TestLink/ssh/#transfering-a-file","title":"Transfering a file","text":"<p>Using a tool like <code>scp</code> or <code>rsync</code> you can transfer an output from the ETLTool to the test link like this: <pre><code>rsync -avz --progress &lt;local file&gt; 52.XXX.XXX.XXX:/home/&lt;user&gt;\n</code></pre></p> <p>Tip</p> <p>The option <code>-z</code> can compress files over a slow upload connection and make the transfer of large files fast.</p>"},{"location":"BCLink/TestLink/ssh/#uploading-a-file","title":"Uploading a file","text":"<p>Once the file (e.g. person) is on the link, you can transfer it to whatever folder you prefer, I am storing files in: <pre><code>/usr/lib/bcos/OMOP-test-data/calum_tests_4Aug/\n</code></pre></p> <p>The command line tool <code>datasettool2</code> can then be used  to load the file into the BCLink <pre><code>[bcos_srv@link-test-dt calum_tests_4Aug]$ datasettool2 load --dataset=person --datafile=./person.csv --database=bclink\nLoading file ./person.csv to dataset PERSON (person)\ndebug1: client_input_channel_req: channel 0 rtype keepalive@openssh.com reply 1\n</code></pre></p> <p>Tip</p> <p>While in <code>root</code> you can change the permissions of a folder with <code>chmod u+x -R &lt;folder&gt;</code> to give access to all, and make sure the files have the correct permissions for the user <code>bcos_srv</code></p>"},{"location":"BCLink/TestLink/Notes/14Sep/","title":"14Sep","text":""},{"location":"BCLink/TestLink/Notes/14Sep/#working-directory-setup","title":"Working directory setup","text":"<p>In the directory: <pre><code>/usr/lib/bcos/OMOP-test-data/calum_tests_14Sep\n</code></pre></p> <p>I have the <code>tsv</code> file created by the ETL-Tool that I want to test uploading <code>person.tsv</code>.</p>"},{"location":"BCLink/TestLink/Notes/14Sep/#clean-the-database","title":"Clean the database","text":"<pre><code>datasettool2 delete-all-rows ds100394 --database=bclink\n</code></pre>"},{"location":"BCLink/TestLink/Notes/14Sep/#commandline-options","title":"CommandLine Options","text":""},{"location":"BCLink/TestLink/Notes/14Sep/#dataset_tool","title":"dataset_tool","text":"<p>The help gives the following information: <pre><code>dataset_tool --load --table=ds100123 --user=&lt;test_user&gt;\n    --data_file=ds100123.txt [or --data_file_list=my_filelist.txt]\n    [--extra='...'] [--no-backup] [--support]\n    [--move-original-file] [--grab-original-file] [--immediate-write]\n    [--force-prepare-data] [--job-parameters=&lt;extra-job-params-file&gt;]\n    [--bcqueue] [--bcqueue-res-path='job-result-path'] [--niceness=10]\n    [--device=XX] [--subj_filter=xx] [--samples_table=xx] database\n</code></pre></p> <p>Some additional information is also given: <pre><code>Arguments for load mode:\n  --table=ds100123\n     Must contain the name of the destination table.\n  --no-backup\n     Don't do a input file backup. Product dataset data needs no backup.\n  --support\n     Uses supp-dataload-batch queue instead of dataload-batch, \n     or supp-dataupd2-batch queue instead of dataupd2-batch.\n  --immediate-write\n     Avoid job queue altogether: calls simpleupd directly.\n  --user=test_user\n     Must contain the SQL user, as whom the data will be inserted into the data set.\n  --data_file=ds100123.txt\n     Must contain the data file in textdb format, to be loaded into the database.\n  --data_file_list=one_line_per_uploadfile.txt Upload multiple files with one submit.\n  --extra=... Add extra parameters into multiple file upload (--data_file_list) supplied. \n  --update_type=direct or --update_type=incremental:      incremental mode skips row updates with missing alleles.\n  --move-original-file\n     Moves the original file in --data_file=&lt;file&gt; into /data/var/lib/bcos/tmp/ to be used on load operation.\n  --grab-original-file\n     Uses the file &lt;file&gt; in place, so it will not move the file for the following upload job.\n  --subj_filter=[--no,--skip,--strict]\n     '--no' means no samples filter,\n     '--skip' means pass mapped rows thrue, others are discarded,\n     '--strict' means if one unmappable row is found, don't do the upload.\n  --action_mode=[load,insert,?] (please verify the meaning first if you use this).\n  --device=UPLOAD_DEVICE\n  --samples_table=samples means that skip / strict upload is done against this samples table.\n</code></pre></p>"},{"location":"BCLink/TestLink/Notes/14Sep/#datasettool2-load","title":"datasettool2 load","text":"<pre><code>datasettool2 load --dataset=&lt;DATASET&gt; [--datafile=&lt;FILE&gt; | --source-dataset=&lt;SOURCE_DATASET&gt;]\n                  [--submission-id] [--queue] [--resfolder=PATH] [--force-prepare-data]\n          [--timing] [--hierarchical-timing] [--sqltimeout=&lt;SEC&gt;]\n          [--database=&lt;DATABASE&gt;] [--user=&lt;USER&gt;] [--developer=&lt;DEVELOPER&gt;]\n          [--pooledconnection] [--job-id=&lt;JOB&gt;] [--job=&lt;JOB&gt;]\n</code></pre> <p><code>--resfolder=&lt;PATH&gt;</code> : appears to have no effect, all the jobs end up in <code>/data/var/lib/bcos/download/&lt;user&gt;</code>, this would be useful if you could specify the job output path.</p>"},{"location":"BCLink/TestLink/Notes/14Sep/#datasettool2-load2","title":"datasettool2 load2","text":"<pre><code>datasettool2 load2 --dataset=&lt;DATASET&gt; --datafile=&lt;FILE&gt; [--force-prepare-data]\n                  [--timing] [--hierarchical-timing] [--sqltimeout=&lt;SEC&gt;]\n          [--database=&lt;DATABASE&gt;] [--user=&lt;USER&gt;] [--developer=&lt;DEVELOPER&gt;]\n          [--pooledconnection] [--job-id=&lt;JOB&gt;] [--job=&lt;JOB&gt;]\n</code></pre>"},{"location":"BCLink/TestLink/Notes/14Sep/#insert-into-the-database","title":"Insert into the database","text":"<p>Using <code>datasettool2 load2</code></p> <pre><code>[bcos_srv@link-test-dt calum_tests_14Sep]$ datasettool2 load2 --dataset=ds100394 --datafile=`pwd`/person.tsv --database=bclink --user=data \nUpload started: dataset=PERSON_V2 (ds100394), file=/usr/lib/bcos/OMOP-test-data/calum_tests_14Sep/person.tsv, id=9 , resultfolder=datasettool2_uploads/9\nUpload done: dataset=PERSON_V2 (ds100394), file=/usr/lib/bcos/OMOP-test-data/calum_tests_14Sep/person.tsv, id=9 , resultfolder=datasettool2_uploads/9\n</code></pre> <p>These jobs get run on the BCLink queue, and the output results folder can be seen here:</p> <p>By using <code>--user=data</code> I can see the job via the GUI too.</p> <pre><code>$ ls -ltr /data/var/lib/bcos/download/data/datasettool2_uploads/9\ntotal 8\n-rw-rw---- 1 bcos_srv bcos_srv 103 Sep 14 17:13 invalid_values.dat\n-rw-rw---- 1 bcos_srv bcos_srv 755 Sep 14 17:13 cover.10126\n</code></pre> <p>Using <code>datasettool2 load</code></p> <pre><code>\n</code></pre>"},{"location":"BCLink/TestLink/Notes/15Sep/","title":"15Sep","text":"<p>Inserting each table <pre><code>for table in person measurement observation condition_occurrence;\ndo\n    datasettool2 delete-all-rows $table --database=bclink\n    dataset_tool --load --table=$table --user=data --data_file=$table.tsv --support  --bcqueue --bcqueue-res-path=./logs/$table  bclink\ndone\n</code></pre></p> <p>Logs: <pre><code>/data/var/lib/bcos/download/data/logs\n</code></pre></p>"},{"location":"BCLink/TestLink/Notes/28Sep/","title":"28Sep","text":""},{"location":"BCLink/TestLink/Notes/28Sep/#problems-as-bcos_srv-user","title":"Problems as <code>bcos_srv</code> user","text":"<p>Issues because of <code>bcos_srv</code>'s home directory, which it does not own. <pre><code>$ echo $HOME\n/data/usr/lib/bcos\n$ ls -ld $HOME\ndrwxrwxr-x. 9 root root 4096 Jul 20 09:56 /data/usr/lib/bcos\n</code></pre></p> <p>Examples:</p> <ul> <li> <p>When trying to use a text editor, e.g. <code>emacs</code>:   <code>Creating directory: permission denied, /data/usr/lib/bcos/.emacs.d/</code></p> </li> <li> <p>When trying to use <code>pip</code>:</p> </li> </ul> <pre><code>$ python3 -m pip install pip --upgrade\nWARNING: The directory '/data/usr/lib/bcos/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pip in /usr/local/lib/python3.6/site-packages (21.2.4)\nERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/data/usr/lib/bcos/.local'\nCheck the permissions.\n</code></pre> <p>Will changing the <code>bcos_srv</code> home directory mess anything up? i.e.: <pre><code>usermod -d &lt;new_dir&gt; bcos_srv\n</code></pre> Most likely?</p>"},{"location":"BCLink/TestLink/Notes/28Sep/#solutions","title":"Solutions","text":"<p>Setting up a python3 virtual enviroment is ok <pre><code>mkdir /usr/lib/bcos/OMOP-test-data/tests_28Sep\ncd /usr/lib/bcos/OMOP-test-data/tests_28Sep\npython3 -m venv automation\nsource automation/bin/activate\npip install pip --upgrade\npip install co-connect-tools\n</code></pre></p>"},{"location":"BCLink/TestLink/Notes/28Sep/#using-the-gui","title":"Using the GUI","text":"<pre><code>yum install -y xorg-x11-server-Xorg xorg-x11-xauth xorg-x11-apps\n</code></pre> <p>X11 forwarding for GUI: <pre><code>ssh -YX &lt;VM&gt;\n</code></pre></p> <p>To make sure <code>tkinter</code> is installed (on CentOS): <pre><code>sudo yum install python3-tkinter\n</code></pre></p>"},{"location":"CaRROT-CDM/","title":"Carrot-CDM","text":"<p>Welcome to our repo for <code>python</code> tools used by/with the Carrot project. The primary functionality is used for performing ETL on health datasets, converting them to the OHDSI Common Data Model (CDM), via a command line interface.</p> <p>CaRROT-CDM contains a pythonic version<sup>1</sup> of the OHDSI CDM, implemented via the class <code>CommonDataModel</code>. CDM tables, such as \"Person\" are defined as classes (e.g. <code>Person</code>) within the code base. </p> <p></p> <p>The primary purpose of the CaRROT-CDM package is to Extract input datasets and Transform them using mapping rules defined in a json file, outputting formatted datasets in <code>tsv</code> format that can be Loaded into a database or other destination (ETL).</p>"},{"location":"CaRROT-CDM/#getting-started","title":"Getting Started","text":"<p>To get started with the ETL process, follow the instructions for installing the CaRROT-CDM package and running the ETL on the following pages:</p> <p>ETL</p> <ol> <li>Frequently Asked Questions </li> <li>Installing </li> <li>About the ETL Process</li> <li>Guide <ol> <li>Configuration</li> <li>Extract</li> <li>Transform</li> <li>Load</li> </ol> </li> </ol>"},{"location":"CaRROT-CDM/#guides","title":"Guides","text":"<p>More detailed guides and documentation for users and developers can be found in the following locations:</p> <p> User Guide Developer Guide </p> <ol> <li> <p>In the default setup a slightly (<code>visit_detail_id</code> link has been removed from Measurement, Observation and Condition Occurrence tables) modified CDM version <code>5.3.1</code> is used to define a subset of tables in python.\u00a0\u21a9</p> </li> </ol>"},{"location":"CaRROT-CDM/CommandLine/","title":"CommandLine","text":""},{"location":"CaRROT-CDM/CommandLine/#command-line-interface","title":"Command Line Interface","text":"<p>To list the available commands available with the <code>cli</code>, you can simply use <code>--help</code>: <pre><code>$ coconnect --help\nUsage: coconnect [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -l, --loglevel TEXT\n  --help               Show this message and exit.\n\nCommands:\n  display   Commands for displaying various types of data and files.\n  generate  Commands to generate helpful files.\n  info      Commands to find information about the package.\n  map       Commands for mapping data to the OMOP CommonDataModel (CDM)\n</code></pre></p>"},{"location":"CaRROT-CDM/CommandLine/#info","title":"Info","text":"<p>Info commands show a few useful pieces of information to check the tool is installed and working correctly.</p> <pre><code>Commands:\n  data_folder     Get the data folder location\n  install_folder  Get the root folder location of coconnect tools\n  version         Get the installed version of the package'\n</code></pre>"},{"location":"CaRROT-CDM/CommandLine/#generate","title":"Generate","text":"<pre><code>$ coconnect generate --help\nUsage: coconnect generate [OPTIONS] COMMAND [ARGS]...\n\n  Commands to generate helpful files.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  cdm        generate a python configuration for the given table\n  synthetic  generate synthetic data from a ScanReport\n</code></pre> <p>These two commands are useful for development of the tool and testing the tool on fake (synthetic) data</p>"},{"location":"CaRROT-CDM/CommandLine/#display","title":"Display","text":"<p>Various helper functions for displaying data are found with the <code>display</code> command: <pre><code>$ coconnect display --help\nUsage: coconnect display [OPTIONS] COMMAND [ARGS]...\n\n  Commands for displaying various types of data and files.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  dag        Display the OMOP mapping json as a DAG\n  dataframe  Display a dataframe\n  diff       Detect differences in either inputs or output csv files\n  flatten    flattern a rules json file\n  json       Show the OMOP mapping json\n  plot       plot from a csv file\n</code></pre></p>"},{"location":"CaRROT-CDM/CommandLine/#example","title":"Example","text":"<p>This quick example shows how you can run OMOP mapping based on a sample json file</p> <p>Firstly run <code>show</code> to display the input <code>json</code> structure <pre><code>$ coconnect display json example/sample_config/lion_structural_mapping.json\n...\n      \"cdm\": {\n            \"person\": [\n                  {\n                        \"birth_datetime\": {\n                              \"source_table\": \"demo.csv\",\n                              \"source_field\": \"dob\",\n                              \"term_mapping\": null,\n                              \"operations\": [\n                                    \"get_datetime\"\n                              ]\n                        },\n            ...\n</code></pre></p> <p>To display this <code>json</code> as a <code>dag</code> (directed acyclic graph) you can run: <pre><code>$ coconnect display dag example/sample_config/lion_structural_mapping.json \n</code></pre></p>"},{"location":"CaRROT-CDM/CommandLine/#map","title":"Map","text":"<p>Commands used for mapping datasets with OMOP are found with the command <code>map</code> <pre><code>$ coconnect map --help\nUsage: coconnect map [OPTIONS] COMMAND [ARGS]...\n\n  Commands for mapping data to the OMOP CommonDataModel (CDM).\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  py   Commands for using python configurations to run the ETL...\n  run  Perform OMOP Mapping given an json file\n</code></pre></p> <p>Using <code>py</code> command inserts an intermediate step in which the <code>json</code> file can be converted into a <code>py</code> file which defines the CDM tables. This allows a user to have more control over the structural mapping.</p> <p>To create a <code>.py</code> configuration file from this input <code>json</code>. The tool automatically registers these files, to see registered files, you can run: <pre><code>$ coconnect map py list\n{}\n</code></pre> Showing that no files have been created and registered.</p> <p>To create your first configuration file, run <code>make</code> specifying the name of the output file/class: <pre><code>$ coconnect map py make --name Lion  example/sample_config/lion_structural_mapping.json --register\n</code></pre> Using the argument <code>--register</code> will register the configuration file via a symbolic link.</p> <p>If you don't want to register the configuration with the tool via the symbolic link (maybe you don't have the sudo permissions to do this, if you did not install the tool yourself), you can instead export an environment variable to your working directory: <pre><code>export COCONNECT_CONFIG_FOLDER=`pwd`\n</code></pre></p> <p>Looking at the file that has been created, you can see the <code>.py</code> configuration file has been made:</p> <pre><code>from coconnect.cdm import define_person, define_condition_occurrence, define_visit_occurrence, define_measurement, define_observation\nfrom coconnect.cdm import CommonDataModel\nimport json\n\nclass Lion(CommonDataModel):\n\n...\n    @define_person\n    def person_0(self):\n        \"\"\"\n        Create CDM object for person\n        \"\"\"\n        self.birth_datetime = self.inputs[\"demo.csv\"][\"dob\"]\n        self.day_of_birth = self.inputs[\"demo.csv\"][\"dob\"]\n        self.gender_concept_id = self.inputs[\"demo.csv\"][\"gender\"]\n        self.gender_source_concept_id = self.inputs[\"demo.csv\"][\"gender\"]\n        self.gender_source_value = self.inputs[\"demo.csv\"][\"gender\"]\n        self.month_of_birth = self.inputs[\"demo.csv\"][\"dob\"]\n....\n</code></pre> <p>Now we're able to run: <pre><code>$ coconnect map py run --pyconf Lion.py example/sample_input_data/*.csv\n...\n\n2021-04-16 10:29:12 - Lion - INFO - finalised observation\n2021-04-16 10:29:12 - Lion - INFO - saving person to output_data//person.csv\n2021-04-16 10:29:12 - Lion - INFO -            gender_concept_id  year_of_birth  ...  race_source_concept_id  ethnicity_source_value\nperson_id                                    ...                                                \n1                       8532           1962  ...                  123456                        \n2                       8507           1972  ...                  123456                        \n3                       8532           1979  ...                  123422                        \n4                       8532           1991  ...                  123456                        \n\n[4 rows x 12 columns]\n...\n</code></pre></p> <p>Outputs are saved in the folder <code>output_data</code></p>"},{"location":"CaRROT-CDM/Common/","title":"Common","text":"<p>               Bases: <code>OrderedDict</code>, <code>Logger</code></p> <p>Class for formatting DestinationFields in the CommonDataModel</p> <p>Inherits from an ordered dictionary, and maps datatypes to lambda functions. The lamba functions encode how to transform and format a pandas series given the datatype.</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>class DataFormatter(collections.OrderedDict,Logger):\n    \"\"\"\n    Class for formatting DestinationFields in the CommonDataModel\n\n    Inherits from an ordered dictionary, and maps datatypes to lambda functions.\n    The lamba functions encode how to transform and format a pandas series given the datatype.\n\n    \"\"\"\n\n    def check_formatting(self,series,function,nsample=50,tolerance=0.3):\n        \"\"\"\n        Apply a formatting function to a subset of a series\n        Args:\n            series (pandas.Series) : input data series\n            function (built-in function): formatting function to be applied\n            nsample (int): number of rows to sample to make checks on (default = 50)\n        Returns:\n           series : modified or original pandas.Series object\n\n        \"\"\"\n        # get the number of rows of the datframe\n        n = len(series)\n        nsample = nsample if n &gt; nsample else n\n\n\n        #sample the series\n        series_slice = series.sample(nsample)\n        #format the sample of the series\n        series_slice_formatted = function(series_slice)\n\n        #if it's just formatting of a number, just return the series if no error has been raised\n        if series_slice_formatted.dtype == 'Float64':\n            return series\n\n        #if it's formatting of text i.e. date string \n        #and the pre- and post-formatting of the series are equal\n        #dont waste time formatting the entire series, just return it as it is\n        series_slice_values = series_slice.dropna().astype(str).unique()\n        series_slice_formatted_values = series_slice_formatted.dropna().astype(str).replace('', np.nan).dropna().unique()\n\n        if np.array_equal(series_slice_values,series_slice_formatted_values):\n            self.logger.debug(f'Sampling {nsample}/{n} values suggests the column '\\\n                              f'{series.name}\" is  already formatted!!')\n            return series\n        else:\n            a=np.array(series_slice.values,dtype=str)\n            b=np.array(series_slice_formatted.values,dtype=str)\n\n            are_equal = a==b\n            ngood = are_equal.sum()\n            fraction_good = round(ngood / nsample,2)\n\n            logger = self.logger.critical if fraction_good &lt;= tolerance else self.logger.warning\n\n            logger(f'Tested fomatting {nsample} rows of {series.name}. The original data is not in the right format.')\n\n            df_bad = pd.concat([series_slice[~are_equal],series_slice_formatted[~are_equal]],axis=1)\n            df_bad.columns = ['original','should be']\n\n            self.logger.warning(f\"\\n {df_bad}\")\n\n            if logger == self.logger.critical:\n                logger(f\"Fraction of good columns = {fraction_good} ({ngood} / {nsample} ), is below the tolerance threshold={tolerance}\")\n                raise DataStandardError(f\"{series.name} has not been formatted correctly\")\n            else:\n                logger(f\"Fraction of good columns ={fraction_good} ({ngood} / {nsample} ), is above the tolerance threshold={tolerance}\")\n\n\n    def __init__(self,errors='coerce'):\n        super().__init__()\n\n        self['Integer'] = lambda x : pd.to_numeric(x,errors=errors).astype('Int64')\n        self['Float']   = lambda x : pd.to_numeric(x,errors=errors).astype('Float64')\n        self['Text20']  = lambda x : x.fillna('').astype(str).apply(lambda x: x[:20])\n        self['Text50']  = lambda x : x.fillna('').astype(str).apply(lambda x: x[:50])\n        self['Text60']  = lambda x : x.fillna('').astype(str).apply(lambda x: x[:60])\n\n        self['Timestamp'] = lambda x : pd.to_datetime(x,errors=errors)\\\n                                        .dt.strftime('%Y-%m-%d %H:%M:%S.%f')\n        self['Date'] = lambda x : pd.to_datetime(x,errors=errors).dt.date\n</code></pre> <p>               Bases: <code>object</code></p> <p>CommonDataModel Table Destination Field.</p> <p>Object for handling output columns (destination fields) in a  Destination Table</p> <p>Attributes:</p> Name Type Description <code>series</code> <code>Series</code> <p>raw column data in the form of a series</p> <code>dtype</code> <code>str</code> <p>data type for how to format the column based on the DataFormatter</p> <code>required</code> <code>bool</code> <p>if the column is required or not                i.e. if the row should be delete if it is not filled</p> <code>pk</code> <code>str</code> <p>primary key label, indicating if the column is the primary required field</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>class DestinationField(object):\n    \"\"\"\n    CommonDataModel Table Destination Field.\n\n    Object for handling output columns (destination fields) in a \n    Destination Table\n\n    Attributes:\n       series (pandas.Series): raw column data in the form of a series\n       dtype (str): data type for how to format the column based on the DataFormatter\n       required (bool): if the column is required or not \n                        i.e. if the row should be delete if it is not filled\n       pk (str): primary key label, indicating if the column is the primary required field\n\n    \"\"\"\n    def __init__(self, dtype: str, required: bool, pk=False):\n        self.series = None\n        self.dtype = dtype\n        self.required = required\n        self.pk = pk\n</code></pre> <p>               Bases: <code>Logger</code></p> <p>Common object that all CDM objects (tables) inherit from.</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>class DestinationTable(Logger):\n    \"\"\"\n    Common object that all CDM objects (tables) inherit from.\n    \"\"\"\n\n    @classmethod\n    def from_df(cls,df,name=None):\n        obj = cls(name)\n        obj.__df = df\n        #for colname in df.columns:\n        #    obj[colname].series = df[colname]\n        return obj\n\n    def __len__(self):\n        return len(self.__df)\n\n    def reset(self):\n        self.clear()\n        self._meta.clear()\n        self.__init_meta()\n\n    def clear(self):\n        self.__df = None\n        #for field in self.fields:\n        #    series = getattr(self,field)\n        #    del series\n\n    def __init_meta(self):\n        self._meta = {'required_fields':{}}\n\n    def __init__(self,name,_type,_version='v5_3_1',format_level=1):\n        \"\"\"\n        Initialise the CDM DestinationTable Object class\n        Args:\n           _type (str): the name of the object being initialsed, e.g. \"person\"\n           _version (str): the CDM version, see https://github.com/OHDSI/CommonDataModel/tags\n        Returns: \n           None\n        \"\"\"\n        self.name = name\n        self._type = _type\n        self.__init_meta()\n\n        self.dtypes = DataFormatter()\n        self.format_level = FormatterLevel(format_level)\n        self.fields = self.get_field_names()\n        #self.do_formatting = not format_level is None\n\n        if len(self.fields) == 0:\n            raise Exception(\"something misconfigured - cannot find any DataTypes for {self.name}\")\n\n        #print a check to see what cdm objects have been initialised\n        self.logger.debug(self.get_destination_fields())\n        self.__df = None\n\n        #get the required fields\n        self.required_fields = [\n            field\n            for field in self.get_field_names()\n            if getattr(self,field).required == True\n        ]\n\n        self.automatically_fill_missing_columns = True\n        self.tools = OperationTools()\n\n\n    def get_field_names(self):\n        \"\"\"\n        From the current object, loop over all member objects and find those that are instances\n        of a DestinationField (column)\n\n        Returns:\n           list : a list of destination fields (columns [series])\n\n        \"\"\"\n        return [\n            item\n            for item in self.__dict__.keys()\n            if isinstance(getattr(self,item),DestinationField)\n        ]\n\n    def get_field_dtypes(self):\n        \"\"\"\n        From the current object, loop over all member objects and find those that are instances\n        of a DestinationField (column)\n\n        Returns:\n           list : a list of destination fields (columns [series])\n\n        \"\"\"\n        return {\n            item:getattr(self,item).dtype\n            for item in self.__dict__.keys()\n            if isinstance(getattr(self,item),DestinationField)\n        }\n\n    def get_ordering(self):\n        \"\"\"\n        Loops over all associated fields and finds which have been marked as being a primary key.\n\n        Returns:\n            list: a string list of the names of primary columns (fields)\n        \"\"\"\n        retval = [\n            field\n            for field in self.fields\n            if getattr(self,field).pk == True\n        ]\n\n        return retval\n\n    def __getitem__(self, key):\n        \"\"\"\n        Retrieve a field (column) from the table (dataframe)\n\n        Args:\n           key (str) : name of a destination field\n        Returns:\n           DestinationField : the destination field object\n        \"\"\"\n\n        return getattr(self, key)\n\n    def __setitem__(self, key, obj):\n        \"\"\"\n        Register a field object with the table\n        \"\"\"\n        return setattr(self, key, obj)\n\n    #def set_format_level(self,level):\n    #    self.format_level = level\n\n    def set_name(self,name):\n        \"\"\"\n        Register/Set the name of the destination table\n        \"\"\"\n        self.name = name\n        self.logger.name = self.name\n\n    def define(self,_):\n        \"\"\"\n        define function, expected to be overloaded by the user defining the object\n        \"\"\"\n        pass\n\n    def get_destination_fields(self):\n        \"\"\"\n        Get a list of all the destination fields that have been \n        loaded and associated to this cdm object\n\n\n        Returns: \n           list: a list of all the destination fields that have been defined\n        \"\"\"\n        return list(self.fields)\n\n    def update(self,that):\n        #extract all objects from the passed object\n        objs = {k:v for k,v in that.__dict__.items() if k!='logger' }\n        #add objects to this class\n        self.__dict__.update(objs)\n\n\n    def set_df(self,df):\n        self.__df = df\n\n    def get_df(self,force_rebuild=False,dont_build=False,dropna=False,**kwargs):\n        \"\"\"\n        Retrieve a dataframe from the current object\n\n        Returns:\n           pandas.Dataframe: extracted dataframe of the cdm object\n        \"\"\"\n\n        if not self.__df is None:\n            self.logger.debug(f\"df({hex(id(self.__df))}) already exists\")\n\n        if dont_build:\n            if self.__df is None:\n                self.__df = pd.DataFrame(columns = self.fields)\n                self.set_df_name()\n            return self.__df\n\n        #if the dataframe has already been built.. just return it\n        if not self.__df is None and not force_rebuild:\n            self.logger.debug('already got a dataframe, so returning the existing one')\n            if dropna:\n                return self.__df.dropna(axis=1)\n            else:\n                return self.__df\n\n        self.define(self)\n\n        #get a dict of all series\n        #each object is a pandas series\n        dfs = {}\n\n        for field in self.fields:\n            obj = getattr(self,field)\n            series = obj.series\n            if series is None:\n                #if required:\n                #    self.logger.error(f\"{field} is all null/none or has not been set/defined\")\n                #    raise RequiredFieldIsNone(f\"{field} is a required for {self.name}.\")\n                continue\n\n            #rename the column to be the final destination field name\n            series = series.rename(field)\n            #register the new series\n            dfs[field] = series\n            self.logger.debug(f'Adding series to dataframe from field \"{field}\"')\n\n        #if there's none defined, dont do anything\n        if len(dfs) == 0:\n            self.logger.warning(\"no objects defined\")\n            self.__df = pd.DataFrame(columns = self.fields)\n            self.set_df_name()\n            return self.__df\n\n        #check the lengths of the dataframes\n        lengths = list(set([len(df) for df in dfs.values()]))\n        if len(lengths)&gt;1:\n            self.logger.error(\"One or more inputs being mapped to this object has a different number of entries\")\n            for name,df in dfs.items():\n                self.logger.error(f\"{name} of length {len(df)}\")\n            raise BadInputs(\"Differring number of rows in the inputs\")\n\n        #create a dataframe from all the series objects\n        df = pd.concat(dfs.values(),axis=1)\n\n        #find which fields in the cdm havent been defined\n        missing_fields = set(self.fields) - set(df.columns)\n\n        #self._meta['defined_columns'] = df.columns.tolist()\n        #self._meta['undefined_columns'] = list(missing_fields)\n\n        #set these to a nan/null series\n        for field in missing_fields:\n            df[field] = np.NaN\n\n        #simply order the columns \n        df = df[self.fields]\n\n        df = self.finalise(df,**kwargs)\n        df = self.format(df)\n\n        if dropna:\n            df = df.dropna(axis=1)\n\n        #register the df\n        self.__df = df\n        self.set_df_name()\n\n        self.logger.info(f\"created df ({hex(id(df))})[{self.get_df_name()}]\")\n        return self.__df\n\n\n    def get_df_name(self):\n        if not self.__df is None:\n            return self.__df.attrs['name']\n\n    def set_df_name(self):\n        if self.__df is None:\n            return\n        name = re.sub(\"[^0-9a-zA-Z]+\",\"_\",self.name)\n        self.__df.attrs['name'] = name\n\n    def format(self,df):\n\n        if self.format_level is FormatterLevel.OFF:\n            self.logger.info('Not formatting data columns')\n            return df\n        elif self.format_level is FormatterLevel.ON:\n            self.logger.info(\"Automatically formatting data columns.\")\n        elif self.format_level is FormatterLevel.CHECK:\n            self.logger.info(\"Performing checks on data formatting.\")\n\n\n        for col in self.fields:\n            #if is already all na/nan, dont bother trying to format\n            is_nan_already = df[col].head(100).isna().all()\n            if is_nan_already:\n                continue\n\n            obj = getattr(self,col)\n\n            #dont try any formatting for primary keys that need to be integers\n            if obj.pk == True or col == 'person_id':\n                continue\n\n            dtype = obj.dtype\n            formatter_function = self.dtypes[dtype]\n\n            nbefore = len(df[col])\n            if nbefore == 0:\n                self.logger.warning(f\"trying to format an empty column ({cols})\")\n\n            nsample = 5 if nbefore &gt; 5 else nbefore\n            sample = df[col].sample(nsample)\n\n            if self.format_level is FormatterLevel.ON:\n                self.logger.debug(f\"Formatting {col}\")\n                try:\n                    df[col] = formatter_function(df[col])\n                except Exception as e:\n                    self.logger.critical(e)\n                    if 'source_files' in self._meta:\n                        self.logger.error(\"This is coming from the source file (table &amp; column) ...\")\n                        self.logger.error(self._meta['source_files'][col])\n                    raise(e)\n\n                if col in self.required_fields:\n                    df = df[~df[col].isna()]\n                    #count the number of rows after\n                    nafter = len(df)\n                    self._meta['required_fields'][col]['after_formatting'] = nafter\n\n                    if nafter == 0 :\n                        self.logger.error(f\"Something wrong with the formatting of the required field {col} using {dtype}\")\n                        self.logger.info(f\"Formatting resulted in all NaN values. Sample of this column before formatting:\")\n                        self.logger.error(sample)\n                        if 'source_files' in self._meta:\n                            self.logger.error(\"This is coming from the source file (table &amp; column) ...\")\n                            self.logger.error(self._meta['source_files'][col])\n                        raise FormattingError(f\"When formatting the required column {col}, using the formatter function {dtype}, all produced values are  NaN/null values.\")\n                    else:\n                        ndiff = nafter - nbefore\n                        if ndiff &gt; 0:\n                            self.logger.warning(f\"Formatting of values in {col} removed {ndiff} rows, leaving {nafter} rows.\")\n\n            elif self.format_level is FormatterLevel.CHECK:\n                self.logger.debug(f\"Checking formatting of {col} to {dtype}\")\n                try:\n                    _ = self.dtypes.check_formatting(df[col],formatter_function)\n                except Exception as e:\n                    if 'source_files' in self._meta:\n                        self.logger.error(\"This is coming from the source file (table &amp; column) ...\")\n                        self.logger.error(self._meta['source_files'][col])\n                    raise(e) \n\n        return df\n\n    def finalise(self,df,start_index=1,**kwargs):\n        \"\"\"\n        Finalise a dataframe by dropping null/nan rows if a required field is missing.\n        also sort the dataframe by the primary key of the table.\n\n        Args:\n            df (pandas.Dataframe): input dataframe\n        Returns:\n            pandas.Dataframe: cleaned output dataframe\n        \"\"\"\n\n        #loop over the non-index fields\n        for field in df.columns[1:]:\n            #if it's not required, skip\n            if field not in self.required_fields:\n                continue\n\n            #count the number of rows before\n            nbefore = len(df)\n            #remove rows which do not have this required field filled\n            df = df[~df[field].isna()]\n\n            #count the number of rows after\n            nafter = len(df)\n            #get the number of rows removed\n            ndiff = nbefore - nafter\n            #if rows have been removed\n            if ndiff&gt;0:\n                #log a warning message if after requiring non-NaN values has removed all rows\n                log = self.logger.warning if nafter &gt; 0 else self.logger.error\n                log(f\"Requiring non-null values in {field} removed {ndiff} rows, leaving {nafter} rows.\")\n\n            #log some metadata\n            self._meta['required_fields'][field] = {\n                'before':nbefore,\n                'after':nafter\n            }\n\n        #now index properly\n        primary_column = df.columns[0]\n        if primary_column != 'person_id':\n            if df[primary_column].head(100).isnull().all():\n                df[primary_column] = df.reset_index().index + start_index\n\n        #return the dataframe sorted by the primary key requested\n        #ordering = self.get_ordering()\n        #if len(ordering) &gt; 0:\n        #    df = df.sort_values(self.get_ordering())\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DataFormatter.check_formatting","title":"<code>check_formatting(series, function, nsample=50, tolerance=0.3)</code>","text":"<p>Apply a formatting function to a subset of a series Args:     series (pandas.Series) : input data series     function (built-in function): formatting function to be applied     nsample (int): number of rows to sample to make checks on (default = 50) Returns:    series : modified or original pandas.Series object</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def check_formatting(self,series,function,nsample=50,tolerance=0.3):\n    \"\"\"\n    Apply a formatting function to a subset of a series\n    Args:\n        series (pandas.Series) : input data series\n        function (built-in function): formatting function to be applied\n        nsample (int): number of rows to sample to make checks on (default = 50)\n    Returns:\n       series : modified or original pandas.Series object\n\n    \"\"\"\n    # get the number of rows of the datframe\n    n = len(series)\n    nsample = nsample if n &gt; nsample else n\n\n\n    #sample the series\n    series_slice = series.sample(nsample)\n    #format the sample of the series\n    series_slice_formatted = function(series_slice)\n\n    #if it's just formatting of a number, just return the series if no error has been raised\n    if series_slice_formatted.dtype == 'Float64':\n        return series\n\n    #if it's formatting of text i.e. date string \n    #and the pre- and post-formatting of the series are equal\n    #dont waste time formatting the entire series, just return it as it is\n    series_slice_values = series_slice.dropna().astype(str).unique()\n    series_slice_formatted_values = series_slice_formatted.dropna().astype(str).replace('', np.nan).dropna().unique()\n\n    if np.array_equal(series_slice_values,series_slice_formatted_values):\n        self.logger.debug(f'Sampling {nsample}/{n} values suggests the column '\\\n                          f'{series.name}\" is  already formatted!!')\n        return series\n    else:\n        a=np.array(series_slice.values,dtype=str)\n        b=np.array(series_slice_formatted.values,dtype=str)\n\n        are_equal = a==b\n        ngood = are_equal.sum()\n        fraction_good = round(ngood / nsample,2)\n\n        logger = self.logger.critical if fraction_good &lt;= tolerance else self.logger.warning\n\n        logger(f'Tested fomatting {nsample} rows of {series.name}. The original data is not in the right format.')\n\n        df_bad = pd.concat([series_slice[~are_equal],series_slice_formatted[~are_equal]],axis=1)\n        df_bad.columns = ['original','should be']\n\n        self.logger.warning(f\"\\n {df_bad}\")\n\n        if logger == self.logger.critical:\n            logger(f\"Fraction of good columns = {fraction_good} ({ngood} / {nsample} ), is below the tolerance threshold={tolerance}\")\n            raise DataStandardError(f\"{series.name} has not been formatted correctly\")\n        else:\n            logger(f\"Fraction of good columns ={fraction_good} ({ngood} / {nsample} ), is above the tolerance threshold={tolerance}\")\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve a field (column) from the table (dataframe)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str) </code> <p>name of a destination field</p> required <p>Returns:    DestinationField : the destination field object</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    Retrieve a field (column) from the table (dataframe)\n\n    Args:\n       key (str) : name of a destination field\n    Returns:\n       DestinationField : the destination field object\n    \"\"\"\n\n    return getattr(self, key)\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.__init__","title":"<code>__init__(name, _type, _version='v5_3_1', format_level=1)</code>","text":"<p>Initialise the CDM DestinationTable Object class Args:    _type (str): the name of the object being initialsed, e.g. \"person\"    _version (str): the CDM version, see https://github.com/OHDSI/CommonDataModel/tags Returns:     None</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def __init__(self,name,_type,_version='v5_3_1',format_level=1):\n    \"\"\"\n    Initialise the CDM DestinationTable Object class\n    Args:\n       _type (str): the name of the object being initialsed, e.g. \"person\"\n       _version (str): the CDM version, see https://github.com/OHDSI/CommonDataModel/tags\n    Returns: \n       None\n    \"\"\"\n    self.name = name\n    self._type = _type\n    self.__init_meta()\n\n    self.dtypes = DataFormatter()\n    self.format_level = FormatterLevel(format_level)\n    self.fields = self.get_field_names()\n    #self.do_formatting = not format_level is None\n\n    if len(self.fields) == 0:\n        raise Exception(\"something misconfigured - cannot find any DataTypes for {self.name}\")\n\n    #print a check to see what cdm objects have been initialised\n    self.logger.debug(self.get_destination_fields())\n    self.__df = None\n\n    #get the required fields\n    self.required_fields = [\n        field\n        for field in self.get_field_names()\n        if getattr(self,field).required == True\n    ]\n\n    self.automatically_fill_missing_columns = True\n    self.tools = OperationTools()\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.__setitem__","title":"<code>__setitem__(key, obj)</code>","text":"<p>Register a field object with the table</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def __setitem__(self, key, obj):\n    \"\"\"\n    Register a field object with the table\n    \"\"\"\n    return setattr(self, key, obj)\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.define","title":"<code>define(_)</code>","text":"<p>define function, expected to be overloaded by the user defining the object</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def define(self,_):\n    \"\"\"\n    define function, expected to be overloaded by the user defining the object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.finalise","title":"<code>finalise(df, start_index=1, **kwargs)</code>","text":"<p>Finalise a dataframe by dropping null/nan rows if a required field is missing. also sort the dataframe by the primary key of the table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>input dataframe</p> required <p>Returns:     pandas.Dataframe: cleaned output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def finalise(self,df,start_index=1,**kwargs):\n    \"\"\"\n    Finalise a dataframe by dropping null/nan rows if a required field is missing.\n    also sort the dataframe by the primary key of the table.\n\n    Args:\n        df (pandas.Dataframe): input dataframe\n    Returns:\n        pandas.Dataframe: cleaned output dataframe\n    \"\"\"\n\n    #loop over the non-index fields\n    for field in df.columns[1:]:\n        #if it's not required, skip\n        if field not in self.required_fields:\n            continue\n\n        #count the number of rows before\n        nbefore = len(df)\n        #remove rows which do not have this required field filled\n        df = df[~df[field].isna()]\n\n        #count the number of rows after\n        nafter = len(df)\n        #get the number of rows removed\n        ndiff = nbefore - nafter\n        #if rows have been removed\n        if ndiff&gt;0:\n            #log a warning message if after requiring non-NaN values has removed all rows\n            log = self.logger.warning if nafter &gt; 0 else self.logger.error\n            log(f\"Requiring non-null values in {field} removed {ndiff} rows, leaving {nafter} rows.\")\n\n        #log some metadata\n        self._meta['required_fields'][field] = {\n            'before':nbefore,\n            'after':nafter\n        }\n\n    #now index properly\n    primary_column = df.columns[0]\n    if primary_column != 'person_id':\n        if df[primary_column].head(100).isnull().all():\n            df[primary_column] = df.reset_index().index + start_index\n\n    #return the dataframe sorted by the primary key requested\n    #ordering = self.get_ordering()\n    #if len(ordering) &gt; 0:\n    #    df = df.sort_values(self.get_ordering())\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.get_destination_fields","title":"<code>get_destination_fields()</code>","text":"<p>Get a list of all the destination fields that have been  loaded and associated to this cdm object</p> <p>Returns:</p> Name Type Description <code>list</code> <p>a list of all the destination fields that have been defined</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def get_destination_fields(self):\n    \"\"\"\n    Get a list of all the destination fields that have been \n    loaded and associated to this cdm object\n\n\n    Returns: \n       list: a list of all the destination fields that have been defined\n    \"\"\"\n    return list(self.fields)\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.get_df","title":"<code>get_df(force_rebuild=False, dont_build=False, dropna=False, **kwargs)</code>","text":"<p>Retrieve a dataframe from the current object</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: extracted dataframe of the cdm object</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def get_df(self,force_rebuild=False,dont_build=False,dropna=False,**kwargs):\n    \"\"\"\n    Retrieve a dataframe from the current object\n\n    Returns:\n       pandas.Dataframe: extracted dataframe of the cdm object\n    \"\"\"\n\n    if not self.__df is None:\n        self.logger.debug(f\"df({hex(id(self.__df))}) already exists\")\n\n    if dont_build:\n        if self.__df is None:\n            self.__df = pd.DataFrame(columns = self.fields)\n            self.set_df_name()\n        return self.__df\n\n    #if the dataframe has already been built.. just return it\n    if not self.__df is None and not force_rebuild:\n        self.logger.debug('already got a dataframe, so returning the existing one')\n        if dropna:\n            return self.__df.dropna(axis=1)\n        else:\n            return self.__df\n\n    self.define(self)\n\n    #get a dict of all series\n    #each object is a pandas series\n    dfs = {}\n\n    for field in self.fields:\n        obj = getattr(self,field)\n        series = obj.series\n        if series is None:\n            #if required:\n            #    self.logger.error(f\"{field} is all null/none or has not been set/defined\")\n            #    raise RequiredFieldIsNone(f\"{field} is a required for {self.name}.\")\n            continue\n\n        #rename the column to be the final destination field name\n        series = series.rename(field)\n        #register the new series\n        dfs[field] = series\n        self.logger.debug(f'Adding series to dataframe from field \"{field}\"')\n\n    #if there's none defined, dont do anything\n    if len(dfs) == 0:\n        self.logger.warning(\"no objects defined\")\n        self.__df = pd.DataFrame(columns = self.fields)\n        self.set_df_name()\n        return self.__df\n\n    #check the lengths of the dataframes\n    lengths = list(set([len(df) for df in dfs.values()]))\n    if len(lengths)&gt;1:\n        self.logger.error(\"One or more inputs being mapped to this object has a different number of entries\")\n        for name,df in dfs.items():\n            self.logger.error(f\"{name} of length {len(df)}\")\n        raise BadInputs(\"Differring number of rows in the inputs\")\n\n    #create a dataframe from all the series objects\n    df = pd.concat(dfs.values(),axis=1)\n\n    #find which fields in the cdm havent been defined\n    missing_fields = set(self.fields) - set(df.columns)\n\n    #self._meta['defined_columns'] = df.columns.tolist()\n    #self._meta['undefined_columns'] = list(missing_fields)\n\n    #set these to a nan/null series\n    for field in missing_fields:\n        df[field] = np.NaN\n\n    #simply order the columns \n    df = df[self.fields]\n\n    df = self.finalise(df,**kwargs)\n    df = self.format(df)\n\n    if dropna:\n        df = df.dropna(axis=1)\n\n    #register the df\n    self.__df = df\n    self.set_df_name()\n\n    self.logger.info(f\"created df ({hex(id(df))})[{self.get_df_name()}]\")\n    return self.__df\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.get_field_dtypes","title":"<code>get_field_dtypes()</code>","text":"<p>From the current object, loop over all member objects and find those that are instances of a DestinationField (column)</p> <p>Returns:</p> Name Type Description <code>list</code> <p>a list of destination fields (columns [series])</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def get_field_dtypes(self):\n    \"\"\"\n    From the current object, loop over all member objects and find those that are instances\n    of a DestinationField (column)\n\n    Returns:\n       list : a list of destination fields (columns [series])\n\n    \"\"\"\n    return {\n        item:getattr(self,item).dtype\n        for item in self.__dict__.keys()\n        if isinstance(getattr(self,item),DestinationField)\n    }\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.get_field_names","title":"<code>get_field_names()</code>","text":"<p>From the current object, loop over all member objects and find those that are instances of a DestinationField (column)</p> <p>Returns:</p> Name Type Description <code>list</code> <p>a list of destination fields (columns [series])</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def get_field_names(self):\n    \"\"\"\n    From the current object, loop over all member objects and find those that are instances\n    of a DestinationField (column)\n\n    Returns:\n       list : a list of destination fields (columns [series])\n\n    \"\"\"\n    return [\n        item\n        for item in self.__dict__.keys()\n        if isinstance(getattr(self,item),DestinationField)\n    ]\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.get_ordering","title":"<code>get_ordering()</code>","text":"<p>Loops over all associated fields and finds which have been marked as being a primary key.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>a string list of the names of primary columns (fields)</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def get_ordering(self):\n    \"\"\"\n    Loops over all associated fields and finds which have been marked as being a primary key.\n\n    Returns:\n        list: a string list of the names of primary columns (fields)\n    \"\"\"\n    retval = [\n        field\n        for field in self.fields\n        if getattr(self,field).pk == True\n    ]\n\n    return retval\n</code></pre>"},{"location":"CaRROT-CDM/Common/#carrot.cdm.objects.common.DestinationTable.set_name","title":"<code>set_name(name)</code>","text":"<p>Register/Set the name of the destination table</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/common.py</code> <pre><code>def set_name(self,name):\n    \"\"\"\n    Register/Set the name of the destination table\n    \"\"\"\n    self.name = name\n    self.logger.name = self.name\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/","title":"CommonDataModel","text":"<p>The Pythonic version of the CommonDataModel is built object-orientated in the subfolder <code>carrot/cdm/</code>: <pre><code>carrot/cdm/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 decorators.py\n\u251c\u2500\u2500 model.py\n\u251c\u2500\u2500 objects\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 versions\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 v5_3_1\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 condition_occurrence.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 drug_exposure.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 measurement.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 observation.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 person.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 procedure_occurrence.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 specimen.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 visit_occurrence.py\n</code></pre></p>"},{"location":"CaRROT-CDM/CommonDataModel/#destination-tables","title":"Destination Tables","text":"<p>All CDM destination tables are formed as objects and are defined in <code>carrot/cdm/objects</code>, inheriting from a base class (<code>DestinationTable</code>, defined in <code>common.py</code>):</p> <ul> <li>Person</li> <li>Condition Occurrence</li> <li>Visit Occurrence</li> <li>Observation</li> <li>Measurement</li> <li>Drug Exposure</li> <li>Procedure Occurrence</li> <li>Specimen</li> </ul>"},{"location":"CaRROT-CDM/CommonDataModel/#generating-more-tables","title":"Generating More Tables","text":"<p>The package contains <code>.csv</code> dumps taken from BCLink that give descriptions of what fields are contained within each CDM table.</p> <p>At present are only dumps from version <code>5.3.1</code> of the CDM:</p> <pre><code>$ ls $(carrot info data_folder)/cdm/BCLINK_EXPORT/5.3.1\nexport-CONDITION_OCCURRENCE.csv export-MEASUREMENT.csv          export-PERSON.csv\nexport-DRUG_EXPOSURE.csv        export-OBSERVATION.csv\n</code></pre> <p>To help generate a pythonic template for a CDM template, the CLI can be used to do this <pre><code>carrot generate cdm drug_exposure 5.3.1\n</code></pre> This command tool outputs the following code that you could use to copy, paster and edit to create a new table for <code>drug_exposure</code> <pre><code>self.drug_exposure_id = DestinationField(dtype=\"Integer\", required=False , pk=True)\nself.person_id = DestinationField(dtype=\"Integer\", required=False )\nself.drug_concept_id = DestinationField(dtype=\"Integer\", required=False )\nself.drug_exposure_start_date = DestinationField(dtype=\"Date\", required=False )\nself.drug_exposure_start_datetime = DestinationField(dtype=\"Timestamp\", required=False )\nself.drug_exposure_end_date = DestinationField(dtype=\"Date\", required=False )\nself.drug_exposure_end_datetime = DestinationField(dtype=\"Timestamp\", required=False )\nself.verbatim_end_date = DestinationField(dtype=\"Date\", required=False )\nself.drug_type_concept_id = DestinationField(dtype=\"Integer\", required=False )\nself.stop_reason = DestinationField(dtype=\"Text20\", required=False )\nself.refills = DestinationField(dtype=\"Integer\", required=False )\nself.quantity = DestinationField(dtype=\"Float\", required=False )\nself.days_supply = DestinationField(dtype=\"Integer\", required=False )\nself.sig = DestinationField(dtype=\"Integer\", required=False )\nself.route_concept_id = DestinationField(dtype=\"Integer\", required=False )\nself.lot_number = DestinationField(dtype=\"Text50\", required=False )\nself.provider_id = DestinationField(dtype=\"Integer\", required=False )\nself.visit_occurrence_id = DestinationField(dtype=\"Integer\", required=False )\nself.drug_source_value = DestinationField(dtype=\"Text50\", required=False )\nself.drug_source_concept_id = DestinationField(dtype=\"Integer\", required=False )\nself.route_source_value = DestinationField(dtype=\"Text50\", required=False )\nself.dose_unit_source_value = DestinationField(dtype=\"Text50\", required=False )\n</code></pre></p>"},{"location":"CaRROT-CDM/CommonDataModel/#destination-fields","title":"Destination Fields","text":"<p>In <code>common.py</code> a class called <code>DestinationField</code> defines how to handle an input pandas series. This pandas series is effectively a column in the output of the CDM Tables, in other words, <code>DestinationField</code> is an object for the <code>destination_field</code>, e.g. <code>person_id</code> in the <code>destination_table</code> <code>person</code>.</p> <pre><code>class DestinationField(object):\n    def __init__(self, dtype: str, required: bool, pk=False):\n        self.series = None\n        self.dtype = dtype\n        self.required = required\n        self.pk = pk\n</code></pre> <ul> <li><code>self.series</code>: initialises as <code>None</code> and will hold a <code>pandas.Series</code> object if the column is to be filled in the output.    </li> <li><code>self.dtype</code>: specifies a string for handling how to format the output of this column so it's in the right format when saved to the final <code>.csv</code> to be uploaded successfully to a BCLink.  </li> <li><code>self.required</code>:  if the <code>destination_field</code> is required (<code>True</code>), any rows of the final table that do not have this column filled (<code>None</code>, <code>NaN</code>), are removed (dropped) from the output.</li> <li><code>self.pk</code>: specifies if this column is the primary key for its associated table. This can be used to order the tables based on this.</li> </ul> <p>               Bases: <code>Logger</code></p> <p>Pythonic Version of the OHDSI CDM.</p> <p>This class controls and manages CDM Table objects that are added to it</p> <p>When self.process() is executed by the user, all added objects are defined, merged, formatted and finalised, before being dumped to an output file (.tsv file by default).</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>class CommonDataModel(Logger):\n    \"\"\"Pythonic Version of the OHDSI CDM.\n\n    This class controls and manages CDM Table objects that are added to it\n\n    When self.process() is executed by the user, all added objects are defined, merged, formatted and finalised, before being dumped to an output file (.tsv file by default).\n\n    \"\"\"\n\n\n    @classmethod\n    def load(cls,inputs,**kwargs):\n        default_kwargs = {'save_files':False,'do_mask_person_id':False,'format_level':0}\n        default_kwargs.update(kwargs)\n        cdm = cls(**default_kwargs)\n        cdm._load_inputs(inputs)\n        return cdm\n\n    @classmethod\n    def from_rules(cls,rules,**kwargs):\n        cdm = cls(**kwargs)\n        cdm.create_and_add_objects(rules)\n        return cdm\n\n\n    def __init__(self, name=None, omop_version='5.3.1',\n                 outputs = None,\n                 save_files=True,\n                 inputs=None,\n                 use_profiler=False,\n                 format_level=None,\n                 do_mask_person_id=True,\n                 drop_duplicates=True,\n                 automatically_fill_missing_columns=True):\n        \"\"\"\n        CommonDataModel class initialisation \n        Args:\n            name (str): Give a name for the class to appear in the logging\n            output_folder (str): Path of where the output tsv/csv files should be written to.\n                                 The default is to save to a folder in the current directory\n                                 called 'output_data'.\n            inputs (dict or DataCollection): inputs can be a dictionary mapping file names to pandas dataframes,\n                                        or can be a DataCollection object\n            use_profiler (bool): Turn on/off profiling of the CPU/Memory of running the current process. \n                                 The default is set to false.\n        \"\"\"\n        self.profiler = None\n        name = self.__class__.__name__ if name is None else self.__class__.__name__ + \"::\" + name\n\n        self.logger.info(f\"CommonDataModel ({omop_version}) created with co-connect-tools version {carrot_version}\")\n\n        self.omop_version = omop_version\n\n        self.drop_duplicates = drop_duplicates\n        self.do_mask_person_id = do_mask_person_id\n        self.execution_order = None\n\n        if format_level == None:\n            format_level = 1\n        try:\n            format_level = int(format_level)\n        except ValueError:\n            self.logger.error(f\"You as specifying format_level='{format_level}' -- this should be an integer!\")\n            raise ValueError(\"format_level not set as an int \")\n\n        self.format_level = FormatterLevel(format_level)\n        self.profiler = None\n\n        self.outputs = outputs\n        self.save_files = save_files\n\n        if use_profiler:\n            self.logger.debug(f\"Turning on cpu/memory profiling\")\n            self.profiler = Profiler(name=name)\n            self.profiler.start()\n\n        #perform some checks on the input data\n        if isinstance(inputs,dict):\n            self.logger.info(\"Running with an DataCollection object\")\n        elif isinstance(inputs,DataCollection):\n            self.logger.info(\"Running with an DataCollection object\")\n        elif inputs is not None:\n            _type = type(inputs).__name__\n            raise BadInputObject(f\"input object {inputs} is of type {_type}, which is not a valid input object\")\n\n        if inputs is not None:\n            if hasattr(self,'inputs'):\n                self.logger.warning(\"overwriting inputs\")\n            self.inputs = inputs\n        elif not hasattr(self,'inputs'):\n            self.inputs = None\n\n        #register opereation tools\n        self.tools = OperationTools()\n\n        #allow rules to be generated automatically or not\n        self.automatically_fill_missing_columns = automatically_fill_missing_columns\n        if self.automatically_fill_missing_columns:\n            self.logger.info(f\"Turning on automatic cdm column filling\")\n\n        #define a person_id masker, if the person_id are to be masked\n        if self.outputs:\n            self.person_id_masker = self.outputs.load_global_ids()\n            self.indexing_conf = self.outputs.load_indexing()\n        else:\n            self.person_id_masker = None\n            self.indexing_conf = None\n\n        #stores the final pandas dataframe for each CDM object\n        # {\n        #   'person':pandas.DataFrame,\n        #   'measurement':pandas.DataFrame.\n        #   ....\n        #}\n        self.__df_map = {}\n\n        #stores the invididual objects associated to this model\n        # {\n        #     'observation':\n        #     {\n        #         'observation_0': &lt;carrot.cdm.objects.observation.Observation object 0x000&gt;,\n        #         'observation_1': &lt;carrot.cdm.objects.observation.Observation object 0x001&gt;,\n        #     },\n        #     'measurement':\n        #     {\n        #         'measurement_0': &lt;carrot.cdm.objects.measurement.Measurement object 0x000&gt;,\n        #         ...\n        #     }\n        #     ...\n        # }\n        self.__objects = {}\n        #check if objects have already been registered with this class\n        #via the decorator methods\n\n        registered_objects = [\n            getattr(self,name)\n            for name in dir(self)\n            if isinstance(getattr(self,name),DestinationTable)\n        ]\n        #if they have, then include them in this model\n        for obj in registered_objects:\n            obj.inputs = self.inputs\n            self.add(obj)\n\n        self.__analyses = {\n            name:getattr(self,name)\n            for name in dir(self)\n            if isinstance(getattr(self,name),analysis)\n        }\n\n        #bookkeep some logs\n        self.logs = {\n            'meta':{\n                'version': carrot_version,\n                'created_by': getpass.getuser(),\n                'created_at': strftime(\"%Y-%m-%dT%H%M%S\", gmtime()),\n                'dataset':name,\n                'total_data_processed':{}\n            }\n        }\n\n    def _load_inputs(self,inputs):\n        for fname in inputs.keys():\n            destination_table,_ = os.path.splitext(fname)\n            try:\n                obj = get_cdm_class(destination_table).from_df(inputs[fname],destination_table)\n            except KeyError:\n                self.logger.warning(f\"Not loading {fname}, this is not a valid CDM Table\")\n                continue\n\n            df = obj.get_df(force_rebuild=False)\n            self[destination_table] = df.set_index(df.columns[0])\n\n    def reset(self):\n        self.__df_map.clear()\n        [x.reset() for x in self.get_all_objects()]\n        self.inputs.reset()\n\n        if self.outputs:\n            self.person_id_masker = self.outputs.load_global_ids()\n            self.indexing_conf = self.outputs.load_indexing()\n        else:\n            self.person_id_masker = None\n            self.indexing_conf = None\n\n\n\n    def close(self):\n        \"\"\"\n        Class destructor:\n              Stops the profiler from running before deleting self\n        \"\"\"\n\n\n        self.logger.info(json.dumps(self.logs['meta'],indent=6))\n        if self.outputs:\n            self.outputs.write_meta(self.logs)\n            self.outputs.finalise()\n\n        if not hasattr(self,'profiler'):\n            return\n        if self.profiler:\n            self.profiler.stop()\n            df_profile = self.profiler.get_df()\n            f_out = self.output_folder\n            f_out = f'{f_out}{os.path.sep}logs{os.path.sep}'\n            if not os.path.exists(f'{f_out}'):\n                self.logger.info(f'making output folder {f_out}')\n                os.makedirs(f'{f_out}')\n\n            date = self.logs['meta']['created_at']\n            fname = f'{f_out}{os.path.sep}statistics_{date}.csv'\n            df_profile.to_csv(fname)\n            self.logger.info(f\"Writen the memory/cpu statistics to {fname}\")\n            self.logger.info(\"Finished\")\n\n    @classmethod\n    def from_existing(cls,**kwargs):\n        \"\"\"\n        Initialise the CDM model from existing data in the CDM format\n        \"\"\"\n        cdm = cls(**kwargs)\n        if 'inputs' not in kwargs:\n            raise NoInputFiles(\"you need to specify some inputs\")\n        inputs = kwargs['inputs']\n        #loop over all input names\n        for fname in inputs.keys():\n            #obtain the name of the destination table\n            #e.g fname='person.tsv' we want 'person'\n            destination_table,_ = os.path.splitext(fname)\n            name = destination_table\n            if '.' in destination_table:\n                destination_table = destination_table.split('.')[0]\n            try:\n                obj = get_cdm_class(destination_table).from_df(inputs[fname],name=name)\n            except KeyError:\n                cdm.logger.warning(f\"Not loading {fname}, this is not a valid CDM Table\")\n                continue\n            cdm.add(obj)\n        return cdm\n\n    def __del__(self):\n        self.__df_map.clear()\n        del self.__df_map\n        self.__objects.clear()\n        del self.__objects\n\n\n    def __getitem__(self,key):\n        \"\"\"\n        Ability lookup processed objects from the CDM\n        Example:\n            cdm = CommonDataModel()\n            ...\n            cdm.process()\n            ...\n            person = cdm['person']\n        Args:\n            key (str): The name of the cdm table to be returned\n        Returns:\n            pandas.DataFrame if a processed object is found, otherwise returns None\n        \"\"\"\n        if key not in self.__df_map.keys():\n            return None\n        else:\n            return self.__df_map[key]\n\n    def __setitem__(self,key,obj):\n        \"\"\"\n        Registration of a new dataframe for a new object\n        Args:\n            key (str) : name of the CDM table (e.g. \"person\")\n            obj (pandas.DataFrame) : dataframe to refer to \n        \"\"\"\n        self.logger.debug(f\"creating {obj} for {key}\")\n        self.__df_map[key] = obj\n\n    def print(self):\n        for name in self.keys():\n            print (self[name].dropna(axis=1))\n\n    def add(self,obj):\n        \"\"\"\n        Function to add a new CDM table (object) to the current model\n        Args:\n            obj (DestinationTable) : CDM Table to be registered with the class\n        \"\"\"\n        if obj._type not in self.__objects:\n            self.__objects[obj._type] = {}\n\n        if obj.name in self.__objects[obj._type].keys():\n            raise Exception(f\"Object called {obj.name} already exists\")\n\n        obj.cdm = self\n        obj.format_level = self.format_level\n\n        self.__objects[obj._type][obj.name] = obj\n        self.logger.info(f\"Added {obj.name} of type {obj._type}\")\n\n\n    def create_and_add_objects(self,config):\n        #loop over the cdm object types defined in the configuration\n        #e.g person, measurement etc..\n        for destination_table,rules_set in config['cdm'].items():\n            #loop over each object instance in the rule set\n            #for example, condition_occurrence may have multiple rulesx\n            #for multiple condition_ocurrences e.g. Headache, Fever ..\n            for name,rules in rules_set.items():\n                #make a new object for the cdm object\n                #Example:\n                # destination_table : person\n                # get_cdm_class returns &lt;Person&gt;\n                # obj : Person()\n                obj = get_cdm_class(destination_table)()\n                #set the name of the object\n                obj.set_name(name)\n\n                #Build a lambda function that will get executed during run time\n                #and will be able to apply these rules to the inputs that are loaded\n                #(this is useful when chunk)\n                obj.define = lambda x,rules=rules : carrot.tools.apply_rules(x,rules,inputs=self.inputs)\n\n                #register this object with the CDM model, so it can be processed\n                self.add(obj)\n\n\n    def add_analysis(self,func,_id=None):\n        if _id is None:\n            _id = hex(id(func))\n        self.__analyses[_id] = func\n\n    def get_analyses(self):\n        return self.__analyses\n    def get_analysis(self,key):\n        return self.__analyses[key]\n\n    def run_analysis(self,f):\n        return f(self)\n\n    def run_analyses(self,analyses=None,max_workers=4):\n\n        def msg(x):\n            self.logger.info(f\"finished with {x}\")\n            self.logger.debug(x.result())\n\n        start = time()\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = {}\n            for name,f in self.__analyses.items():\n                self.logger.info(f\"Start thread for {f} \")\n                future = executor.submit(f, self)\n                future.add_done_callback(msg)\n                futures[future] = name\n\n            while True:\n                status = {futures[f]:{'status':'running' if f.running() else 'done' if f.done() else 'waiting'} for f in futures}\n                self.logger.debug(json.dumps(status,indent=6))\n                if all([f.done() for f in futures]):\n                    break\n                sleep(1)\n\n\n        results = {}\n        for future in concurrent.futures.as_completed(futures):\n            _id = futures[future]\n            results[_id] = future.result()\n\n        end = time() - start\n        self.logger.info(f\"Running analyses took {end} seconds\")\n        return results\n\n    def find_one(self,config,cols=None,dropna=False):\n        return self.filter(config,cols,dropna).sample(frac=1).iloc[0]\n\n    def find(self,config,cols=None,dropna=False):\n        return self.filter(config,cols,dropna)\n\n    def _filter(self,df,filters):\n        ops = {\n            '&gt;': operator.gt,\n            '&lt;': operator.lt,\n            '&gt;=': operator.ge,\n            '&lt;=': operator.le,\n            '==': operator.eq\n        }\n\n\n        if not isinstance(filters,dict):\n            raise NotImplementedError(\"filter must be a 'dict' .\")\n\n        for col,value in filters.items():\n            if isinstance(value,dict):\n                for op_str,val in value.items():\n                    df = df[ops[op_str](df[col],val)]                        \n            else:\n                df = df[df[col] == value]\n        return df\n\n\n    def filter(self,config,cols=None,dropna=False):\n        retval = copy.deepcopy(self)\n        for table,spec in config.items():\n            df = retval[table]\n            if isinstance(df,DestinationTable):\n                df = df.get_df()\n            for col,func in spec.items():\n                df = df[df[col].apply(func)]\n\n            retval[table] = df\n\n        if dropna:\n            retval = retval.dropna(axis=1)\n        if cols is not None:\n            index = retval.index.name\n            retval = retval.reset_index()\n            retval = retval[[col for col,keep in cols.items() if keep]]\n            if index in retval.columns:\n                retval = retval.set_index(index)\n\n        return retval\n\n\n        # for obj in config:\n        #     if isinstance(obj,str):\n        #         df = self[obj].get_df()\n        #         if df.index.name != 'person_id':\n        #             df = df.set_index('person_id')\n        #     elif isinstance(obj,dict):\n        #         for key,value in obj.items():\n        #             print (self[key])\n        #             df = self[key]\n        #             df = self._filter(df,value)\n        #             if df.index.name != 'person_id':\n        #                 df = df.set_index('person_id')\n        #             if retval is None:\n        #                 retval = df\n        #             else:\n        #                 retval = retval.merge(df,left_index=True,right_index=True)\n        #     else:\n        #         raise NotImplementedError(\"need to pass a json object to filter()\")\n\n\n    def get_all_objects(self):\n        return [ obj for collection in self.__objects.values() for obj in collection.values()]\n\n\n    def get_start_index(self,destination_table):\n        self.logger.debug(f'getting start index for {destination_table}')\n\n        if self.indexing_conf == None or not self.indexing_conf :\n            self.logger.debug(f\"no indexing specified, so starting the index for {destination_table} from 1\")\n            return 1\n\n        if destination_table in self.indexing_conf:\n            return int(self.indexing_conf[destination_table])\n        else:\n            self.logger.warning(self.indexing_conf)\n            self.logger.warning(\"indexing configuration has be parsed \"\n                                f\"but this table ({destination_table}) \"\n                                \"has not be passed, so starting from 1\")\n            return 1\n\n    def clear_objects(self,destination_table=None):\n        if destination_table:\n            self.__objects[destination_table].clear()\n        else:\n            for destination_table in self.__objects:\n                self.__objects[destination_table].clear()\n\n    def get_objects(self,destination_table=None):\n        \"\"\"\n        For a given destination table:\n        * Retrieve all associated objects that have been registered with the class\n\n        Args:\n            destination_table (str) : name of a destination (CDM) table e.g. \"person\"\n        Returns:\n            list : a list of destination table objects \n                   e.g. [&lt;person_0&gt;, &lt;person_1&gt;] \n                   which would be objects for male and female mapping\n        \"\"\"\n        if destination_table == None:\n            return self.__objects\n\n        self.logger.debug(f\"looking for {destination_table}\")\n        if destination_table not in self.__objects.keys():\n            self.logger.error(f\"Trying to obtain the table '{destination_table}', but cannot find any objects\")\n            raise Exception(\"Something wrong!\")\n\n        return [\n            obj\n            for obj in self.__objects[destination_table].values()\n        ]\n\n\n    def mask_person_id(self,df,destination_table):\n        \"\"\"\n        Given a dataframe object, apply a masking map on the person_id, if one has been created\n        Args:\n            df (pandas.Dataframe) : input pandas dataframe\n        Returns:\n            pandas.Dataframe: modified dataframe with person id masked\n        \"\"\"\n\n        if 'person_id' in df.columns:\n            #if masker has not been defined, define it\n            if destination_table == 'person':\n                if self.person_id_masker is not None:\n                    start_index = int(list(self.person_id_masker.values())[-1]) + 1\n                    new = False\n                else:\n                    self.person_id_masker = {}\n                    start_index = self.get_start_index(destination_table)\n                    new = True\n\n                person_id_masker = {}\n                for i,x in enumerate(df['person_id'].unique()):\n                    index = i+start_index\n                    if x in self.person_id_masker:\n                        existing_index = self.person_id_masker[x]\n                        self.logger.error(f\"'{x}' already found in the person_id_masker\")\n                        self.logger.error(f\"'{existing_index}' assigned to this already\")\n                        self.logger.error(f\"was trying to set '{index}'\")\n                        self.logger.error(f\"Most likely cause is this is duplicate data!\")\n                        raise PersonExists('Duplicate person found!')\n                    person_id_masker[x] = index\n\n                self.person_id_masker.update(person_id_masker)\n\n                if self.outputs:\n                    dfp = pd.DataFrame(((s,t)\n                                        for t,s in person_id_masker.items()),\n                                       columns=['SOURCE_SUBJECT','TARGET_SUBJECT'])\n\n                    mode = 'w' if new else 'a'\n                    self.outputs.write(f\"person_ids\",dfp,mode)\n\n            #apply the masking\n            if self.person_id_masker is None:\n                raise Exception(f\"Person ID masking cannot be performed on\"\n                                f\" {destination_table} as no masker based on a person table has been defined!\")\n\n            nbefore = len(df['person_id'])\n            df['person_id'] = df['person_id'].map(self.person_id_masker)\n\n            self.logger.debug(f\"Just masked person_id using integers\")\n            if destination_table != 'person':\n                df.dropna(subset=['person_id'],inplace=True)\n                nafter = len(df['person_id'])\n                ndiff = nbefore - nafter\n                df.attrs['valid_person_id'] = {'before':nbefore,'after':nafter}\n                #if rows have been removed\n                if ndiff&gt;0:\n                    self.logger.error(\"There are person_ids in this table that are not in the output person table!\")\n                    self.logger.error(\"Either they are not in the original data, or while creating the person table, \")\n                    self.logger.error(\"studies have been removed due to lack of required fields, such as birthdate.\")\n                    self.logger.error(f\"{nafter}/{nbefore} were good, {ndiff} studies are removed.\")\n\n        return df\n\n    def count_objects(self):\n        \"\"\"\n        For each CDM (destination) table, count the number of objects associated\n        e.g.\n        {\n           \"observation\": 6,\n           \"condition_occurrence\": 1,\n           \"person\": 2\n        }\n        \"\"\"\n        count_map = json.dumps({\n            key: len(obj.keys())\n            for key,obj in self.__objects.items()\n        },indent=6)\n        self.logger.info(f\"Number of objects to process for each table...\\n{count_map}\")\n\n    def keys(self):\n        \"\"\"\n        For cdm.keys(), return the keys of which objects have been mapped.\n        Hence which CDM table dataframes have been created.\n        This should be used AFTER cdm.process() has been run, which creates the dataframes.\n        \"\"\"\n        return self.__df_map.keys()\n\n    def objects(self):\n        \"\"\"\n        Method to retrieve the input objects to the CDM \n        \"\"\"\n        return self.__objects\n\n\n    def process(self,object_list=None,conserve_memory=False):\n        \"\"\"\n        Process chunked data, processes as follows\n        * While the chunking of is not yet finished\n        * Loop over all CDM tables (via execution order) \n          and process the table (see process_table), returning a dataframe\n        * Register the retrieve dataframe with the model  \n        * For the current chunk slice, save the data/logs to files\n        * Retrieve the next chunk of data\n\n        \"\"\"\n        self.execution_order = self.get_execution_order()\n        self.logger.info(f\"Starting processing in order: {self.execution_order}\")\n        self.count_objects()\n\n        for destination_table in self.execution_order:\n            first = True\n            i = 0\n            while True:                \n                df_generator = self.process_table(destination_table,object_list=object_list)\n                ntables = 0\n                nrows = 0\n                dfs = []\n                #print (self.drop_duplicates)\n                for j,obj in enumerate(df_generator):\n\n                    df = obj.get_df()\n                    ntables +=1\n                    nrows += len(df)\n                    if conserve_memory and self.save_files:\n                        mode = None if first else 'a'\n                        self.save_dataframe(destination_table,df,mode=mode)\n                        first = False\n                        obj.clear()\n                        del df\n                        df = None\n                    else:\n                        dfs.append(df)\n\n                if not conserve_memory:\n                    df = pd.concat(dfs,ignore_index=True)#.sort_values(df.columns[0])\n                    if self.save_files:\n                        if self.drop_duplicates and destination_table != 'person':\n                            nbefore = len(df)\n                            df_hash = pd.util.hash_pandas_object(df.drop(df.columns[0],axis=1),index=False)\n                            df_temp = df[df_hash.duplicated(keep=False)].head(10).dropna(axis=1)\n                            df = df[~df_hash.duplicated()]\n                            nafter = len(df)\n                            ndiff = nbefore - nafter\n                            if ndiff&gt;0:\n                                self.logger.error(f\"Removed {ndiff} row(s) due to duplicates found when merging {destination_table}\")\n                                self.logger.warning(\"Example duplicates...\")\n                                self.logger.warning(df_temp.set_index(df_temp.columns[0]))\n\n                        mode = None if first else 'a'\n                        self.save_dataframe(destination_table,df,mode=mode)\n                        first = False\n\n                    for col in df.columns:\n                        if col.endswith(\"_id\"):\n                            df[col] = df[col].astype(float).astype(pd.Int64Dtype())\n                    if df.index.name == 'index' or df.index.name is None:\n                        df = df.set_index(df.columns[0])\n\n                    self[destination_table] = df\n\n\n                self.logger.info(f'finalised {destination_table} on iteration {i} producing {nrows} rows from {ntables} tables')\n\n                #move onto the next iteration                \n                i+=1\n\n                if self.inputs:\n                    try:\n                        self.inputs.next()\n                    except StopIteration:\n                        break\n                else:\n                    break\n\n            #if inputs are defined, and we havent just finished the last table,\n            #reset the inputs\n            if self.inputs and not destination_table == self.execution_order[-1]:\n                self.inputs.reset()\n\n        #for destination_table in self.execution_order:\n        #    index = self.get_start_index(destination_table)\n        #    print (index)\n\n\n    def process_simult(self,object_list=None,conserve_memory=False):\n        \"\"\"\n        process simulataneously\n        \"\"\"\n        self.execution_order = self.get_execution_order()\n        self.logger.info(f\"Starting processing in order: {self.execution_order}\")\n        self.count_objects()\n        i=0\n        while True:\n            for destination_table in self.execution_order:\n                df_generator = self.process_table(destination_table,object_list=object_list)\n                ntables = 0\n                nrows = 0\n                dfs = []\n                for j,obj in enumerate(df_generator):\n                    df = obj.get_df()\n\n                    ntables +=1\n                    nrows += len(df)\n                    if self.save_files:\n                        mode = None if j==0 else 'a'\n                        self.save_dataframe(destination_table,df,mode=mode)\n                        first = False\n                    if not conserve_memory:\n                        dfs.append(df)\n                    #else:\n                    #    obj.clear()\n                    #    del df\n                    #    df = None\n                if not conserve_memory:\n                    self[destination_table] = pd.concat(dfs,ignore_index=True)\n\n            if self.inputs:\n                try:\n                    self.inputs.next()\n                except StopIteration:\n                    break\n            else:\n                break\n            i+=1\n\n\n\n    def get_tables(self):\n        return list(self.__objects.keys())\n\n    def get_execution_order(self):\n        if not self.execution_order:\n            self.execution_order = sorted(self.__objects.keys(), key=lambda x: x != 'person')\n        return self.execution_order\n\n    def set_execution_order(self,order):\n        self.execution_order = order\n\n    def process_table(self,destination_table,object_list=None):\n        \"\"\"\n        Process a CDM (destination) table. The method proceeds as follows:\n        * Given a destination table name e.g. 'person' \n        * Retrieve all objects belonging to the given CDM table (e.g. &lt;person_0, person_1&gt;)\n        * Loop over each object\n        * Retrieve a dataframe for that object given it's definition/rules (see get_df)\n        * Concatenate all retrieve dataframes together by stacking on top of each other vertically\n        * Create new indexes for the primary column so the go from 1-N \n\n        Args:\n            destination_table (str) : name of a destination table to process (e.g. 'person')\n            object_list (list) : [optional] list of objects to process\n        Returns:\n            list(pandas.Dataframe): a dataframes in the CDM format for this destination table\n        \"\"\"\n        objects = self.get_objects(destination_table)\n        if object_list:\n            objects = [obj for obj in object_list if obj in objects]\n\n        nobjects = len(objects)\n        extra = \"\"\n        if nobjects&gt;1:\n            extra=\"s\"\n        self.logger.info(f\"for {destination_table}: found {nobjects} object{extra}\")\n\n        if len(objects) == 0:\n            yield None\n\n        #execute them all\n        dfs = []\n        self.logger.info(f\"working on {destination_table}\")\n        logs = {'objects':{}}\n\n        if destination_table not in self.logs['meta']['total_data_processed']:\n            self.logs['meta']['total_data_processed'][destination_table] = 0\n\n        nrows_processed = self.logs['meta']['total_data_processed'][destination_table]\n\n        for i,obj in enumerate(objects):\n            self.logger.info(f\"starting on {obj.name}\")\n\n            start_index = self.get_start_index(destination_table)\n            start_index += nrows_processed\n\n            #force_rebuild=True,\n            df = obj.get_df(start_index=start_index)\n\n            self.logger.info(f\"finished {obj.name} ({hex(id(df))}) \"\n                             f\"... {i+1}/{len(objects)} completed, {len(df)} rows\") \n            if len(df) == 0:\n                self.logger.warning(f\".. no outputs were found \")\n                continue\n\n            if self.do_mask_person_id:\n                df = self.mask_person_id(df,destination_table)\n\n            obj._meta.update(df.attrs)\n            nrows_processed += len(df)\n            self.logs['meta']['total_data_processed'][destination_table] = nrows_processed\n            if destination_table not in self.logs:\n                self.logs[destination_table] = {}\n\n            self.logs[destination_table][hex(id(df))] = obj._meta\n\n            obj.set_df(df)\n            yield obj\n\n    def save_dataframe(self,table,df=None,mode=None):\n        if self.outputs:\n            _id = hex(id(df))\n            self.logger.info(f\"saving dataframe ({_id}) to {self.outputs}\")\n            self.outputs.write(table,df,mode)\n        else:\n            self.logger.info(f\"called save_dateframe but outputs are not defined. save_files: {self.save_files}\")\n\n    def set_person_id_map(self,person_id_map):\n        self.person_id_masker = person_id_map\n\n    def set_indexing_map(self,indexing):\n        self.indexing_conf = indexing\n\n    def set_outfile_separator(self,sep):\n        \"\"\"\n        Set which separator to use, e.g. ',' or '\\t' \n\n        Args:\n            sep (str): which separator to use when writing csv (tsv) files\n        \"\"\"\n        self._outfile_separator = sep\n\n    def set_indexing(self,index_map,strict_check=False):\n        \"\"\"\n        Create indexes on input files which would allow rules to use data from \n        different input tables.\n\n        Args:\n            index_map (dict): a map between the filename and what should be the column used for indexing \n\n        \"\"\"\n        if self.inputs == None:\n            raise NoInputFiles('Trying to indexing before any inputs have been setup')\n\n        for key,index in index_map.items():\n            if key not in self.inputs.keys():\n                self.logger.warning(f\"trying to set index '{index}' for '{key}' but this has not been loaded as an inputs!\")\n                continue\n\n            if index not in self.inputs[key].columns:\n                self.logger.error(f\"trying to set index '{index}' on dataset '{key}', but this index is not in the columns! something really wrong!\")\n                continue\n            self.inputs[key].index = self.inputs[key][index].rename('index') \n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Ability lookup processed objects from the CDM Example:     cdm = CommonDataModel()     ...     cdm.process()     ...     person = cdm['person'] Args:     key (str): The name of the cdm table to be returned Returns:     pandas.DataFrame if a processed object is found, otherwise returns None</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def __getitem__(self,key):\n    \"\"\"\n    Ability lookup processed objects from the CDM\n    Example:\n        cdm = CommonDataModel()\n        ...\n        cdm.process()\n        ...\n        person = cdm['person']\n    Args:\n        key (str): The name of the cdm table to be returned\n    Returns:\n        pandas.DataFrame if a processed object is found, otherwise returns None\n    \"\"\"\n    if key not in self.__df_map.keys():\n        return None\n    else:\n        return self.__df_map[key]\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.__init__","title":"<code>__init__(name=None, omop_version='5.3.1', outputs=None, save_files=True, inputs=None, use_profiler=False, format_level=None, do_mask_person_id=True, drop_duplicates=True, automatically_fill_missing_columns=True)</code>","text":"<p>CommonDataModel class initialisation  Args:     name (str): Give a name for the class to appear in the logging     output_folder (str): Path of where the output tsv/csv files should be written to.                          The default is to save to a folder in the current directory                          called 'output_data'.     inputs (dict or DataCollection): inputs can be a dictionary mapping file names to pandas dataframes,                                 or can be a DataCollection object     use_profiler (bool): Turn on/off profiling of the CPU/Memory of running the current process.                           The default is set to false.</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def __init__(self, name=None, omop_version='5.3.1',\n             outputs = None,\n             save_files=True,\n             inputs=None,\n             use_profiler=False,\n             format_level=None,\n             do_mask_person_id=True,\n             drop_duplicates=True,\n             automatically_fill_missing_columns=True):\n    \"\"\"\n    CommonDataModel class initialisation \n    Args:\n        name (str): Give a name for the class to appear in the logging\n        output_folder (str): Path of where the output tsv/csv files should be written to.\n                             The default is to save to a folder in the current directory\n                             called 'output_data'.\n        inputs (dict or DataCollection): inputs can be a dictionary mapping file names to pandas dataframes,\n                                    or can be a DataCollection object\n        use_profiler (bool): Turn on/off profiling of the CPU/Memory of running the current process. \n                             The default is set to false.\n    \"\"\"\n    self.profiler = None\n    name = self.__class__.__name__ if name is None else self.__class__.__name__ + \"::\" + name\n\n    self.logger.info(f\"CommonDataModel ({omop_version}) created with co-connect-tools version {carrot_version}\")\n\n    self.omop_version = omop_version\n\n    self.drop_duplicates = drop_duplicates\n    self.do_mask_person_id = do_mask_person_id\n    self.execution_order = None\n\n    if format_level == None:\n        format_level = 1\n    try:\n        format_level = int(format_level)\n    except ValueError:\n        self.logger.error(f\"You as specifying format_level='{format_level}' -- this should be an integer!\")\n        raise ValueError(\"format_level not set as an int \")\n\n    self.format_level = FormatterLevel(format_level)\n    self.profiler = None\n\n    self.outputs = outputs\n    self.save_files = save_files\n\n    if use_profiler:\n        self.logger.debug(f\"Turning on cpu/memory profiling\")\n        self.profiler = Profiler(name=name)\n        self.profiler.start()\n\n    #perform some checks on the input data\n    if isinstance(inputs,dict):\n        self.logger.info(\"Running with an DataCollection object\")\n    elif isinstance(inputs,DataCollection):\n        self.logger.info(\"Running with an DataCollection object\")\n    elif inputs is not None:\n        _type = type(inputs).__name__\n        raise BadInputObject(f\"input object {inputs} is of type {_type}, which is not a valid input object\")\n\n    if inputs is not None:\n        if hasattr(self,'inputs'):\n            self.logger.warning(\"overwriting inputs\")\n        self.inputs = inputs\n    elif not hasattr(self,'inputs'):\n        self.inputs = None\n\n    #register opereation tools\n    self.tools = OperationTools()\n\n    #allow rules to be generated automatically or not\n    self.automatically_fill_missing_columns = automatically_fill_missing_columns\n    if self.automatically_fill_missing_columns:\n        self.logger.info(f\"Turning on automatic cdm column filling\")\n\n    #define a person_id masker, if the person_id are to be masked\n    if self.outputs:\n        self.person_id_masker = self.outputs.load_global_ids()\n        self.indexing_conf = self.outputs.load_indexing()\n    else:\n        self.person_id_masker = None\n        self.indexing_conf = None\n\n    #stores the final pandas dataframe for each CDM object\n    # {\n    #   'person':pandas.DataFrame,\n    #   'measurement':pandas.DataFrame.\n    #   ....\n    #}\n    self.__df_map = {}\n\n    #stores the invididual objects associated to this model\n    # {\n    #     'observation':\n    #     {\n    #         'observation_0': &lt;carrot.cdm.objects.observation.Observation object 0x000&gt;,\n    #         'observation_1': &lt;carrot.cdm.objects.observation.Observation object 0x001&gt;,\n    #     },\n    #     'measurement':\n    #     {\n    #         'measurement_0': &lt;carrot.cdm.objects.measurement.Measurement object 0x000&gt;,\n    #         ...\n    #     }\n    #     ...\n    # }\n    self.__objects = {}\n    #check if objects have already been registered with this class\n    #via the decorator methods\n\n    registered_objects = [\n        getattr(self,name)\n        for name in dir(self)\n        if isinstance(getattr(self,name),DestinationTable)\n    ]\n    #if they have, then include them in this model\n    for obj in registered_objects:\n        obj.inputs = self.inputs\n        self.add(obj)\n\n    self.__analyses = {\n        name:getattr(self,name)\n        for name in dir(self)\n        if isinstance(getattr(self,name),analysis)\n    }\n\n    #bookkeep some logs\n    self.logs = {\n        'meta':{\n            'version': carrot_version,\n            'created_by': getpass.getuser(),\n            'created_at': strftime(\"%Y-%m-%dT%H%M%S\", gmtime()),\n            'dataset':name,\n            'total_data_processed':{}\n        }\n    }\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.__setitem__","title":"<code>__setitem__(key, obj)</code>","text":"<p>Registration of a new dataframe for a new object Args:     key (str) : name of the CDM table (e.g. \"person\")     obj (pandas.DataFrame) : dataframe to refer to</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def __setitem__(self,key,obj):\n    \"\"\"\n    Registration of a new dataframe for a new object\n    Args:\n        key (str) : name of the CDM table (e.g. \"person\")\n        obj (pandas.DataFrame) : dataframe to refer to \n    \"\"\"\n    self.logger.debug(f\"creating {obj} for {key}\")\n    self.__df_map[key] = obj\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.add","title":"<code>add(obj)</code>","text":"<p>Function to add a new CDM table (object) to the current model Args:     obj (DestinationTable) : CDM Table to be registered with the class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def add(self,obj):\n    \"\"\"\n    Function to add a new CDM table (object) to the current model\n    Args:\n        obj (DestinationTable) : CDM Table to be registered with the class\n    \"\"\"\n    if obj._type not in self.__objects:\n        self.__objects[obj._type] = {}\n\n    if obj.name in self.__objects[obj._type].keys():\n        raise Exception(f\"Object called {obj.name} already exists\")\n\n    obj.cdm = self\n    obj.format_level = self.format_level\n\n    self.__objects[obj._type][obj.name] = obj\n    self.logger.info(f\"Added {obj.name} of type {obj._type}\")\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.close","title":"<code>close()</code>","text":"Class destructor <p>Stops the profiler from running before deleting self</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def close(self):\n    \"\"\"\n    Class destructor:\n          Stops the profiler from running before deleting self\n    \"\"\"\n\n\n    self.logger.info(json.dumps(self.logs['meta'],indent=6))\n    if self.outputs:\n        self.outputs.write_meta(self.logs)\n        self.outputs.finalise()\n\n    if not hasattr(self,'profiler'):\n        return\n    if self.profiler:\n        self.profiler.stop()\n        df_profile = self.profiler.get_df()\n        f_out = self.output_folder\n        f_out = f'{f_out}{os.path.sep}logs{os.path.sep}'\n        if not os.path.exists(f'{f_out}'):\n            self.logger.info(f'making output folder {f_out}')\n            os.makedirs(f'{f_out}')\n\n        date = self.logs['meta']['created_at']\n        fname = f'{f_out}{os.path.sep}statistics_{date}.csv'\n        df_profile.to_csv(fname)\n        self.logger.info(f\"Writen the memory/cpu statistics to {fname}\")\n        self.logger.info(\"Finished\")\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.count_objects","title":"<code>count_objects()</code>","text":"<p>For each CDM (destination) table, count the number of objects associated e.g. {    \"observation\": 6,    \"condition_occurrence\": 1,    \"person\": 2 }</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def count_objects(self):\n    \"\"\"\n    For each CDM (destination) table, count the number of objects associated\n    e.g.\n    {\n       \"observation\": 6,\n       \"condition_occurrence\": 1,\n       \"person\": 2\n    }\n    \"\"\"\n    count_map = json.dumps({\n        key: len(obj.keys())\n        for key,obj in self.__objects.items()\n    },indent=6)\n    self.logger.info(f\"Number of objects to process for each table...\\n{count_map}\")\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.from_existing","title":"<code>from_existing(**kwargs)</code>  <code>classmethod</code>","text":"<p>Initialise the CDM model from existing data in the CDM format</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>@classmethod\ndef from_existing(cls,**kwargs):\n    \"\"\"\n    Initialise the CDM model from existing data in the CDM format\n    \"\"\"\n    cdm = cls(**kwargs)\n    if 'inputs' not in kwargs:\n        raise NoInputFiles(\"you need to specify some inputs\")\n    inputs = kwargs['inputs']\n    #loop over all input names\n    for fname in inputs.keys():\n        #obtain the name of the destination table\n        #e.g fname='person.tsv' we want 'person'\n        destination_table,_ = os.path.splitext(fname)\n        name = destination_table\n        if '.' in destination_table:\n            destination_table = destination_table.split('.')[0]\n        try:\n            obj = get_cdm_class(destination_table).from_df(inputs[fname],name=name)\n        except KeyError:\n            cdm.logger.warning(f\"Not loading {fname}, this is not a valid CDM Table\")\n            continue\n        cdm.add(obj)\n    return cdm\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.get_objects","title":"<code>get_objects(destination_table=None)</code>","text":"<p>For a given destination table: * Retrieve all associated objects that have been registered with the class</p> <p>Parameters:</p> Name Type Description Default <code>destination_table</code> <code>str) </code> <p>name of a destination (CDM) table e.g. \"person\"</p> <code>None</code> <p>Returns:     list : a list of destination table objects             e.g. [, ]             which would be objects for male and female mapping Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def get_objects(self,destination_table=None):\n    \"\"\"\n    For a given destination table:\n    * Retrieve all associated objects that have been registered with the class\n\n    Args:\n        destination_table (str) : name of a destination (CDM) table e.g. \"person\"\n    Returns:\n        list : a list of destination table objects \n               e.g. [&lt;person_0&gt;, &lt;person_1&gt;] \n               which would be objects for male and female mapping\n    \"\"\"\n    if destination_table == None:\n        return self.__objects\n\n    self.logger.debug(f\"looking for {destination_table}\")\n    if destination_table not in self.__objects.keys():\n        self.logger.error(f\"Trying to obtain the table '{destination_table}', but cannot find any objects\")\n        raise Exception(\"Something wrong!\")\n\n    return [\n        obj\n        for obj in self.__objects[destination_table].values()\n    ]\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.keys","title":"<code>keys()</code>","text":"<p>For cdm.keys(), return the keys of which objects have been mapped. Hence which CDM table dataframes have been created. This should be used AFTER cdm.process() has been run, which creates the dataframes.</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def keys(self):\n    \"\"\"\n    For cdm.keys(), return the keys of which objects have been mapped.\n    Hence which CDM table dataframes have been created.\n    This should be used AFTER cdm.process() has been run, which creates the dataframes.\n    \"\"\"\n    return self.__df_map.keys()\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.mask_person_id","title":"<code>mask_person_id(df, destination_table)</code>","text":"<p>Given a dataframe object, apply a masking map on the person_id, if one has been created Args:     df (pandas.Dataframe) : input pandas dataframe Returns:     pandas.Dataframe: modified dataframe with person id masked</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def mask_person_id(self,df,destination_table):\n    \"\"\"\n    Given a dataframe object, apply a masking map on the person_id, if one has been created\n    Args:\n        df (pandas.Dataframe) : input pandas dataframe\n    Returns:\n        pandas.Dataframe: modified dataframe with person id masked\n    \"\"\"\n\n    if 'person_id' in df.columns:\n        #if masker has not been defined, define it\n        if destination_table == 'person':\n            if self.person_id_masker is not None:\n                start_index = int(list(self.person_id_masker.values())[-1]) + 1\n                new = False\n            else:\n                self.person_id_masker = {}\n                start_index = self.get_start_index(destination_table)\n                new = True\n\n            person_id_masker = {}\n            for i,x in enumerate(df['person_id'].unique()):\n                index = i+start_index\n                if x in self.person_id_masker:\n                    existing_index = self.person_id_masker[x]\n                    self.logger.error(f\"'{x}' already found in the person_id_masker\")\n                    self.logger.error(f\"'{existing_index}' assigned to this already\")\n                    self.logger.error(f\"was trying to set '{index}'\")\n                    self.logger.error(f\"Most likely cause is this is duplicate data!\")\n                    raise PersonExists('Duplicate person found!')\n                person_id_masker[x] = index\n\n            self.person_id_masker.update(person_id_masker)\n\n            if self.outputs:\n                dfp = pd.DataFrame(((s,t)\n                                    for t,s in person_id_masker.items()),\n                                   columns=['SOURCE_SUBJECT','TARGET_SUBJECT'])\n\n                mode = 'w' if new else 'a'\n                self.outputs.write(f\"person_ids\",dfp,mode)\n\n        #apply the masking\n        if self.person_id_masker is None:\n            raise Exception(f\"Person ID masking cannot be performed on\"\n                            f\" {destination_table} as no masker based on a person table has been defined!\")\n\n        nbefore = len(df['person_id'])\n        df['person_id'] = df['person_id'].map(self.person_id_masker)\n\n        self.logger.debug(f\"Just masked person_id using integers\")\n        if destination_table != 'person':\n            df.dropna(subset=['person_id'],inplace=True)\n            nafter = len(df['person_id'])\n            ndiff = nbefore - nafter\n            df.attrs['valid_person_id'] = {'before':nbefore,'after':nafter}\n            #if rows have been removed\n            if ndiff&gt;0:\n                self.logger.error(\"There are person_ids in this table that are not in the output person table!\")\n                self.logger.error(\"Either they are not in the original data, or while creating the person table, \")\n                self.logger.error(\"studies have been removed due to lack of required fields, such as birthdate.\")\n                self.logger.error(f\"{nafter}/{nbefore} were good, {ndiff} studies are removed.\")\n\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.objects","title":"<code>objects()</code>","text":"<p>Method to retrieve the input objects to the CDM</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def objects(self):\n    \"\"\"\n    Method to retrieve the input objects to the CDM \n    \"\"\"\n    return self.__objects\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.process","title":"<code>process(object_list=None, conserve_memory=False)</code>","text":"<p>Process chunked data, processes as follows * While the chunking of is not yet finished * Loop over all CDM tables (via execution order)    and process the table (see process_table), returning a dataframe * Register the retrieve dataframe with the model * For the current chunk slice, save the data/logs to files * Retrieve the next chunk of data</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def process(self,object_list=None,conserve_memory=False):\n    \"\"\"\n    Process chunked data, processes as follows\n    * While the chunking of is not yet finished\n    * Loop over all CDM tables (via execution order) \n      and process the table (see process_table), returning a dataframe\n    * Register the retrieve dataframe with the model  \n    * For the current chunk slice, save the data/logs to files\n    * Retrieve the next chunk of data\n\n    \"\"\"\n    self.execution_order = self.get_execution_order()\n    self.logger.info(f\"Starting processing in order: {self.execution_order}\")\n    self.count_objects()\n\n    for destination_table in self.execution_order:\n        first = True\n        i = 0\n        while True:                \n            df_generator = self.process_table(destination_table,object_list=object_list)\n            ntables = 0\n            nrows = 0\n            dfs = []\n            #print (self.drop_duplicates)\n            for j,obj in enumerate(df_generator):\n\n                df = obj.get_df()\n                ntables +=1\n                nrows += len(df)\n                if conserve_memory and self.save_files:\n                    mode = None if first else 'a'\n                    self.save_dataframe(destination_table,df,mode=mode)\n                    first = False\n                    obj.clear()\n                    del df\n                    df = None\n                else:\n                    dfs.append(df)\n\n            if not conserve_memory:\n                df = pd.concat(dfs,ignore_index=True)#.sort_values(df.columns[0])\n                if self.save_files:\n                    if self.drop_duplicates and destination_table != 'person':\n                        nbefore = len(df)\n                        df_hash = pd.util.hash_pandas_object(df.drop(df.columns[0],axis=1),index=False)\n                        df_temp = df[df_hash.duplicated(keep=False)].head(10).dropna(axis=1)\n                        df = df[~df_hash.duplicated()]\n                        nafter = len(df)\n                        ndiff = nbefore - nafter\n                        if ndiff&gt;0:\n                            self.logger.error(f\"Removed {ndiff} row(s) due to duplicates found when merging {destination_table}\")\n                            self.logger.warning(\"Example duplicates...\")\n                            self.logger.warning(df_temp.set_index(df_temp.columns[0]))\n\n                    mode = None if first else 'a'\n                    self.save_dataframe(destination_table,df,mode=mode)\n                    first = False\n\n                for col in df.columns:\n                    if col.endswith(\"_id\"):\n                        df[col] = df[col].astype(float).astype(pd.Int64Dtype())\n                if df.index.name == 'index' or df.index.name is None:\n                    df = df.set_index(df.columns[0])\n\n                self[destination_table] = df\n\n\n            self.logger.info(f'finalised {destination_table} on iteration {i} producing {nrows} rows from {ntables} tables')\n\n            #move onto the next iteration                \n            i+=1\n\n            if self.inputs:\n                try:\n                    self.inputs.next()\n                except StopIteration:\n                    break\n            else:\n                break\n\n        #if inputs are defined, and we havent just finished the last table,\n        #reset the inputs\n        if self.inputs and not destination_table == self.execution_order[-1]:\n            self.inputs.reset()\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.process_simult","title":"<code>process_simult(object_list=None, conserve_memory=False)</code>","text":"<p>process simulataneously</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def process_simult(self,object_list=None,conserve_memory=False):\n    \"\"\"\n    process simulataneously\n    \"\"\"\n    self.execution_order = self.get_execution_order()\n    self.logger.info(f\"Starting processing in order: {self.execution_order}\")\n    self.count_objects()\n    i=0\n    while True:\n        for destination_table in self.execution_order:\n            df_generator = self.process_table(destination_table,object_list=object_list)\n            ntables = 0\n            nrows = 0\n            dfs = []\n            for j,obj in enumerate(df_generator):\n                df = obj.get_df()\n\n                ntables +=1\n                nrows += len(df)\n                if self.save_files:\n                    mode = None if j==0 else 'a'\n                    self.save_dataframe(destination_table,df,mode=mode)\n                    first = False\n                if not conserve_memory:\n                    dfs.append(df)\n                #else:\n                #    obj.clear()\n                #    del df\n                #    df = None\n            if not conserve_memory:\n                self[destination_table] = pd.concat(dfs,ignore_index=True)\n\n        if self.inputs:\n            try:\n                self.inputs.next()\n            except StopIteration:\n                break\n        else:\n            break\n        i+=1\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.process_table","title":"<code>process_table(destination_table, object_list=None)</code>","text":"<p>Process a CDM (destination) table. The method proceeds as follows: * Given a destination table name e.g. 'person'  * Retrieve all objects belonging to the given CDM table (e.g. ) * Loop over each object * Retrieve a dataframe for that object given it's definition/rules (see get_df) * Concatenate all retrieve dataframes together by stacking on top of each other vertically * Create new indexes for the primary column so the go from 1-N  <p>Parameters:</p> Name Type Description Default <code>destination_table</code> <code>str) </code> <p>name of a destination table to process (e.g. 'person')</p> required <code>object_list</code> <code>list) </code> <p>[optional] list of objects to process</p> <code>None</code> <p>Returns:     list(pandas.Dataframe): a dataframes in the CDM format for this destination table</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def process_table(self,destination_table,object_list=None):\n    \"\"\"\n    Process a CDM (destination) table. The method proceeds as follows:\n    * Given a destination table name e.g. 'person' \n    * Retrieve all objects belonging to the given CDM table (e.g. &lt;person_0, person_1&gt;)\n    * Loop over each object\n    * Retrieve a dataframe for that object given it's definition/rules (see get_df)\n    * Concatenate all retrieve dataframes together by stacking on top of each other vertically\n    * Create new indexes for the primary column so the go from 1-N \n\n    Args:\n        destination_table (str) : name of a destination table to process (e.g. 'person')\n        object_list (list) : [optional] list of objects to process\n    Returns:\n        list(pandas.Dataframe): a dataframes in the CDM format for this destination table\n    \"\"\"\n    objects = self.get_objects(destination_table)\n    if object_list:\n        objects = [obj for obj in object_list if obj in objects]\n\n    nobjects = len(objects)\n    extra = \"\"\n    if nobjects&gt;1:\n        extra=\"s\"\n    self.logger.info(f\"for {destination_table}: found {nobjects} object{extra}\")\n\n    if len(objects) == 0:\n        yield None\n\n    #execute them all\n    dfs = []\n    self.logger.info(f\"working on {destination_table}\")\n    logs = {'objects':{}}\n\n    if destination_table not in self.logs['meta']['total_data_processed']:\n        self.logs['meta']['total_data_processed'][destination_table] = 0\n\n    nrows_processed = self.logs['meta']['total_data_processed'][destination_table]\n\n    for i,obj in enumerate(objects):\n        self.logger.info(f\"starting on {obj.name}\")\n\n        start_index = self.get_start_index(destination_table)\n        start_index += nrows_processed\n\n        #force_rebuild=True,\n        df = obj.get_df(start_index=start_index)\n\n        self.logger.info(f\"finished {obj.name} ({hex(id(df))}) \"\n                         f\"... {i+1}/{len(objects)} completed, {len(df)} rows\") \n        if len(df) == 0:\n            self.logger.warning(f\".. no outputs were found \")\n            continue\n\n        if self.do_mask_person_id:\n            df = self.mask_person_id(df,destination_table)\n\n        obj._meta.update(df.attrs)\n        nrows_processed += len(df)\n        self.logs['meta']['total_data_processed'][destination_table] = nrows_processed\n        if destination_table not in self.logs:\n            self.logs[destination_table] = {}\n\n        self.logs[destination_table][hex(id(df))] = obj._meta\n\n        obj.set_df(df)\n        yield obj\n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.set_indexing","title":"<code>set_indexing(index_map, strict_check=False)</code>","text":"<p>Create indexes on input files which would allow rules to use data from  different input tables.</p> <p>Parameters:</p> Name Type Description Default <code>index_map</code> <code>dict</code> <p>a map between the filename and what should be the column used for indexing</p> required Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def set_indexing(self,index_map,strict_check=False):\n    \"\"\"\n    Create indexes on input files which would allow rules to use data from \n    different input tables.\n\n    Args:\n        index_map (dict): a map between the filename and what should be the column used for indexing \n\n    \"\"\"\n    if self.inputs == None:\n        raise NoInputFiles('Trying to indexing before any inputs have been setup')\n\n    for key,index in index_map.items():\n        if key not in self.inputs.keys():\n            self.logger.warning(f\"trying to set index '{index}' for '{key}' but this has not been loaded as an inputs!\")\n            continue\n\n        if index not in self.inputs[key].columns:\n            self.logger.error(f\"trying to set index '{index}' on dataset '{key}', but this index is not in the columns! something really wrong!\")\n            continue\n        self.inputs[key].index = self.inputs[key][index].rename('index') \n</code></pre>"},{"location":"CaRROT-CDM/CommonDataModel/#carrot.cdm.model.CommonDataModel.set_outfile_separator","title":"<code>set_outfile_separator(sep)</code>","text":"<p>Set which separator to use, e.g. ',' or '       ' </p> <p>Parameters:</p> Name Type Description Default <code>sep</code> <code>str</code> <p>which separator to use when writing csv (tsv) files</p> required Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/model.py</code> <pre><code>def set_outfile_separator(self,sep):\n    \"\"\"\n    Set which separator to use, e.g. ',' or '\\t' \n\n    Args:\n        sep (str): which separator to use when writing csv (tsv) files\n    \"\"\"\n    self._outfile_separator = sep\n</code></pre>"},{"location":"CaRROT-CDM/CommonErrors/","title":"CommonErrors","text":"<p>Below are a list of common errors that are often encountered. </p>"},{"location":"CaRROT-CDM/CommonErrors/#error-no-matching-distribution-found-for-carrot-cdm","title":"<code>ERROR: No matching distribution found for carrot-cdm</code>","text":"<p>When installing carrot-cdm you see messages like this: <pre><code>$ pip install carrot-cdm\nCollecting carrot-cdm\n  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x10d7c39a0&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/carrot-cdm/\n...\n...\nERROR: Could not find a version that satisfies the requirement carrot-cdm (from versions: none)\nERROR: No matching distribution found for carrot-cdm\n</code></pre></p> <p>The most common cause of this error is a lack of internet connection or lack of connection to pypi.org</p> <p>Check this via the <code>ping</code> command, which should give you a response, e.g.: <pre><code>$ ping pypi.org\nPING pypi.org (151.101.192.223): 56 data bytes\n64 bytes from 151.101.192.223: icmp_seq=0 ttl=58 time=20.124 ms\n</code></pre> If you see <pre><code>$ ping pypi.org\nping: cannot resolve pypi.org: Unknown host\n</code></pre> This means your machine cannot establish a connection to <code>pypi</code> to download and install the tool. </p> <p> FAQ: How can I install the tool offline? </p>"},{"location":"CaRROT-CDM/CommonErrors/#carrottoolsfile_helpersmissinginputfiles","title":"<code>carrot.tools.file_helpers.MissingInputFiles</code>","text":"<p>One of the most common errors is due to missing or badly named files.</p> <p>For example you might see the following error: <pre><code>carrot.tools.file_helpers.MissingInputFiles: Found the following files ['foo.csv','bar.csv'] in the json file, that are not in the loaded file list... ['Foo.csv','Bar.csv']\n</code></pre></p> <p>The message is saying that the tool loaded the files <code>['Foo.csv','Bar.csv']</code> but in the <code>json</code> file for the rules, it is expecting the files <code>['foo.csv','bar.csv']</code>.</p> <p>This is a common problem due to files being renamed based on what has been used to generate the <code>json</code> file.</p> <p>Danger</p> <p>The file names in the <code>json</code> file must match the file names, or the files in the folder, you pass as inputs. Otherwise the tool is not going to be able to find or known about that dataset and be able to perform the mapping.</p>"},{"location":"CaRROT-CDM/CommonErrors/#when-supplying-a-directory","title":"When supplying a directory","text":"<p>For the <code>INPUTS</code> argument, if you supply a directory, the tool will look in that directory for any file matching the extention <code>.csv</code>, if you file names do not match this ending, then the tool wont be able to pick them up and you'll have to supply them individually.</p>"},{"location":"CaRROT-CDM/CommonErrors/#jsondecoderjsondecodeerror","title":"<code>json.decoder.JSONDecodeError</code>","text":"<p>A message along the lines of: <pre><code>json.decoder.JSONDecodeError: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)\n</code></pre></p> <p>Tells you that something is wrong with the input <code>json</code> file for your rules.</p> <p>First you should check that the file supplied with the <code>--rules</code> argument is a valid <code>json</code> file containing the mapping-rules needed by the tool.</p> <p>Often the <code>json.decoder.JSONDecodeError</code> will tell you what line and column there is an error coming from. Common issues are due to bad formating or accidental insertions of whitespace characters for a text editor, if the file has been opened up and manually edited.</p>"},{"location":"CaRROT-CDM/CommonErrors/#carrotcdmobjectscommonformattingerror","title":"carrot.cdm.objects.common.FormattingError","text":"<p>This is an error message telling you that something in your input data is not in the right format for the tool to handle and format. </p> <p>Example</p> <pre><code>carrot.cdm.objects.common.FormattingError: The column person_id using the formatter function Integer produced NaN values in a required column\n</code></pre> <p>In the above example, this error message means that while trying to format the <code>person_id</code> column into a Integer, every single value failed, meaning the column in the input data that is used for the <code>person_id</code> is not in a format that can be converted into an Integer.</p> <p>Inspecting further you may see something like: <pre><code>2021-08-06 16:55:23 - person_0 - ERROR - Something wrong with the formatting of the field 'person_id'.\n2021-08-06 16:55:23 - person_0 - ERROR - Using the formatter 'Integer' failed on all values.\n2021-08-06 16:55:23 - person_0 - INFO - Sample of this column before formatting:\n2021-08-06 16:55:23 - person_0 - ERROR - index\ns4    s4\nd2    d2\ns3    s3\ns1    s1\nName: person_id, dtype: object\n</code></pre> Which shows that indeed the <code>person_id</code> column contains strings and was not able to be convert to an Integer.</p> <p>Note</p> <p>By default, the <code>person_id</code> column is expected to already be an integer. </p>"},{"location":"CaRROT-CDM/CommonErrors/#requiring-non-null-values-in-removed-x-rows-leaving-y-rows","title":"Requiring non-null values in  removed X rows, leaving Y rows. <p>This may come as a <code>WARNING</code> or <code>ERROR</code> message in your logs. <pre><code>WARNING - Requiring non-null values in &lt;destination_field&gt; removed X rows, leaving Y rows.\nERROR - Requiring non-null values in &lt;destination_field&gt; removed X rows, leaving Y rows\n</code></pre></p> <p>This is an <code>ERROR</code> message if <code>Y</code> is 0. In that instances, if <code>X</code> is small then it's not likely to be a genuine problem. You want to watch out for if <code>X&gt;&gt;Y</code> and <code>Y = 0</code>, this indicates that the mapping of the <code>&lt;destination_field&gt;</code> is failing.</p>  <p>Example</p> <p><pre><code>person_0 - WARNING - Requiring non-null values in gender_concept_id removed 4 rows, leaving 6 rows.\n</code></pre> You are most likely to see two person objects in the rules <code>json</code> file, as males and females are mapped separately. I.e. the <code>source_value</code> (e.g. 'M' or 'F') is mapped to the <code>concept_id</code> for male and female separately. </p> <p>This <code>WARNING</code> message is saying that for 10 rows of data (10 people), 4 rows were removed because the gender_concept_id did not map to anything. The most likely explanation for this is that <code>person_0</code> is mapping only one sex (e.g females) and therefore drops all rows for the other (e.g. males).</p> <p>There is most likely a <code>person_1</code> object which will be the other sex, and you should see that it maps the other 6 people (and removes 4 people instead): <pre><code>person_1 - WARNING - Requiring non-null values in gender_concept_id removed 6 rows, leaving 4 rows.\n</code></pre> In the output file produced for the <code>person</code> table, which is a merge of <code>person_0</code> and <code>person_1</code>, you'll see there are 10 rows of people.</p>","text":""},{"location":"CaRROT-CDM/ConditionOccurrence/","title":"ConditionOccurrence","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Condition Occurrence object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/condition_occurrence.py</code> <pre><code>class ConditionOccurrence(DestinationTable):\n    \"\"\"\n    CDM Condition Occurrence object class\n    \"\"\"\n\n    name = 'condition_occurrence'\n    def __init__(self,name=None):\n        self.condition_occurrence_id       = DestinationField(dtype=\"Integer\"   , required=True , pk=True)\n        self.person_id                     = DestinationField(dtype=\"Integer\"   , required=True )\n        self.condition_concept_id          = DestinationField(dtype=\"Integer\"   , required=True )\n        self.condition_start_date          = DestinationField(dtype=\"Date\"      , required=False )\n        self.condition_start_datetime      = DestinationField(dtype=\"Timestamp\" , required=True )\n        self.condition_end_date            = DestinationField(dtype=\"Date\"      , required=False )\n        self.condition_end_datetime        = DestinationField(dtype=\"Timestamp\" , required=False )\n        self.condition_type_concept_id     = DestinationField(dtype=\"Integer\"   , required=False )\n        self.stop_reason                   = DestinationField(dtype=\"Text20\"    , required=False )\n        self.provider_id                   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.visit_occurrence_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.condition_source_value        = DestinationField(dtype=\"Text50\"    , required=False )\n        self.condition_source_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.condition_status_source_value = DestinationField(dtype=\"Text50\"    , required=False )\n        self.condition_status_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the condition_occurrence objects\n        * condition_concept_id is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n        if self.automatically_fill_missing_columns == True:\n            if df['condition_start_date'].isnull().all():\n                df['condition_start_date'] = self.tools.get_date(df['condition_start_datetime'])\n\n            if df['condition_end_date'].isnull().all():\n                df['condition_end_date'] = self.tools.get_date(df['condition_end_datetime'])\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/ConditionOccurrence/#carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the condition_occurrence objects * condition_concept_id is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/condition_occurrence.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the condition_occurrence objects\n    * condition_concept_id is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n    if self.automatically_fill_missing_columns == True:\n        if df['condition_start_date'].isnull().all():\n            df['condition_start_date'] = self.tools.get_date(df['condition_start_datetime'])\n\n        if df['condition_end_date'].isnull().all():\n            df['condition_end_date'] = self.tools.get_date(df['condition_end_datetime'])\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/DataCollection/","title":"DataCollection","text":"<p>               Bases: <code>Logger</code></p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/common.py</code> <pre><code>class DataCollection(Logger):\n    def __init__(self,chunksize=None,nrows=None,**kwargs):\n        self.logger.info(\"DataCollection Object Created\")\n        self.__bricks = {}\n        self.chunksize = chunksize\n        self.nrows = nrows\n\n        if self.chunksize is not None:\n            self.logger.info(f\"Using a chunksize of '{self.chunksize}' nrows\")\n\n    def print(self):\n        print (self.all())\n\n    def all(self):\n        return {\n            key:self[key]\n            for key in self.keys()\n        }\n    def finalise(self):\n        pass\n\n    def keys(self):\n        return self.__bricks.keys()\n\n    def items(self):\n        return self.__bricks.items()\n\n    def __setitem__(self,key,obj):\n        self.logger.info(f\"Registering  {key} [{obj}]\")\n        self.__bricks[key] = obj\n\n    def load_global_ids(self):\n        return\n\n    def load_indexing(self):\n        return\n\n    def next(self):\n        #loop over all loaded files\n        self.logger.info(\"Getting next chunk of data\")\n\n        used_bricks = []\n        for key,brick in self.items():\n            if brick.is_finished():\n                continue\n\n            if brick.is_init():\n                used_bricks.append(brick)\n            else:\n                continue\n\n            self.logger.info(f\"Getting the next chunk of size '{self.chunksize}' for '{key}'\")\n            brick.get_chunk(self.chunksize)\n            n = len(brick.get_df())\n            self.logger.info(f\"--&gt; Got {n} rows\")\n            if n == 0:\n                brick.set_finished(True)\n\n        #check if all __dataframe objects are empty\n        #if they are, raise a StopIteration as processing has finished\n        if all([x.is_finished() for x in used_bricks]):\n            self.logger.info(\"All input files for this object have now been used.\")\n            raise StopIteration\n\n\n    def get_handler(self,key):\n        brick = self.__bricks[key]\n        return brick.get_handler()\n\n    def get_all(self):\n        self.logger.info(f\"Retrieving initial dataframes for the first time\")\n        for b in self.__bricks.values():\n            b.get_chunk(self.chunksize)\n            b.set_init(True)\n\n    def get(self,key):\n        return self.__bricks[key]\n\n    def __getitem__(self,key):\n        brick = self.__bricks[key]\n        if not brick.is_init():\n            self.logger.info(f\"Retrieving initial dataframe for '{key}' for the first time\")\n            brick.get_chunk(self.chunksize)\n            brick.set_init(True)\n\n        #if any(not x.is_init() for x in self.__bricks.values()):\n        #    self.get_all()\n\n        df = brick.get_df()\n        self.logger.debug(f\"Got brick {brick}\")\n        return df\n\n    def reset(self):\n        self.logger.info(f\"resetting used bricks\")\n        for key,brick in self.items():\n            brick.reset()\n</code></pre> <p>               Bases: <code>DataCollection</code></p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/plugins/local.py</code> <pre><code>class LocalDataCollection(DataCollection):\n    def __init__(self,file_map=None,chunksize=None,nrows=None,output_folder=None,sep=',',write_mode='w',write_separate=False,**kwargs):\n        super().__init__(chunksize=chunksize,nrows=nrows)\n\n        self.__output_folder = output_folder\n        self.__separator = sep\n        self.__write_mode = write_mode\n        self.__write_separate = write_separate\n\n        if file_map is not None:\n            self._load_input_files(file_map)\n\n    def get_output_folder(self):\n        return self.__output_folder \n\n    def get_global_ids(self):\n        if not self.__output_folder:\n            return\n\n        files = glob.glob(self.__output_folder+os.path.sep+\"person_ids.*\"+self.get_outfile_extension())\n        return files\n\n    def load_global_ids(self):\n        if self.__write_mode == 'w':\n            return\n\n        files = self.get_global_ids()\n        if not files:\n            return\n\n        self.logger.warning(f\"Loading existing person ids from...\")\n        self.logger.warning(f\"{files}\")\n        return pd.concat([pd.read_csv(fname,sep=self.__separator).set_index('TARGET_SUBJECT')['SOURCE_SUBJECT']\n                          for fname in files\n        ]).to_dict()\n\n    def get_separator(self):\n        return self.__separator\n\n    def get_outfile_extension(self):\n        \"\"\"\n        Work out what the extension of the output file for the dataframes should be.\n\n        Given the '_outfile_separator' to be used in `df.to_csv`,\n        work out the file extension.\n\n        At current, only tab separated and comma separated values (files) are supported\n\n        Returns:\n           str: outfile extension name\n\n        \"\"\"\n        if self.__separator == ',':\n            return 'csv'\n        elif self.__separator == '\\t':\n            return 'tsv'\n        else:\n            self.logger.warning(f\"Don't know what to do with the extension '{self.__separator}' \")\n            self.logger.warning(\"Defaulting to csv\")\n            return 'csv'\n\n\n    def load_meta(self,name='.meta'):\n        f_out = self.__output_folder\n        fname = f\"{f_out}{os.path.sep}{name}.json\"\n        if not os.path.exists(fname):\n            return\n        with open(fname,'r') as f:\n            data = json.load(f)\n            return data\n\n    def load_indexing(self):\n        meta = self.load_meta()\n        if not meta:\n            return\n\n        indexing = {}\n        for _,v in meta.items():\n            v = v['meta']['total_data_processed']\n            for k,n in v.items():\n                if k not in indexing:\n                    indexing[k] = 0\n                indexing[k] += n\n        return indexing\n\n    def write_meta(self,data,name='.meta'):\n        if not isinstance(data,dict):\n            raise NotImplementedError(f\"{type(data)} must be of type dict\")\n\n        data = {hex(id(data)):data}\n\n        mode = self.__write_mode\n        f_out = self.__output_folder\n        if not os.path.exists(f'{f_out}'):\n            self.logger.info(f'making output folder {f_out}')\n            os.makedirs(f'{f_out}')\n\n        fname = f\"{f_out}{os.path.sep}{name}.json\"\n        if os.path.exists(fname) and mode == 'a':\n            with open(fname,'r') as f:\n                existing_data = json.load(f)\n                data = {**existing_data,**data}\n        #rewrite it\n        with open(f\"{f_out}{os.path.sep}{name}.json\",\"w\") as f:\n            json.dump(data,f,indent=6)\n            return\n\n    def write(self,name,df,mode='w'):\n\n        f_out = self.__output_folder\n        if not os.path.exists(f'{f_out}'):\n            self.logger.info(f'making output folder {f_out}')\n            os.makedirs(f'{f_out}')\n\n        if mode == None:\n            mode = self.__write_mode\n\n        if self.__write_separate:\n            time = strftime(\"%Y-%m-%dT%H%M%S\", gmtime())\n            if 'name' in df.attrs:\n                name = name + '.' + df.attrs['name']\n            name = name + \".\"+ hex(id(df)) + \".\" + time\n            mode = 'w'\n\n\n\n        file_extension = self.get_outfile_extension()\n        fname = f'{f_out}{os.path.sep}{name}.{file_extension}'\n        #force mode to write if the file doesnt exist yet\n        if not os.path.exists(fname):\n            mode = 'w'\n\n        header=True\n        if mode == 'a':\n            header = False\n        if mode == 'w':\n            self.logger.info(f'saving {name} to {fname}')\n        else:\n            self.logger.info(f'updating {name} in {fname}')\n\n        for col in df.columns:\n            if col.endswith(\"_id\"):\n                df[col] = df[col].astype(float).astype(pd.Int64Dtype())\n\n        df.set_index(df.columns[0],inplace=True)\n        self.logger.debug(df.dtypes)\n        df.to_csv(fname,mode=mode,header=header,index=True,sep=self.__separator)\n\n        self.logger.debug(df.dropna(axis=1,how='all'))\n        self.logger.info(\"finished save to file\")\n        return fname\n\n    def _load_input_files(self,file_map):\n        for name,path in file_map.items():\n            df = pd.read_csv(path,\n                             chunksize=self.chunksize,\n                             nrows=self.nrows,\n                             dtype=str)\n            self[name] = DataBrick(df)\n\n    def load_input_dataframe(self,file_map):\n        for name,df in file_map.items():\n            self[name] = DataBrick(df)\n</code></pre> <p>               Bases: <code>DataCollection</code></p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/plugins/sql.py</code> <pre><code>class SqlDataCollection(DataCollection):\n    def __init__(self,connection_string,chunksize=None,nrows=None,write_mode='r',drop_existing=False,**kwargs):\n        super().__init__(chunksize=chunksize)\n\n        #default mode is 'r' aka replace\n        self.__write_mode =  write_mode\n\n        self.engine = create_engine(connection_string)\n\n        if drop_existing and database_exists(self.engine.url):\n            self.drop_database()\n\n        if not database_exists(self.engine.url):\n            create_database(self.engine.url)\n\n        self.logger.info(self.engine)\n        #get the names of existing tables\n        self.build()\n\n    def drop_database(self):\n        drop_database(self.engine.url)\n\n    def reset(self):\n        self.build()\n        self.__df = None\n        self.__end = False\n        return True\n\n    def build(self):\n        insp  = inspect(self.engine)\n        chunksize = self.chunksize\n        if chunksize == None:\n            chunksize = 1e6\n\n        self.existing_tables = insp.get_table_names()\n        for table in self.existing_tables:\n            df_handler = pd.read_sql(table,self.engine,chunksize=chunksize)\n            b = DataBrick(df_handler,name=table)\n\n            #if table in self.keys():\n            #    del self[table]\n\n            self[table] = b\n\n    def write(self,name,df,mode='w'):\n        #set the method of pandas based on the mode supplied\n\n        if mode == None:\n            mode = self.__write_mode\n\n        if mode == 'w':\n            #write mode we probably mean r mode, r = replace (rather than read)\n            mode = 'r'\n\n        if mode == 'a':\n            if_exists = 'append'\n        elif mode == 'r':\n            if_exists = 'replace'\n        elif mode == 'ww':\n            if_exists = 'fail'\n        else:\n            self.logger.error(f\"cant write {name} \")\n            raise Exception(f\"Unknown mode for dumping to sql, mode = '{mode}'\")\n\n        #check if the table exists already\n        table_exists = name in self.existing_tables\n\n        #index the dataframe\n        pk = df.columns[0]\n        df.set_index(pk,inplace=True)\n        self.logger.info(f'updating {name} in {self.engine}')\n\n        #check if the table already exists in the psql database\n        if table_exists and mode == 'a':\n            #get the last row\n            last_row_existing = pd.read_sql(f\"select {pk} from {name} \"\n                                            f\"order by {pk} desc limit 1\",\n                                                self.engine)\n\n            #if there's already a row and the mode is set to append\n            if len(last_row_existing) &gt; 0:\n                #get the cell value of the (this will be the id, e.g. condition_occurrence_id)\n                last_pk_existing = last_row_existing.iloc[0,0]\n                #get the index integer of this current dataframe\n                first_pk_new = df.index[0]\n                #workout and increase the indexing so the indexes are new\n                index_diff = last_pk_existing - first_pk_new\n                if index_diff &gt;= 0:\n                    self.logger.info(\"increasing index as already exists in psql\")\n                    df.index += index_diff + 1\n\n        #dump to sql\n        df.to_sql(name, self.engine,if_exists=if_exists) \n\n        self.logger.info(\"finished save to psql\")\n</code></pre> <p>               Bases: <code>LocalDataCollection</code></p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/plugins/bclink.py</code> <pre><code>class BCLinkDataCollection(LocalDataCollection):\n    def __init__(self,bclink_settings,**kwargs):\n\n        self.logger.info('setup bclink collection')\n        self.bclink_helpers = BCLinkHelpers(**bclink_settings)\n\n        super().__init__(**kwargs)\n        self.job_ids = []\n\n    def finalise(self):\n        self.logger.info(\"finalising, waiting for jobs to finish\")\n        self.logger.info(f\"job_ids to wait for: {self.job_ids}\")\n\n        #print (self.get_output_folder())\n\n        running_jobs = self.job_ids\n        while True:\n            running_jobs = [j for j in running_jobs if not self.bclink_helpers.check_logs(j)]\n            if len(running_jobs)==0:\n                break\n            self.logger.info(f\"Waiting for {running_jobs} to finish\")\n            time.sleep(5)\n\n        self.logger.info(f\"done!\")\n\n    def retrieve(self):\n        tables = self.bclink_helpers.get_table_map()\n        for name in tables:\n            df = self.bclink_helpers.get_table(name)\n            b = DataBrick(df,name=name)\n            self[name] = b\n\n\n    def write(self,*args,**kwargs):\n        f_out = super().write(*args,**kwargs)\n        destination_table = args[0]\n        self.load(f_out,destination_table)\n\n    def load(self,f_out,destination_table):\n        job_id = self.bclink_helpers.load_table(f_out,destination_table)\n        if job_id:\n            self.job_ids.append(job_id)\n\n    def load_indexing(self):\n        indexer = self.bclink_helpers.get_indicies()\n        if indexer:\n            self.logger.info(f'retrieved {indexer}')\n        return indexer\n\n    def load_global_ids(self):\n        data = self.bclink_helpers.get_global_ids()\n        if not data:\n            return\n        if len(data.splitlines()) == 0:\n            return\n\n        sep = self.get_separator()\n        data = io.StringIO(data)\n        df_ids = pd.read_csv(data,\n                             sep=sep).set_index('TARGET_SUBJECT')['SOURCE_SUBJECT']\n        return df_ids.to_dict()\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/common.py</code> <pre><code>class DataBrick:\n    def __init__(self,df_handler,name=None):\n        self.name = name\n        self.__df_handler = df_handler\n        self.__df = None\n        self.__end = False\n        self.__is_init = False\n\n    def get_handler(self):\n        return self.__df_handler\n\n    def is_finished(self):\n        return self.__end\n\n    def set_finished(self,value):\n        self.__end = value\n\n    def is_init(self):\n        return self.__is_init\n\n    def set_init(self,value):\n        self.__is_init = value\n\n    def reset(self):\n        if isinstance(self.__df_handler,pd.io.parsers.TextFileReader):\n            options = self.__df_handler.orig_options\n            f = self.__df_handler.f\n            del self.__df_handler\n            options['engine'] = 'c'\n            if isinstance(f,io.StringIO):\n                f.seek(0)\n            self.__df_handler = pd.io.parsers.TextFileReader(f,**options)\n\n        self.__df = None\n        self.__end = False\n        self.__is_init = False\n        return True\n\n    def get_chunk(self,chunksize):\n        if self.__end == True:\n            return\n        #if the df handler is a TextFileReader, get a dataframe chunk\n        if isinstance(self.__df_handler,pd.io.parsers.TextFileReader):\n            try:\n                #for this file reader, get the next chunk of data\n                self.__df = self.__df_handler.get_chunk(chunksize)\n            except StopIteration:#,ValueError):\n                #otherwise, if at the end of the file reader, return an empty frame\n                self.__df = pd.DataFrame(columns=self.__df.columns) if self.__df is not None else None\n                self.__end = True\n        elif isinstance(self.__df_handler,pd.DataFrame):\n            #if we're handling non-chunked data\n            if self.__df is not None:\n                #return an empty dataframe if we've already loaded this dataframe\n                self.__df = pd.DataFrame(columns=self.__df.columns)\n            else:\n                #otherwise return the dataframe as it's the first time we're getting it\n                self.__df = self.__df_handler\n            self.__end = True\n        elif isinstance(self.__df_handler, GeneratorType):\n            try:\n                self.__df = next(self.__df_handler)\n            except StopIteration:\n                self.__df = pd.DataFrame(columns=self.__df.columns) if self.__df is not None else None\n                self.__end = True\n        else:\n            raise NotImplementedError(f\"{type(self.__df_handler)} not implemented\")\n\n    def get_df(self):\n        return self.__df\n</code></pre>"},{"location":"CaRROT-CDM/DataCollection/#carrot.io.LocalDataCollection.get_outfile_extension","title":"<code>get_outfile_extension()</code>","text":"<p>Work out what the extension of the output file for the dataframes should be.</p> <p>Given the '_outfile_separator' to be used in <code>df.to_csv</code>, work out the file extension.</p> <p>At current, only tab separated and comma separated values (files) are supported</p> <p>Returns:</p> Name Type Description <code>str</code> <p>outfile extension name</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/io/plugins/local.py</code> <pre><code>def get_outfile_extension(self):\n    \"\"\"\n    Work out what the extension of the output file for the dataframes should be.\n\n    Given the '_outfile_separator' to be used in `df.to_csv`,\n    work out the file extension.\n\n    At current, only tab separated and comma separated values (files) are supported\n\n    Returns:\n       str: outfile extension name\n\n    \"\"\"\n    if self.__separator == ',':\n        return 'csv'\n    elif self.__separator == '\\t':\n        return 'tsv'\n    else:\n        self.logger.warning(f\"Don't know what to do with the extension '{self.__separator}' \")\n        self.logger.warning(\"Defaulting to csv\")\n        return 'csv'\n</code></pre>"},{"location":"CaRROT-CDM/DrugExposure/","title":"DrugExposure","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Drug Exposure object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/drug_exposure.py</code> <pre><code>class DrugExposure(DestinationTable):\n    \"\"\"\n    CDM Drug Exposure object class\n    \"\"\"\n\n    name = 'drug_exposure'\n    def __init__(self,name=None):\n        self.drug_exposure_id             = DestinationField(dtype=\"Integer\"   , required=True , pk=True)\n        self.person_id                    = DestinationField(dtype=\"Integer\"   , required=True )\n        self.drug_concept_id              = DestinationField(dtype=\"Integer\"   , required=True )\n        self.drug_exposure_start_date     = DestinationField(dtype=\"Date\"      , required=False )\n        self.drug_exposure_start_datetime = DestinationField(dtype=\"Timestamp\" , required=True )\n        self.drug_exposure_end_date       = DestinationField(dtype=\"Date\"      , required=False )\n        self.drug_exposure_end_datetime   = DestinationField(dtype=\"Timestamp\" , required=False )\n        self.verbatim_end_date            = DestinationField(dtype=\"Date\"      , required=False )\n        self.drug_type_concept_id         = DestinationField(dtype=\"Integer\"   , required=False )\n        self.stop_reason                  = DestinationField(dtype=\"Text20\"    , required=False )\n        self.refills                      = DestinationField(dtype=\"Integer\"   , required=False )\n        self.quantity                     = DestinationField(dtype=\"Float\"     , required=False )\n        self.days_supply                  = DestinationField(dtype=\"Integer\"   , required=False )\n        self.sig                          = DestinationField(dtype=\"Integer\"   , required=False )\n        self.route_concept_id             = DestinationField(dtype=\"Integer\"   , required=False )\n        self.lot_number                   = DestinationField(dtype=\"Text50\"    , required=False )\n        self.provider_id                  = DestinationField(dtype=\"Integer\"   , required=False )\n        self.visit_occurrence_id          = DestinationField(dtype=\"Integer\"   , required=False )\n        self.drug_source_value            = DestinationField(dtype=\"Text50\"    , required=False )\n        self.drug_source_concept_id       = DestinationField(dtype=\"Integer\"   , required=False )\n        self.route_source_value           = DestinationField(dtype=\"Text50\"    , required=False )\n        self.dose_unit_source_value       = DestinationField(dtype=\"Text50\"    , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the drug_exposure objects\n        * drug_concept_id  is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n        if self.automatically_fill_missing_columns == True:\n            if df['drug_exposure_start_date'].isnull().all():\n                df['drug_exposure_start_date'] = self.tools.get_date(df['drug_exposure_start_datetime'])\n\n            if df['drug_exposure_end_date'].isnull().all():\n                df['drug_exposure_end_date'] = self.tools.get_date(df['drug_exposure_end_datetime'])\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/DrugExposure/#carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the drug_exposure objects * drug_concept_id  is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/drug_exposure.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the drug_exposure objects\n    * drug_concept_id  is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n    if self.automatically_fill_missing_columns == True:\n        if df['drug_exposure_start_date'].isnull().all():\n            df['drug_exposure_start_date'] = self.tools.get_date(df['drug_exposure_start_datetime'])\n\n        if df['drug_exposure_end_date'].isnull().all():\n            df['drug_exposure_end_date'] = self.tools.get_date(df['drug_exposure_end_datetime'])\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/FAQ/","title":"FAQ","text":"<p>Before getting started, please take a look a the following frequently asked questions.</p>"},{"location":"CaRROT-CDM/FAQ/#etl","title":"ETL","text":""},{"location":"CaRROT-CDM/FAQ/#what-is-the-carrot-etl-tool","title":"What is the Carrot ETL (Tool)?","text":"<p>Extract, transform and load of your dataset into a Common Data Model (CDM) format that is loaded into bclink via a command line tool.</p> <p>The following command (CLI tool) has the ability to do the full process: <pre><code>carrot etl --help\n</code></pre> Overview Setting up automation </p> <p>Otherwise, you have to do each step manually:</p> <p> Extract Transform Load </p>"},{"location":"CaRROT-CDM/FAQ/#how-can-i-pseudonymise-my-data","title":"How can I pseudonymise my data?","text":"<p>When <code>carrot-cdm</code> is installed a standalone package is also installed <code>co-connect-pseudonymise</code> (source code).</p> <p>Note</p> <p>The pseudonymisation script (<code>co-connect-pseudonymise</code>) can be install separately if you want to run the pseudonymisation in a separate environment and don't want to install all the addditonal packages that <code>carrot-cdm</code> will install.  </p> <p>A CLI is setup called <code>pseudonymise</code> which can be run on <code>csv</code> files: <pre><code>$ pseudonymise csv --help\nUsage: pseudonymise csv [OPTIONS] INPUT\n\n  Command to pseudonymise csv files, given a salt, the name of the columns to\n  pseudonymise and the input file.\n\nOptions:\n  -s, --salt TEXT           salt hash  [required]\n  -c, --column, --id TEXT   name of the identifier columns  [required]\n  -o, --output-folder TEXT  path of the output folder  [required]\n  --chunksize INTEGER       set the chunksize when loading data, useful for\n                            large files\n  --help                    Show this message and exit.\n</code></pre></p> <p>A detailed guide on how to use this feature can be found here:</p> <p> Pseudonymisation guide </p>"},{"location":"CaRROT-CDM/FAQ/#can-i-pseudonymise-the-data-myself","title":"Can I pseudonymise the data myself?","text":"<p>Yes. You do not need to use the tool packaged in carrot-cdm, you can do this process yourself to create a masked for the person (study) identifiers in your dataset.</p>"},{"location":"CaRROT-CDM/FAQ/#transforming-mapping-a-dataset","title":"Transforming (mapping) a dataset","text":""},{"location":"CaRROT-CDM/FAQ/#what-is-the-transform-tool","title":"What is the transform tool?","text":"<p>Our command line interface tool for performing only the 'T' part of the 'ETL' process.. a transformation of your dataset into a Common Data Model (CDM):</p> With commandline argumentsUsing a configuration file <p>Alternatively, using commandline arguments, the tool can be run as: <pre><code>carrot run map --help\n</code></pre></p> <p>If the input data and rules file are specified in a <code>yaml</code> configuration file, and no load section is specified, the <code>etl</code> command can be used: <pre><code>carrot etl --config &lt;config&gt; \n</code></pre></p> <p> Overview Manual from the Command Line Manual from a GUI </p>"},{"location":"CaRROT-CDM/FAQ/#what-is-the-rules-json","title":"What is the rules <code>.json</code>?","text":"<p>A <code>json</code> encoded file that contains information of how multiple CDM tables and CDM objects need to be created by the transform process</p> <p> Rules JSON </p>"},{"location":"CaRROT-CDM/FAQ/#what-should-my-input-files-be-called","title":"What should my input files be called?","text":"<p>The rules <code>.json</code> file can be called anything.</p> <p>Your input files need to be <code>csv</code> files. If the mapping instructions have been followed correctly, the names encoded in the WhiteRabbit scan report should match the names of your input files, and therefore match the names in the <code>.json</code> file.</p> <p>The helper command: <pre><code>carrot display rules json &lt;rules file&gt; --list-tables\n</code></pre> can be used to display what input file names the tool will be trying to load. </p> <p>Example</p> <p><pre><code>carrot display rules json $(carrot info data_folder)/test/rules/rules_14June2021.json --list-tables\n</code></pre> <pre><code>  [\n     \"Demographics.csv\",\n     \"Symptoms.csv\",\n     \"covid19_antibody.csv\"\n  ]\n</code></pre></p> <p>Tip</p> <p>If these names differ from the names of your input files, you'll have to rename the files or manually edit the <code>.json</code> files and replace all occurrences of a bad file name with the correct file name.</p>"},{"location":"CaRROT-CDM/FAQ/#what-should-i-pass-as-inputs","title":"What should I pass as <code>INPUTS</code>?","text":"<p>You need to pass a space separated list of files (or directories containing <code>.csv</code> files). </p> <p>The rules <code>.json</code> file is supplied via the option <code>--rules</code>.</p> <p>The <code>--help</code> message displays the following syntax for how the <code>map</code> tool (for transform) can be run:</p> <p><pre><code>carrot run map --help\n</code></pre> <pre><code>Usage: carrot run map [OPTIONS] [INPUTS]...\n\n  Perform OMOP Mapping given an json file and a series of input files\n\n  INPUTS should be a space separated list of individual input files or\n  directories (which contain .csv files)\n\nOptions:\n  --rules TEXT                    input json file containing all the mapping\n                                  rules to be applied  [required]\n...\n</code></pre></p>"},{"location":"CaRROT-CDM/FAQ/#how-can-i-install-the-software-offline-no-internet-connection","title":"How can I install the software offline (no internet connection)?","text":"<p>You need to download the source code and its dependencies, somewhere which has a stable internet connection.</p>"},{"location":"CaRROT-CDM/FAQ/#download-wheel-files","title":"Download wheel files","text":"<ul> <li>Make a new directory</li> <li>Next execute the command <code>python3 -m pip download carrot-cdm</code>: <pre><code>$ python3 -m pip download carrot-cdm\nCollecting carrot-cdm\n  Downloading co_connect_tools-0.4.13-py3-none-any.whl (300 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300 kB 3.4 MB/s            \nCollecting tabulate\n  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\nCollecting requests\n  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\nCollecting openpyxl\n  Using cached openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n</code></pre></li> </ul> <p>Which will download <code>.whl</code> files into your directory: <pre><code>$ ls \nJinja2-3.0.3-py3-none-any.whl\nMarkupSafe-2.0.1-cp38-cp38-macosx_10_9_x86_64.whl\nPySimpleGUI-4.55.1-py3-none-any.whl\nPyYAML-6.0-cp38-cp38-macosx_10_9_x86_64.whl\n...\n</code></pre></p>"},{"location":"CaRROT-CDM/FAQ/#transfer-the-wheel-files","title":"Transfer the wheel files","text":"<p>Now you can zip and move this folder to a location/machine that you want to install carrot-cdm, one that doesn't have an internet connection to pypi.org.</p> <ul> <li>In this new environment, create a new working directory and setup a virtual environment</li> </ul> <p>Warning</p> <p>this new environment should be the same architecture, i.e. if you download on a Windows environment and then try to install on MacOS, this won't work.</p> <ul> <li>Copy/unzip the .whl files into a new folder, e.g. <code>deps/</code></li> </ul> <pre><code>python3 -m venv .\nsource bin/activate\n</code></pre>"},{"location":"CaRROT-CDM/FAQ/#install-the-package-offline","title":"Install the package offline","text":"<p>You can use <code>pip</code> to install  <pre><code>pip install --no-index --find-links &lt;path to folder containing .whl files&gt; carrot-cdm\n</code></pre></p> <p>Warning</p> <p>Dependening on your python version, you made need a recent version of pip. You can download the latest version of pip and tranfer the .whl file to this offline environment, and install the update with: <pre><code>pip install pip-21.3.1-py3-none-any.whl\n</code></pre></p> <p>Example: <pre><code>$ pip install --no-index --find-links . carrot-cdm\nLooking in links: .\n...\n...\nProcessing ./click-8.0.3-py3-none-any.whl\nProcessing ./Jinja2-3.0.3-py3-none-any.whl\n...\n...\nSuccessfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 blessed-1.19.0 certifi-2021.10.8 charset-normalizer-2.0.9 click-8.0.3 carrot-cdm-0.6.2 coloredlogs-15.0.1 docutils-0.18.1 et-xmlfile-1.1.0 graphviz-0.19.1 humanfriendly-10.0 idna-3.3 inquirer-2.8.0 lockfile-0.12.2 numpy-1.21.4 openpyxl-3.0.9 pandas-1.3.5 psutil-5.8.0 pysimplegui-4.55.1 python-daemon-2.3.0 python-dateutil-2.8.2 python-editor-1.0.4 pytz-2021.3 pyyaml-6.0 readchar-2.0.1 requests-2.26.0 six-1.16.0 tabulate-0.8.9 urllib3-1.26.7 wcwidth-0.2.5\n</code></pre></p> <p>Check if the tool installed offline, and you're good to go <pre><code>carrot --version\n</code></pre> Outputs: <pre><code>0.6.2\n</code></pre></p>"},{"location":"CaRROT-CDM/FAQ/#how-do-i-install-using-conda","title":"How do I install using conda?","text":"<p>Create your virtual environment (with conda python&gt;=3.6) <pre><code>conda create --name myenv \n</code></pre> Activate it: <pre><code>conda activate myenv\n</code></pre></p> <p>We could recommned that you just install pip in your virtual enviroment at this point and continue with the normal install instructions, i.e: <pre><code>conda install pip\n</code></pre> followed by: <pre><code>pip install carrot-cdm\n</code></pre></p>"},{"location":"CaRROT-CDM/FileHelpers/","title":"FileHelpers","text":"Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def load_csv(_map,chunksize=None,\n             dtype=str,nrows=None,\n             lower_col_names=False,\n             load_path=\"\",\n             rules=None,\n             sep=',',\n             na_values=['']):\n\n    if isinstance(_map,list):\n        _map = {\n            os.path.basename(x):x\n            for x in _map\n        }\n\n    logger = Logger(\"carrot.tools.load_csv\")\n\n    if isinstance(_map,list) or isinstance(_map,tuple):\n        _map = { x:x for x in _map}\n    elif isinstance(_map,str):\n        _map = { _map:_map }\n\n    if rules is not None:\n        logger.debug(\"rules .json file supplied\")\n        if not isinstance(rules,dict):\n            rules = load_json(rules)\n\n        inputs_from_cli = list(_map.keys())\n\n        source_map = get_mapped_fields_from_rules(rules)\n        inputs_from_json = list(source_map.keys())\n\n        if len(inputs_from_cli) == 0:\n            raise MissingInputFiles (f\"You haven't loaded any input files!\")\n\n        logger.debug(f\"{len(inputs_from_cli)} input files loaded\")\n        logger.debug(f\"{inputs_from_cli}\")\n\n        missing_inputs = list(set(inputs_from_json) - set(inputs_from_cli))\n        if len(missing_inputs) &gt; 0 :\n            raise MissingInputFiles (f\"Found the following files {missing_inputs} in the json file, that are not in the loaded file list... {inputs_from_cli}\")\n\n        #reduce the mapping of inputs, if we dont need them all\n        _map = {\n            k: {\n                'file':v,\n                'fields':source_map[k]\n            }\n            for k,v in _map.items()\n            if k in source_map\n        }\n\n\n    if not nrows is None:\n        chunksize = nrows if chunksize is None else chunksize\n\n    retval = io.LocalDataCollection(chunksize=chunksize)\n\n    for key,obj in _map.items():\n        fields = None\n        if isinstance(obj,str):\n            fname = obj\n        else:\n            fname = obj['file']\n            fields = obj['fields']\n\n        df = pd.read_csv(load_path+fname,\n                         chunksize=chunksize,\n                         #iterator=True,\n                         nrows=nrows,\n                         sep=sep,\n                         keep_default_na=False,\n                         na_values=na_values,\n                         dtype=dtype,\n                         usecols=fields)\n\n        df.attrs = {'original_file':load_path+fname}\n\n\n        if isinstance(df,pd.DataFrame):\n            #this should be removed\n            if lower_col_names:\n                df.columns = df.columns.str.lower()\n\n        retval[key] = io.DataBrick(df,name=key)\n\n    return retval\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def load_json(f_in):\n\n    if os.path.exists(f_in):\n        data = json.load(open(f_in))\n    else:\n        try:\n            data = json.loads(f_in)\n        except Exception as err:\n            raise FileNotFoundError(f\"{f_in} not found. Or cannot parse as json\")\n\n    return data\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def get_file_map_from_dir(_dir,ext='.csv'):\n    if not os.path.isdir(_dir):\n        _dir = os.path.abspath(\n            os.path.join(\n                os.path.dirname(__file__),'..','data',_dir)\n        )\n\n    _map = {}\n    for fname in glob.glob(f\"{_dir}{os.path.sep}*{ext}\"):\n        key = os.path.basename(fname)\n        _map[key] = fname\n\n    return _map\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def get_mapped_fields_from_rules(rules):\n    #extract a tuple of source tables and source fields\n    sources = [\n        (x['source_table'],x['source_field'])\n        for cdm_obj_set in rules['cdm'].values()\n        for cdm_obj in cdm_obj_set.values()\n        for x in cdm_obj.values()\n    ]\n\n    source_map = {}\n    for (table,field) in sources:\n        if table not in source_map:\n            source_map[table] = []\n        source_map[table].append(field)\n\n    source_map = {\n        k:list(set(v))\n        for k,v in source_map.items()\n    }\n\n    return source_map\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def get_separator_from_filename(fname):\n    _, fileExtension = os.path.splitext(fname)\n    if fileExtension == '.tsv':\n        return '\\t'\n    else:\n        return ','\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/file_helpers.py</code> <pre><code>def diff_csv(file1,file2,separator=None,nrows=None):\n    logger = Logger(\"CSV File Diff\")\n\n    if separator == None:\n        sep1 = get_separator_from_filename(file1)\n        sep2 = get_separator_from_filename(file2)\n    else:\n        sep1 = separator\n        sep2 = separator\n\n    df1 = pd.read_csv(file1,sep=sep1,nrows=nrows)\n    df2 = pd.read_csv(file2,sep=sep2,nrows=nrows)\n\n    exact_match = df1.equals(df2)\n    if exact_match:\n        return\n\n    df = pd.concat([df1,df2]).drop_duplicates(keep=False)\n\n    if len(df) &gt; 0:\n        logger.error(\" ======== Differing Rows ========== \")\n        logger.error(df)\n        m = df1.merge(df2, on=df.columns[0], how='outer', suffixes=['', '_'], indicator=True)[['_merge']]\n        m = m[~m['_merge'].str.contains('both')]\n        file1 = file1.split('/')[-1]\n        file2 = file2.split('/')[-1]\n\n        m['_merge'] = m['_merge'].map({'left_only':file1,'right_only':file2})\n        m = m.rename(columns={'_merge':'Only Contained Within'})\n        m.index.name = 'Row Number'\n        logger.error(m.reset_index().to_dict(orient='records'))\n        raise DifferingRows(\"Something not right with the rows, changes detected.\")\n\n    elif len(df1.columns) != len(df2.columns):\n\n        raise DifferingColumns('in df1 but not df2',list(set(df1.columns) - set(df2.columns)),'\\n',\n                               'in df2 but not df1',list(set(df2.columns) - set(df1.columns)))\n\n    else:\n        logger.error(\" ======= Rows are likely in a different order ====== \")\n        for i in range(len(df1)):\n            if not (df1.iloc[i] == df2.iloc[i]).any():\n                print ('Row',i,'is in a different location')\n        raise Exception(\"differences detected\")\n</code></pre>"},{"location":"CaRROT-CDM/Installing/","title":"Installing","text":"<p>Caution</p> <p>This tool is only stable with python versions <code>&gt;=3.6</code> on Unix distributions (macOS, Ubuntu, Centos7) and on Windows. </p> <p>The carrot-cdm package is available to be installed via pip. The latest tagged version of the tool can be found here. See the next section on how to upgrade the package to the latest version.</p> <p>Tip</p> <p>It is recommended that you setup your own virtual python environment, for example using <code>venv</code>. <pre><code>python3 -m venv  .\nsource bin/activate\n</code></pre></p> <p>Tip</p> <p>When installing via <code>pip</code>, you should remember to update <code>pip</code> first, otherwise the install can hang: <pre><code>python -m pip install --upgrade pip\n</code></pre></p> <p>To install the package via pip you should execute the command: <pre><code>python -m pip install carrot-cdm\n</code></pre></p> <p>Otherwise, to ensure the right version of python (python3) is used to install (as you may have installed <code>python2</code> and <code>python3</code>) from pip you can also do: <pre><code>python3 -m pip install carrot-cdm\n</code></pre></p> <p>Tip</p> <p>If you are struggling to install from <code>pip</code> due to lack of root permissions, or you are not using a virtual python environment (e.g. conda), you can install as a user with the command: <pre><code>python3 -m pip install carrot-cdm --user\n</code></pre> This will install the package into a local user folder.</p> <p>Tip</p> <p>If you have trouble with <code>pip</code> hanging on installing dependencies, try to install using the argument <code>--no-cache-dir</code>. Also make sure that you have updated <code>pip</code> via <code>pip3 install --upgrade pip</code>.</p>"},{"location":"CaRROT-CDM/Installing/#upgrading","title":"Upgrading","text":"<p>To upgrade the tool to the latest version, run: <pre><code>python -m pip install carrot-cdm --upgrade\n</code></pre> Otherwise, to be sure, uninstall the package and reinstall it.</p> <p>Tip</p> <p>Running the command: <pre><code>carrot info version\n</code></pre> will display the installed verison of the tool.</p>"},{"location":"CaRROT-CDM/Installing/#installing-from-source","title":"Installing from Source","text":"<p>Alternatively you can download the source code, unpack and install as a local package: <pre><code>cd &lt;downloaded_source_code_folder&gt;\npython3 -m pip install -e . \n</code></pre></p>"},{"location":"CaRROT-CDM/Installing/#verifying","title":"Verifying","text":"<p>To verify the package is installed you can test the following information commands:</p> <ul> <li>show the tool version (this may take a few seconds, the first time you run the package)</li> </ul> <p><pre><code>carrot info version\n</code></pre> Alternatively... <pre><code>carrot --version\n</code></pre></p> <ul> <li>show the location of the installation folder  </li> </ul> <pre><code>carrot info install_folder\n</code></pre>"},{"location":"CaRROT-CDM/Logger/","title":"Logger","text":"Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/logger.py</code> <pre><code>class Logger():\n    @property\n    def logger(self):\n        return _Logger(type(self).__name__)\n</code></pre>"},{"location":"CaRROT-CDM/Measurement/","title":"Measurement","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Measurement object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/measurement.py</code> <pre><code>class Measurement(DestinationTable):\n    \"\"\"\n    CDM Measurement object class\n    \"\"\"\n\n    name = 'measurement'\n    def __init__(self,name=None):\n        self.measurement_id                = DestinationField(dtype=\"Integer\"   , required=True  , pk=True)\n        self.person_id                     = DestinationField(dtype=\"Integer\"   , required=True )\n        self.measurement_concept_id        = DestinationField(dtype=\"Integer\"   , required=True )\n        self.measurement_date              = DestinationField(dtype=\"Date\"      , required=False )\n        self.measurement_datetime          = DestinationField(dtype=\"Timestamp\" , required=True )\n        self.measurement_type_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.operator_concept_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.value_as_number               = DestinationField(dtype=\"Float\"     , required=False )\n        self.value_as_concept_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.unit_concept_id               = DestinationField(dtype=\"Integer\"   , required=False )\n        self.range_low                     = DestinationField(dtype=\"Float\"     , required=False )\n        self.range_high                    = DestinationField(dtype=\"Float\"     , required=False )\n        self.provider_id                   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.visit_occurrence_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.measurement_source_value      = DestinationField(dtype=\"Text50\"    , required=False )\n        self.measurement_source_concept_id = DestinationField(dtype=\"Integer\"   , required=False )\n        self.unit_source_value             = DestinationField(dtype=\"Text50\"    , required=False )\n        self.value_source_value            = DestinationField(dtype=\"Text50\"    , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the measurement objects\n        * measurement_concept_id is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n        if self.automatically_fill_missing_columns == True:\n            if df['measurement_date'].isnull().all():\n                df['measurement_date'] = self.tools.get_date(df['measurement_datetime'])\n            if df['value_as_number'].isnull().all():\n                df['value_as_number'] = pd.to_numeric(df['measurement_source_value'],errors='coerce').astype('Float64')\n\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/Measurement/#carrot.cdm.objects.versions.v5_3_1.measurement.Measurement.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the measurement objects * measurement_concept_id is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/measurement.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the measurement objects\n    * measurement_concept_id is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n    if self.automatically_fill_missing_columns == True:\n        if df['measurement_date'].isnull().all():\n            df['measurement_date'] = self.tools.get_date(df['measurement_datetime'])\n        if df['value_as_number'].isnull().all():\n            df['value_as_number'] = pd.to_numeric(df['measurement_source_value'],errors='coerce').astype('Float64')\n\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/Observation/","title":"Observation","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Observation object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/observation.py</code> <pre><code>class Observation(DestinationTable):\n    \"\"\"\n    CDM Observation object class\n    \"\"\"\n\n    name = 'observation'\n    def __init__(self,name=None):\n        self.observation_id                = DestinationField(dtype=\"Integer\"   , required=True , pk=True)\n        self.person_id                     = DestinationField(dtype=\"Integer\"   , required=True )\n        self.observation_concept_id        = DestinationField(dtype=\"Integer\"   , required=True )\n        self.observation_date              = DestinationField(dtype=\"Date\"      , required=False )\n        self.observation_datetime          = DestinationField(dtype=\"Timestamp\" , required=True )\n        self.observation_type_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.value_as_number               = DestinationField(dtype=\"Float\"     , required=False )\n        self.value_as_string               = DestinationField(dtype=\"Text60\"    , required=False )\n        self.value_as_concept_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.qualifier_concept_id          = DestinationField(dtype=\"Integer\"   , required=False )\n        self.unit_concept_id               = DestinationField(dtype=\"Integer\"   , required=False )\n        self.provider_id                   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.visit_occurrence_id           = DestinationField(dtype=\"Integer\"   , required=False )\n        self.observation_source_value      = DestinationField(dtype=\"Text50\"    , required=False )\n        self.observation_source_concept_id = DestinationField(dtype=\"Integer\"   , required=False )\n        self.unit_source_value             = DestinationField(dtype=\"Text50\"    , required=False )\n        self.qualifier_source_value        = DestinationField(dtype=\"Text50\"    , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the observation objects\n        * observation_concept_id is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n        if self.automatically_fill_missing_columns == True:\n            if df['observation_date'].isnull().all():\n                df['observation_date'] = self.tools.get_date(df['observation_datetime'])\n\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/Observation/#carrot.cdm.objects.versions.v5_3_1.observation.Observation.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the observation objects * observation_concept_id is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/observation.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the observation objects\n    * observation_concept_id is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n    if self.automatically_fill_missing_columns == True:\n        if df['observation_date'].isnull().all():\n            df['observation_date'] = self.tools.get_date(df['observation_datetime'])\n\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/Person/","title":"Person","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Person object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/person.py</code> <pre><code>class Person(DestinationTable):\n    \"\"\"\n    CDM Person object class\n    \"\"\"\n    name = 'person'\n    def __init__(self,name=None):\n        self.person_id                   = DestinationField(dtype=\"Integer\"   , required=True , pk=True)\n        self.gender_concept_id           = DestinationField(dtype=\"Integer\"   , required=True )\n        self.year_of_birth               = DestinationField(dtype=\"Integer\"   , required=False )\n        self.month_of_birth              = DestinationField(dtype=\"Integer\"   , required=False )\n        self.day_of_birth                = DestinationField(dtype=\"Integer\"   , required=False )\n        self.birth_datetime              = DestinationField(dtype=\"Timestamp\" , required=True )\n        self.race_concept_id             = DestinationField(dtype=\"Integer\"   , required=False )\n        self.ethnicity_concept_id        = DestinationField(dtype=\"Integer\"   , required=False )\n        self.location_id                 = DestinationField(dtype=\"Integer\"   , required=False )\n        self.provider_id                 = DestinationField(dtype=\"Integer\"   , required=False )\n        self.care_site_id                = DestinationField(dtype=\"Integer\"   , required=False )\n        self.person_source_value         = DestinationField(dtype=\"Text50\"    , required=False )\n        self.gender_source_value         = DestinationField(dtype=\"Text50\"    , required=False )\n        self.gender_source_concept_id    = DestinationField(dtype=\"Integer\"   , required=False )\n        self.race_source_value           = DestinationField(dtype=\"Text50\"    , required=False )\n        self.race_source_concept_id      = DestinationField(dtype=\"Integer\"   , required=False )\n        self.ethnicity_source_value      = DestinationField(dtype=\"Text50\"    , required=False )\n        self.ethnicity_source_concept_id = DestinationField(dtype=\"Integer\"   , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n    def finalise(self,df,**kwargs):\n        \"\"\"\n        Overload the finalise function here for any specifics for the person table\n        \"\"\"\n        df = super().finalise(df,**kwargs)\n        return df\n\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the person objects\n        * year_of_birth is automatically converted to a year (int)\n        * month_of_birth is automatically converted to a month (int)\n        * day_of_birth is automatically converted to a day (int)\n        * birth_datetime is automatically coverted to a datatime\n\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n        df = super().get_df(**kwargs)\n        if self.automatically_fill_missing_columns == True:\n            if df['year_of_birth'].isnull().all():\n                df['year_of_birth'] = self.tools.get_year(df['birth_datetime'])\n\n            if df['month_of_birth'].isnull().all():\n                df['month_of_birth'] = self.tools.get_month(df['birth_datetime'])\n\n            if df['day_of_birth'].isnull().all():\n                df['day_of_birth'] = self.tools.get_day(df['birth_datetime'])\n\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/Person/#carrot.cdm.objects.versions.v5_3_1.person.Person.finalise","title":"<code>finalise(df, **kwargs)</code>","text":"<p>Overload the finalise function here for any specifics for the person table</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/person.py</code> <pre><code>def finalise(self,df,**kwargs):\n    \"\"\"\n    Overload the finalise function here for any specifics for the person table\n    \"\"\"\n    df = super().finalise(df,**kwargs)\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/Person/#carrot.cdm.objects.versions.v5_3_1.person.Person.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the person objects * year_of_birth is automatically converted to a year (int) * month_of_birth is automatically converted to a month (int) * day_of_birth is automatically converted to a day (int) * birth_datetime is automatically coverted to a datatime</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/person.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the person objects\n    * year_of_birth is automatically converted to a year (int)\n    * month_of_birth is automatically converted to a month (int)\n    * day_of_birth is automatically converted to a day (int)\n    * birth_datetime is automatically coverted to a datatime\n\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n    df = super().get_df(**kwargs)\n    if self.automatically_fill_missing_columns == True:\n        if df['year_of_birth'].isnull().all():\n            df['year_of_birth'] = self.tools.get_year(df['birth_datetime'])\n\n        if df['month_of_birth'].isnull().all():\n            df['month_of_birth'] = self.tools.get_month(df['birth_datetime'])\n\n        if df['day_of_birth'].isnull().all():\n            df['day_of_birth'] = self.tools.get_day(df['birth_datetime'])\n\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/ProcedureOccurrence/","title":"ProcedureOccurrence","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Procedure Occurrence object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/procedure_occurrence.py</code> <pre><code>class ProcedureOccurrence(DestinationTable):\n    \"\"\"\n    CDM Procedure Occurrence object class\n    \"\"\"\n\n    name = 'procedure_occurrence'\n    def __init__(self,name=None):\n        self.procedure_occurrence_id     = DestinationField(dtype=\"Integer\"   , required=False , pk=True)\n        self.person_id                   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.procedure_concept_id        = DestinationField(dtype=\"Integer\"   , required=False )\n        self.procedure_date              = DestinationField(dtype=\"Date\"      , required=False )\n        self.procedure_datetime          = DestinationField(dtype=\"Timestamp\" , required=False )\n        self.procedure_type_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.modifier_concept_id         = DestinationField(dtype=\"Integer\"   , required=False )\n        self.quantity                    = DestinationField(dtype=\"Integer\"   , required=False )\n        self.provider_id                 = DestinationField(dtype=\"Integer\"   , required=False )\n        self.visit_occurrence_id         = DestinationField(dtype=\"Integer\"   , required=False )\n        self.procedure_source_value      = DestinationField(dtype=\"Text50\"    , required=False )\n        self.procedure_source_concept_id = DestinationField(dtype=\"Integer\"   , required=False )\n        self.qualifier_source_value      = DestinationField(dtype=\"Text50\"    , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the procedure_occurrence objects\n        * procedure_concept_id is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n\n        if self.automatically_fill_missing_columns == True:\n            if df['procedure_date'].isnull().all():\n                df['procedure_date'] = self.tools.get_date(df['procedure_datetime'])\n\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/ProcedureOccurrence/#carrot.cdm.objects.versions.v5_3_1.procedure_occurrence.ProcedureOccurrence.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the procedure_occurrence objects * procedure_concept_id is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/procedure_occurrence.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the procedure_occurrence objects\n    * procedure_concept_id is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n\n    if self.automatically_fill_missing_columns == True:\n        if df['procedure_date'].isnull().all():\n            df['procedure_date'] = self.tools.get_date(df['procedure_datetime'])\n\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/Profiling/","title":"Profiling","text":"<p>               Bases: <code>Logger</code></p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/profiling.py</code> <pre><code>class Profiler(Logger):\n    def __init__(self,name=None,interval=0.1):\n\n        if name == None:\n            name = self.__class__.__name__\n        else:\n            name = f\"{self.__class__.__name__}_{name}\"\n\n\n        #retrieve the process id for the current run\n        self.pid = os.getpid()\n        #create a psutil instance to montior this\n        self.py = psutil.Process(self.pid)\n        #set the interval (seconds) of how often to check the cpu and memory\n        self.interval = interval\n        self.logger.info(f\"tracking {self.pid} every {self.interval} seconds\")\n        #count the number of cpus the computer running this process has\n        self.cpu_count = psutil.cpu_count()\n        self.logger.info(f\"{self.cpu_count} cpus available\")\n        #initiate a threaded function\n        #that will run in a separate process and can monitor CPU/memory in the background\n        self.th = threading.Thread(target=self.track)\n\n        #init some global variables\n        self.tracking = []\n        self.init_time = time.time()\n        self._stop = False\n        self._df = None\n\n    def start(self):\n        #start the thread\n        self.logger.info(\"starting profiling\")\n        self.th.start()\n\n    def stop(self):\n        #stop the thread\n        self._stop = True\n        self.th.join()\n        self.logger.info(\"finished profiling\")\n\n    def get_df(self):\n        #build a little dataframe for cpu/memory v.s. time,\n        #if it has not been built already\n        if self._df is None:\n            self._df = pd.DataFrame(self.tracking)\n        return self._df\n\n    def summary(self):\n        #print the dataframe created for cpu/memory v.s. time\n        self.logger.info(self.get_df())\n\n    def track(self):\n        \"\"\"\n        Main function to profile CPU and memory usage\n        \"\"\"\n        #while the program has been told to profile the usage\n        while self._stop == False:\n            #from the current process, calculate the current memory usage (in GB)\n            memory = self.py.memory_info()[0]/2.**30\n            #also calculate the CPU % in use at this epoch in time\n            cpu = self.py.cpu_percent() / self.cpu_count\n            #calcuate the current time - time since the start of the process\n            current_time = time.time() - self.init_time\n            #log the data\n            info = {'time[s]':current_time,'memory[GB]':memory,'cpu[%]':cpu}\n            self.tracking.append(info)\n            #sleep the number of seconds requested\n            time.sleep(self.interval)\n\n        #once finished, call the summary function\n        self.summary()\n</code></pre>"},{"location":"CaRROT-CDM/Profiling/#carrot.tools.profiling.Profiler.track","title":"<code>track()</code>","text":"<p>Main function to profile CPU and memory usage</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/profiling.py</code> <pre><code>def track(self):\n    \"\"\"\n    Main function to profile CPU and memory usage\n    \"\"\"\n    #while the program has been told to profile the usage\n    while self._stop == False:\n        #from the current process, calculate the current memory usage (in GB)\n        memory = self.py.memory_info()[0]/2.**30\n        #also calculate the CPU % in use at this epoch in time\n        cpu = self.py.cpu_percent() / self.cpu_count\n        #calcuate the current time - time since the start of the process\n        current_time = time.time() - self.init_time\n        #log the data\n        info = {'time[s]':current_time,'memory[GB]':memory,'cpu[%]':cpu}\n        self.tracking.append(info)\n        #sleep the number of seconds requested\n        time.sleep(self.interval)\n\n    #once finished, call the summary function\n    self.summary()\n</code></pre>"},{"location":"CaRROT-CDM/RuleHelpers/","title":"RuleHelpers","text":"Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/rules_helpers.py</code> <pre><code>def apply_rules(this,rules,inputs=None):\n    this.logger.info(\"Called apply_rules\")\n\n    if inputs is None:\n        inputs = this.inputs\n\n    this._meta['source_files'] = {}\n    for destination_field,rule in rules.items():\n        source_table_name = rule['source_table']\n        source_field_name = rule['source_field']\n        operations = None\n        if 'operations' in rule:\n            operations = rule['operations']\n        term_mapping = None\n        if 'term_mapping' in rule:\n            term_mapping = rule['term_mapping']\n\n        source_table = get_source_table(inputs,source_table_name)\n        source_field = get_source_field(source_table,source_field_name)\n        series = source_field.copy()\n\n        if operations is not None:\n            for operation in operations:\n                function = this.tools[operation]\n                series = function(series)\n\n        if term_mapping is not None:\n            if isinstance(term_mapping,dict):\n                # value level mapping\n                # - term_mapping is a dictionary between values and concepts\n                # - map values in the input data, based on this map\n\n                #need to make the value a string for mapping\n                #pandas has a weird behaviour that when the value is an Int\n                #the resulting series is a float64\n                term_mapping = {k:str(v) for k,v in term_mapping.items()}\n                series = series.map(term_mapping)\n            else:\n                # field level mapping.\n                # - term_mapping is the concept_id\n                # - set all values in this column to it\n                series.values[:] = term_mapping\n\n        this[destination_field].series = series\n        this._meta['source_files'][destination_field] = {'table':source_table_name,'field':source_field_name}\n        this.logger.info(f\"Mapped {destination_field}\")\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/rules_helpers.py</code> <pre><code>def get_source_table(inputs,name):\n    #make a copy of the input data column slice\n    if name not in inputs.keys():\n        short_keys = {key[:31]:key for key in inputs.keys()}\n        if name in short_keys:\n            name = short_keys[name]\n        elif name.lower() in short_keys:\n            name = short_keys[name.lower()]\n        else:\n            raise TableNotFoundError(f\"Cannot find {name} in inputs. Options are {inputs.keys()}\")\n    table = inputs[name]\n    table.name = name\n    return inputs[name]\n</code></pre> Source code in <code>docs/CaRROT-CDM/source_code/carrot/tools/rules_helpers.py</code> <pre><code>def get_source_field(table,name):\n    if name not in table:\n        if name.lower() in table:\n            return table[name.lower()]\n        else:\n            raise FieldNotFoundError(f\"Cannot find {name} in table {table.name}. Options are {table.columns.tolist()}\")\n    return table[name]\n</code></pre>"},{"location":"CaRROT-CDM/Specimen/","title":"Specimen","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Specimen object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/specimen.py</code> <pre><code>class Specimen(DestinationTable):\n    \"\"\"\n    CDM Specimen object class\n    \"\"\"\n\n    name = 'specimen'\n    def __init__(self,name=None):\n\n        self.specimen_id                 = DestinationField(dtype=\"Integer\"   , required=False , pk=True)\n        self.person_id                   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.specimen_concept_id         = DestinationField(dtype=\"Integer\"   , required=False )\n        self.specimen_type_concept_id    = DestinationField(dtype=\"Integer\"   , required=False )\n        self.specimen_date               = DestinationField(dtype=\"Date\"      , required=False )\n        self.specimen_datetime           = DestinationField(dtype=\"Timestamp\" , required=False )\n        self.quantity                    = DestinationField(dtype=\"Float\"     , required=False )\n        self.unit_concept_id             = DestinationField(dtype=\"Integer\"   , required=False )\n        self.anatomic_site_concept_id    = DestinationField(dtype=\"Integer\"   , required=False )\n        self.disease_status_concept_id   = DestinationField(dtype=\"Integer\"   , required=False )\n        self.specimen_source_id          = DestinationField(dtype=\"Text50\"    , required=False )\n        self.specimen_source_value       = DestinationField(dtype=\"Text50\"    , required=False )\n        self.unit_source_value           = DestinationField(dtype=\"Text50\"    , required=False )\n        self.anatomic_site_source_value  = DestinationField(dtype=\"Text50\"    , required=False )\n        self.disease_status_source_value = DestinationField(dtype=\"Text50\"    , required=False )\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n\n    def get_df(self,**kwargs):\n        \"\"\"\n        Overload/append the creation of the dataframe, specifically for the specimen objects\n        * specimen_concept_id is required to be not null\n          this can happen when spawning multiple rows from a person\n          we just want to keep the ones that have actually been filled\n\n        Returns:\n           pandas.Dataframe: output dataframe\n        \"\"\"\n\n        df = super().get_df(**kwargs)\n\n        if self.automatically_fill_missing_columns == True:\n            if df['specimen_date'].isnull().all():\n                df['specimen_date'] = self.tools.get_date(df['specimen_datetime'])\n        return df\n</code></pre>"},{"location":"CaRROT-CDM/Specimen/#carrot.cdm.objects.versions.v5_3_1.specimen.Specimen.get_df","title":"<code>get_df(**kwargs)</code>","text":"<p>Overload/append the creation of the dataframe, specifically for the specimen objects * specimen_concept_id is required to be not null   this can happen when spawning multiple rows from a person   we just want to keep the ones that have actually been filled</p> <p>Returns:</p> Type Description <p>pandas.Dataframe: output dataframe</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/specimen.py</code> <pre><code>def get_df(self,**kwargs):\n    \"\"\"\n    Overload/append the creation of the dataframe, specifically for the specimen objects\n    * specimen_concept_id is required to be not null\n      this can happen when spawning multiple rows from a person\n      we just want to keep the ones that have actually been filled\n\n    Returns:\n       pandas.Dataframe: output dataframe\n    \"\"\"\n\n    df = super().get_df(**kwargs)\n\n    if self.automatically_fill_missing_columns == True:\n        if df['specimen_date'].isnull().all():\n            df['specimen_date'] = self.tools.get_date(df['specimen_datetime'])\n    return df\n</code></pre>"},{"location":"CaRROT-CDM/SyntheticData/","title":"SyntheticData","text":"<p>Generating synthetic data can be performed via WhiteRabbit, by hand or via the use of co-connect-tools.</p>"},{"location":"CaRROT-CDM/SyntheticData/#cli-for-generating-synthetic-data","title":"CLI for generating synthetic data","text":"<p>Synthetic data can be generated from the CCOM website or from the original <code>xlsx</code> ScanReport file.</p> <pre><code>$ carrot generate synthetic --help\nUsage: carrot generate synthetic [OPTIONS] COMMAND [ARGS]...\n\n  Commands to generate synthetic data.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  ccom  generate synthetic data from a ScanReport ID from CCOM\n  xlsx  generate synthetic data from a ScanReport xlsx file\n</code></pre>"},{"location":"CaRROT-CDM/SyntheticData/#xlsx","title":"XLSX","text":"<p>If you have the original WhiteRabbit scan report locally, you can use this command to generate the synthetic data.</p> <pre><code>$ carrot generate synthetic xlsx --help\nUsage: carrot generate synthetic xlsx [OPTIONS] REPORT\n\n  generate synthetic data from a ScanReport xlsx file\n\nOptions:\n  -n, --number-of-events INTEGER  number of rows to generate  [required]\n  -o, --output-directory TEXT     folder to save the synthetic data to\n                                  [required]\n  --fill-column-with-values TEXT  select columns to fill values for\n  --help                          Show this message and exit.\n</code></pre> <p>Example</p> <p>The following command with use a local scan report (<code>ScanReport.xlsx</code>) to generate <code>1000</code> synthetic data events for each table in the report, it will fill all columns labelled <code>ID</code> with incrementing values (this is a feature to mimic genuine IDs) and put the output into a folder <code>synthetic_data</code>. <pre><code>carrot generate synthetic xlsx --fill-column-with-values ID -n 1000 -o synthetic_data ScanReport.xlsx\n</code></pre></p>"},{"location":"CaRROT-CDM/SyntheticData/#ccom","title":"CCOM","text":"<p>If you cannot obtain the original WhiteRabbit scan report, you can also generate synthetic data using our CCOM website by connecting via the api to the database.</p> <pre><code>$ carrot generate synthetic ccom --help\nUsage: carrot generate synthetic ccom [OPTIONS]\n\n  generate synthetic data from a ScanReport ID from CCOM\n\nOptions:\n  -i, --report-id INTEGER         ScanReport ID on the website  [required]\n  -n, --number-of-events INTEGER  number of rows to generate  [required]\n  -o, --output-directory TEXT     folder to save the synthetic data to\n                                  [required]\n  --fill-column-with-values TEXT  select columns to fill values for\n  -t, --token TEXT                specify the carrot_token for accessing\n                                  the CCOM website\n  -u, --url TEXT                  url endpoint for the CCOM website to ping\n  --help                          Show this message and exit.\n</code></pre> <p>Warning</p> <p>To be able to use this feature you need to have an access token to be able to connect to the database. If you are using this locally, you can supply <code>--url http://localhost:8080</code> and via <code>http://localhost:8080/admin</code> you can generate a token to be used. </p> <p>Example</p> <p>This feature can be used to also generate <code>1000</code> events for the scan report table with ID 106: <pre><code>carrot generate synthetic ccom -i 106 -n 1000 -o synthetic_data --token &lt;hashed token&gt;\n</code></pre></p>"},{"location":"CaRROT-CDM/VisitOccurrence/","title":"VisitOccurrence","text":"<p>               Bases: <code>DestinationTable</code></p> <p>CDM Visit Occurrence object class</p> Source code in <code>docs/CaRROT-CDM/source_code/carrot/cdm/objects/versions/v5_3_1/visit_occurrence.py</code> <pre><code>class VisitOccurrence(DestinationTable):\n    \"\"\"\n    CDM Visit Occurrence object class\n    \"\"\"\n\n    name = 'visit_occurrence'\n    def __init__(self,name=None):\n        self.visit_occurrence_id           = DestinationField(dtype=\"Integer\"   , required=True, pk=True)\n        self.person_id                     = DestinationField(dtype=\"Integer\"   , required=True)\n        self.visit_concept_id              = DestinationField(dtype=\"Integer\"   , required=True)\n        self.visit_start_date              = DestinationField(dtype=\"Date\"      , required=True)\n        self.visit_start_datetime          = DestinationField(dtype=\"Timestamp\" , required=False)\n        self.visit_end_date                = DestinationField(dtype=\"Date\"      , required=True)\n        self.visit_end_datetime            = DestinationField(dtype=\"Timestamp\" , required=False)\n        self.visit_type_concept_id         = DestinationField(dtype=\"Integer\"   , required=True)\n        self.provider_id                   = DestinationField(dtype=\"Integer\"   , required=False)\n        self.care_site_id                  = DestinationField(dtype=\"Integer\"   , required=False)\n        self.visit_source_value            = DestinationField(dtype=\"Text50\"    , required=False)\n        self.visit_source_concept_id       = DestinationField(dtype=\"Integer\"   , required=False)\n        self.admitting_source_concept_id   = DestinationField(dtype=\"Integer\"   , required=False)\n        self.admitting_source_value        = DestinationField(dtype=\"Text50\"    , required=False)\n        self.discharge_to_concept_id       = DestinationField(dtype=\"Integer\"   , required=False)\n        self.discharge_to_source_value     = DestinationField(dtype=\"Text50\"    , required=False)\n        self.preceding_visit_occurrence_id = DestinationField(dtype=\"Integer\"   , required=False)\n\n        if name is None:\n            name = hex(id(self))\n        super().__init__(name,self.name)\n</code></pre>"},{"location":"CaRROT-CDM/CLI/CLI/","title":"CLI","text":""},{"location":"CaRROT-CDM/CLI/Run/","title":"Run","text":""},{"location":"CaRROT-CDM/ETL/","title":"ETL","text":"<p>The Carrot ETL process runs via a Command Line Interface, installed with the carrot-cdm package.</p> <p>In the context of the Carrot workflow ETL stands for (and means):</p> <ul> <li>Extract: input data is extracted into <code>.csv</code> format and (optionally) pseudonymised    </li> <li>Transform: a CDM model is created and processed, given the extracted data and a <code>json</code> transformation rules file which tells the software how to map (transform) the data.    </li> <li>Load: inserts the data into a database or other destination.</li> </ul>"},{"location":"CaRROT-CDM/ETL/#workflow","title":"Workflow","text":"<p>Our ETL workflow is provided in the form of the so-called ETL-Tool which is a command line interface (CLI): <pre><code>carrot etl --help\n</code></pre> <pre><code>Usage: carrot etl [OPTIONS] COMMAND [ARGS]...\n\n  Command group for running the full ETL of a dataset\n\nOptions:\n  --config, --config-file TEXT  specify a yaml configuration file\n  -d, --daemon                  run the ETL as a daemon process\n  -l, --log-file TEXT           specify the log file to write to\n  --help                        Show this message and exit.\n</code></pre></p> <p>This is an automated tool, meaning it is able to run (optionally as a background process) and detect changes in the inputs or configuration files to process new data dumps. </p> <p>Currently, automation using the <code>carrot etl</code> CLI is possible for loading to a BC-Link or outputing to a local file storage system.</p>"},{"location":"CaRROT-CDM/ETL/#carrot-bc-link-workflow","title":"Carrot--BC-LINK Workflow","text":"<p>The whole point in transforming data into the OMOP CDM format is so the data can be uploaded to BC-Link. This workflow can be performed in one step with the correct configuration of the input <code>yaml</code> file when running <code>carrot etl --config &lt;yaml file&gt;</code>, see:  Configuring the Yaml File </p> <p>However, the process may need to be decoupled into multiple steps; for example, if BC-Link is not hosted on a machine that has access to the original data. In this scenario the <code>carrot etl</code> or <code>carrot run map</code> can be used to perform the transform (OMOP mapping), the output files can then be transferred to the machine hosting BC-Link and be uploaded (from the command-line, or using the BC-Link GUI)</p>"},{"location":"CaRROT-CDM/ETL/#architecture-overview","title":"Architecture Overview","text":"<p>A schematic diagram of the Carrot/bclink ETL is given below: </p> <p> The following breaks down what each part of the process is doing... </p>"},{"location":"CaRROT-CDM/ETL/#extract","title":"Extract","text":"<ul> <li>Formats the input data to abide by the co-connect data-standards [manual]</li> <li>Creates data-dumps of the input datasets in the form of <code>csv</code> files for each dataset table. [manual]</li> <li>Pseudonymises the input datasets, masking any person identifiers or person IDs [optionally automated]</li> </ul>"},{"location":"CaRROT-CDM/ETL/#transform","title":"Transform","text":"<ul> <li> <p>The transform mapping is executed with the command <code>carrot run map [arguments]</code>, where additional arguments pass the paths to a mapping-rules <code>json</code> file and input data <code>csv</code> files [optionally automated]:</p> <ul> <li>A new pythonic <code>CommonDataModel</code> is created.   </li> <li><code>DataCollection</code> is created to handle/chunk the input files and is added to the <code>CommonDataModel</code>.  </li> <li>The mapping-rules <code>json</code> is used to create new CDM Tables (e.g. Person).<ul> <li>For each CDM Table, multiple tables can be created. E.g. there may be multiple Condition Occurrences defined across multiple input data files and columns (fields).  </li> <li>The rules <code>json</code> encodes so-called \"term-mapping\" - how to map raw values into OHDSI concept IDs for the output. These are setup as lambda functions and passed to the object's <code>define</code> function </li> </ul> </li> <li>Processing of the <code>CommonDataModel</code> is triggered:   <ul> <li>[optionally chunked] A new chunk of <code>DataCollection</code> is grabbed.   </li> <li>Each CDM table is looped over:  <ul> <li>All objects of this CDM table are found and looped over:<ul> <li>The <code>define</code> function is called to apply the rules.</li> <li>A new dataframe is retrieved for the object.</li> </ul> </li> </ul> </li> <li>All retrieved dataframes are merged.   </li> <li>The merged dataframe is has a formatting check applied</li> <li>The merged dataframe primary keys are correctly indexed</li> <li>The final dataframe is saved/appended to an output <code>tsv</code> file.   </li> <li>This is repeated until all chunks are processed.   </li> </ul> </li> </ul> </li> </ul>"},{"location":"CaRROT-CDM/ETL/#loads","title":"Loads","text":"<ul> <li>Uploads the output <code>tsv</code> files into the bclink job-queue which formats and uploads these into the bclink database [optionally automated]</li> </ul>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/","title":"ETL Developer Guide","text":""},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#1-initial-setup","title":"1. Initial Setup","text":"<p>The following is a walkthrough of how to run an automated ETL process using test data available as part of the package.</p> <p>It is assumed that BCLink systems has already been an installed on a host machine. </p> Setting up a working directory <p>To use ETL with <code>bclink</code>, you must be the user <code>bcos_srv</code>, i.e. when ssh'd into the machine hosting <code>bclink</code>: <pre><code>ssh &lt;IP address of host machine&gt;\nsudo -s\nsu - bcos_srv\n</code></pre></p> <p>The best practise is to create a working directory in the following location: <pre><code>mkdir /usr/lib/bcos/MyWorkingDirectory/\ncd /usr/lib/bcos/MyWorkingDirectory/\n</code></pre></p> Installing carrot-cdm <p>It is also best practise to setup a virtual python environment and install the tool:</p> <pre><code>python3 -m venv automation\nsource automation/bin/activate\npip install pip --upgrade\npip install carrot-cdm\n</code></pre> <p>Check the version: <pre><code>carrot info version\n</code></pre> Which should display version <code>&gt;=0.4.1</code> for the automation to work.</p> <p>Detailed Installation Instructions</p> Get input data <p>Test data can be found in th <code>data_folder</code>: <pre><code>$ ls $(carrot info data_folder)/test/\nautomation  expected_outputs  inputs  rules  scan_report\n</code></pre></p> <p>Copy the data (or use a symbolic link) into your current working directory: <pre><code>$ pwd\n/usr/lib/bcos/MyWorkingDirectory\n$ mkdir input_data\n$ cp -r $(carrot info data_folder)/test/inputs/original input_data/001\n$ ls input_data/001\ncovid19_antibody.csv  Covid19_test.csv  Demographics.csv  Symptoms.csv  vaccine.csv\n</code></pre></p> Get a rules <code>json</code> file <p>As associated example mapping rules <code>json</code> file for this test dataset can be found and copied over to the working directory: <pre><code>$ cp -r $(carrot info data_folder)/test/rules/rules_14June2021.json rules.json\n$ carrot display json rules.json |&amp; head -15\n{\n  \"metadata\": {\n        \"date_created\": \"2021-06-14T15:27:37.123947\",\n        \"dataset\": \"Test\"\n  },\n  \"cdm\": {\n        \"observation\": {\n              \"observation_0\": {\n                    \"observation_concept_id\": {\n                          \"source_table\": \"Demographics.csv\",\n                          \"source_field\": \"ethnicity\",\n                          \"term_mapping\": {\n                                \"Asian\": 35825508\n                          }\n                    },\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#2-setup-data","title":"2. Setup Data","text":"<p>To the run the tool and automatically upload data to <code>bclink</code>, you must be logged in as the user <code>bcos_srv</code>, therefore this user must have permissions to view the data.</p> Granting data access to a user <p>There are many ways of doing this on CentOS via <code>chown</code> and/or <code>chmod</code>. You should contact your system administrator to do this if your are not experienced and/or don't have <code>root</code> access (that you may need). <pre><code>chown -R &lt;user/group&gt;:&lt;group&gt; &lt;path to data&gt;\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#3-setup-a-yaml-configuration-file","title":"3. Setup a <code>yaml</code> configuration file","text":"<p>The next step is to create and configure a <code>yaml</code> file for the tool to digest. This yaml file must contain the location of the \"rules\" <code>json</code> file, provided to you by the connect team, and the path of the input data.</p> Minimal YAML <p>Create a file called <code>config.yaml</code> and insert the following lines and save: <pre><code>rules: &lt;path to rules .json file&gt;\ndata: \n  - input: &lt;path to folder containing input .csv file&gt;\n    output: &lt;path to a folder where you want to save the output data e.g. `./output_data/`&gt;\n</code></pre></p> <p>Example: <pre><code>rules: /usr/lib/bcos/MyWorkingDirectory/rules.json \ndata: \n   - input: /usr/lib/bcos/MyWorkingDirectory/input_data/\n     output: /usr/lib/bcos/MyWorkingDirectory/output_data/\n</code></pre></p> YAML with multiple data folders <p>Similarly if you have multiple data dumps, you can configure the yaml like so: <pre><code>rules: &lt;path to rules .json file&gt;\ndata: \n  - input: &lt;path to folder containing input .csv file&gt;\n    output: &lt;path to a folder where you want to save the output data e.g. `./output_data/001/`&gt;\n  - input: &lt;path to a 2nd folder containing input .csv file&gt;\n    output: &lt;path to a 2nd folder where you want to save the output data e.g. `./output_data/002/`&gt;\n</code></pre></p> <p>Note</p> <p>While the tool is running you can edit this file to append more data paths...</p> YAML to watch a directory for new data dumps <p>Create a new file called <code>config.yaml</code> that will do the following:</p> <ol> <li>Look every 1 minute in the folder <code>input_data</code> for any subfolders containing input <code>csv</code> data files</li> <li>Run pseudonymisation on this data with a salt of value <code>00ed1234da</code> and save the pseudonymised data in the folder <code>pseudonymised_input_data</code></li> <li>Run the transform of the dataset into CDM based on the rules saved in <code>rules.json</code> </li> <li>Upload this data into bclink link tables (e.g. person.tsv \u2192 person_test_data_v1)</li> </ol> <pre><code>clean: true\nrules: /usr/lib/bcos/MyWorkingDirectory/rules.json\nlog: /usr/lib/bcos/MyWorkingDirectory/carrot.log\ndata: \n  watch: \n     minutes: 1\n  input: /usr/lib/bcos/MyWorkingDirectory/input_data\n  output: /usr/lib/bcos/MyWorkingDirectory/mapped_data\n  pseudonymise: \n     output: /usr/lib/bcos/MyWorkingDirectory/pseudonymised_input_data\n     salt: 00ed1234da\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#4-setup-and-check-bclink-tables","title":"4. Setup and Check BCLink tables","text":"<p>By default, if the Carrot documentation for setting up BCLink has been followed correctly, you should already have tables created for the various bclink tables, e.g.:  </p> <ul> <li>person  </li> <li>condition_occurrence  </li> <li>measurement  </li> <li>observation  </li> <li>drug_exposure   </li> </ul> Check if your tables exist <p>With a minimal <code>yaml</code> configuration, you can perform a check to see if the tables exist and that the tool is able to interact with them.</p> <p><pre><code>carrot etl bclink --config config.yml check_tables\n</code></pre> Example output: <pre><code>(automation) [bcos_srv@link-test-dt Demo]$ carrot etl bclink --config config.yml check_tables\n2021-11-17 10:33:31 - check_tables - INFO - printing to see if tables exist\n2021-11-17 10:33:31 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person' ) bclink\n2021-11-17 10:33:31 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2021-11-17 10:33:31 - check_tables - INFO - {\n  \"person\": true,\n  \"condition_occurrence\": true\n  }\n</code></pre></p> If you need to create new tables via the GUI <p>If you want to do some testing on new tables</p> <p>Log in to the bclink web-gui as the 'data' user.</p> <p>Create a table (example <code>person</code>) </p> <p>Make a not of the dataset name and ID of the dataset i.e. <code>ds100421</code> </p> If you need to create new tables via the command line <p>Creating a person table called <code>person_test_data_v1</code> <pre><code>cd /usr/lib/bcos/OMOP\n$dataset_tool --create --form=PERSON --table=person_test_data_v1 --setname='PERSON_TEST_DATA_V1' --user=data bclink\nCreated dataset with table person_test_data_v1\n</code></pre></p> <p>CO-CONNECT-Tools also has this feature to create tables based on what has been setup in the <code>yaml</code> configuration file <pre><code>carrot etl bclink --config &lt;config&gt; create_tables\n</code></pre> Example output: <pre><code>2021-11-05 11:03:10 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person_004' ) bclink\n2021-11-05 11:03:10 - run_bash_cmd - NOTICE - dataset_tool --create --table=person_004 --setname=PERSON_004 --user=data --form=PERSON bclink\n2021-11-05 11:03:10 - bclink_helpers - INFO - Created dataset with table person_004\n--&gt; All done with success.\n...\n</code></pre></p> If you needed to create tables / if your table check fails <p>If you have had problems with your tables and need to overwrite the default behaviour of the tool, you can append to the <code>yaml</code> file the following configurtion, to force the tool to upload the output (destination) tables to specific BCLink tables <pre><code>bclink:\n  tables:\n    person: &lt;bclink id of person table&gt;\n    observation: &lt;bclink id of observation table&gt; \n    measurement: &lt;bclink id of measurement table&gt; \n    condition_occurrence: &lt;bclink id of condition_occurrence table&gt; \n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#5-clean-the-tables","title":"5. Clean the tables","text":"<p>Before you run the ETL (for the first time), it's important to make sure there's no existing data present in the tables you are uploading to.</p> From the Command Line <p><pre><code>carrot etl bclink --config config.yml clean_tables \n</code></pre> Example output: <pre><code>(automation) [bcos_srv@link-test-dt Demo]$ carrot etl bclink --config config2.yml clean_tables\n2021-11-17 11:13:03 - bclink_helpers - INFO - Cleaning table person_001\n2021-11-17 11:13:03 - run_bash_cmd - NOTICE - datasettool2 delete-all-rows person_001 --database=bclink\n2021-11-17 11:13:05 - bclink_helpers - WARNING - Deleting all rows from dataset PERSON_001 (person_001)\n2021-11-17 11:13:05 - bclink_helpers - WARNING - Deleted all 0 rows from dataset PERSON_001 (person_001)\n2021-11-17 11:13:05 - bclink_helpers - INFO - Cleaning table condition_occurrence_001\n2021-11-17 11:13:05 - run_bash_cmd - NOTICE - datasettool2 delete-all-rows condition_occurrence_001 --database=bclink\n2021-11-17 11:13:07 - bclink_helpers - WARNING - Deleting all rows from dataset CONDITION_OCCURRENCE_001 (condition_occurrence_001)\n2021-11-17 11:13:07 - bclink_helpers - WARNING - Deleted all 6 rows from dataset CONDITION_OCCURRENCE_001 (condition_occurrence_001)\n2021-11-17 11:13:07 - clean_tables - INFO - removing output/001/\n</code></pre></p> Specify within the YAML <p>Alternatively you can tell the tool to do this automatically by specifying it in the <code>yaml</code> configuration file, by appending the configuration: <pre><code>clean: true\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#6-run-the-etl","title":"6. Run the ETL","text":"<p>Finally you are ready to execute the ETL...</p> Execute the full ETL <p><pre><code>carrot etl bclink --config config.yml execute\n</code></pre> Example output:</p>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#start-of-the-process","title":"Start of the Process","text":"<pre><code>2021-11-17 11:18:06 - _process_list_data - INFO - ETL process has begun\n2021-11-17 11:18:06 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person_001 bclink\n2021-11-17 11:18:06 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence_001 bclink\n2021-11-17 11:18:06 - bclink_helpers - INFO - ======== BCLINK SUMMARY ========\n2021-11-17 11:18:06 - bclink_helpers - INFO - {\n  \"person\": {\n        \"bclink_table\": \"person_001\",\n        \"nrows\": \"0\"\n  },\n  \"condition_occurrence\": {\n        \"bclink_table\": \"condition_occurrence_001\",\n        \"nrows\": \"0\"\n  }\n}\n2021-11-17 11:18:06 - _process_list_data - INFO - New data found! [{'input': 'data/001/', 'output': 'output/001/'}]\n2021-11-17 11:18:06 - execute - INFO - Executing steps ['clean', 'extract', 'transform', 'load']\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#extracting-data","title":"Extracting data","text":"<pre><code>...\n2021-11-17 11:18:10 - execute - INFO - Executing ETL...\n2021-11-17 11:18:10 - extract - INFO - starting extraction processes\n2021-11-17 11:18:10 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person_001 bclink\n2021-11-17 11:18:10 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence_001 bclink\n2021-11-17 11:18:10 - transform - INFO - starting data transform processes\n2021-11-17 11:18:10 - transform - INFO - inputs: ['data/001/']\n2021-11-17 11:18:10 - transform - INFO - output_folder: output/001/\n2021-11-17 11:18:10 - transform - INFO - indexer: {}\n2021-11-17 11:18:10 - transform - INFO - existing_global_ids: None\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#transforming-data","title":"Transforming data","text":"<pre><code>...\n2021-11-17 11:18:11 - InputData - INFO - Registering  Questionnaire.csv [&lt;class 'pandas.io.parsers.TextFileReader'&gt;]\n2021-11-17 11:18:11 - InputData - INFO - Registering  Demo.csv [&lt;class 'pandas.io.parsers.TextFileReader'&gt;]\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - CommonDataModel created with version 0.0.0\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - Running with the output to be dumped to a folder 'output/001/'\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - Running with an InputData object\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - Added FEMALE of type person\n...\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - Starting processing in order: ['person', 'condition_occurrence']\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - Number of objects to process for each table...\n{\n  \"person\": 2,\n  \"condition_occurrence\": 2\n}\n...\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - working on person\n2021-11-17 11:18:11 - FEMALE - INFO - Called apply_rules\n2021-11-17 11:18:11 - FEMALE - INFO - Mapped birth_datetime\n2021-11-17 11:18:11 - FEMALE - INFO - Mapped gender_concept_id\n2021-11-17 11:18:11 - FEMALE - INFO - Mapped gender_source_concept_id\n2021-11-17 11:18:11 - FEMALE - INFO - Mapped gender_source_value\n2021-11-17 11:18:11 - FEMALE - INFO - Mapped person_id\n2021-11-17 11:18:11 - FEMALE - INFO - Performing checks on data formatting.\n2021-11-17 11:18:11 - FEMALE - WARNING - Requiring non-null values in gender_concept_id removed 1 rows, leaving 3 rows.\n...\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - saving person to output/001//person.tsv\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - saving condition_occurrence to output/001//condition_occurrence.tsv\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - finished save to file\n2021-11-17 11:18:11 - CommonDataModel::Demo_test - INFO - making output folder output/001//logs/\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#loading-data","title":"Loading Data","text":"<pre><code>...\n2021-11-17 11:18:11 - load - INFO - starting loading data processes\n2021-11-17 11:18:11 - load - INFO - starting loading global ids\n2021-11-17 11:18:11 - load - INFO - starting loading cdm tables\n2021-11-17 11:18:11 - run_bash_cmd - NOTICE - dataset_tool --load --table=person_001 --user=data --data_file=output/001//person.tsv --support --bcqueue bclink\n2021-11-17 11:18:11 - bclink_helpers - INFO - submitted job to bclink queue: link-test-dt:bcos_srv-1221\n...\n2021-11-17 11:19:00 - bclink_helpers - INFO -    BATCH                     UPDDATE         UPD_COMPLETION_DATE    JOB STATUS  ACTION\n0   1327  2021-11-17-11.18.13.428504  2021-11-17-11.18.13.740396  13648     OK  INSERT\n2021-11-17 11:19:00 - bclink_helpers - INFO - Getting log for person_001 id=13648\n2021-11-17 11:19:00 - run_bash_cmd - NOTICE - cat /data/var/lib/bcos/download/data/job13648/cover.13648\n2021-11-17 11:19:00 - bclink_helpers - TEXT -\n2021-11-17 11:19:00 - bclink_helpers - TEXT - Job #13646    Wed Nov 17 11:18:11 2021    BC|SNPmax 6.0.0-rc21\n2021-11-17 11:19:00 - bclink_helpers - TEXT - ### Application: supp-dataload-batch\n2021-11-17 11:19:00 - bclink_helpers - TEXT - ### User:        data\n2021-11-17 11:19:00 - bclink_helpers - TEXT - ### Database:  bclink\n2021-11-17 11:19:00 - bclink_helpers - TEXT - ### Run on local/localhost.localdomain\n...\n2021-11-17 11:19:05 - execute - INFO - looking for duplicates and deleting any\n2021-11-17 11:19:05 - drop_duplicates - INFO - printing to see if tables exist\n2021-11-17 11:19:05 - drop_duplicates - INFO - Looking for duplicates in condition_occurrence (condition_occurrence_001)\n2021-11-17 11:19:05 - run_bash_cmd - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence_001' ) bclink\n...\n2021-11-17 11:19:05 - bclink_helpers - INFO - no duplicates detected\n2021-11-17 11:19:05 - bclink_helpers - INFO - [\n  {\n        \"job_id\": \"13648\",\n        \"table\": \"person\",\n        \"bclink_table\": \"person_001\",\n        \"From\": \" output/001//person.tsv\",\n        \"To\": \"   &lt;data&gt; PERSON_001 (person_001)\",\n        \"new_rows\": \"4 new row(s) inserted\"\n  },\n  {\n        \"job_id\": \"13649\",\n        \"table\": \"condition_occurrence\",\n        \"bclink_table\": \"condition_occurrence_001\",\n        \"From\": \" output/001//condition_occurrence.tsv\",\n        \"To\": \"   &lt;data&gt; CONDITION_OCCURRENCE_001 (condition_occurrence_001)\",\n        \"new_rows\": \"6 new row(s) inserted\"\n  }\n]\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Developer-Guide/#finished-and-waiting-for-changed","title":"Finished and waiting for changed","text":"<pre><code>2021-11-17 11:19:05 - execute - INFO - done!\n2021-11-17 11:19:05 - _process_list_data - INFO - Finished!... Listening for changes to data in config.yml\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/","title":"ETL Guide","text":""},{"location":"CaRROT-CDM/ETL/ETL-Guide/#1-initial-setup","title":"1. Initial Setup","text":"<p>The following is a walkthrough of how to run an automated ETL process using test data available as part of the package.</p> <p>It is assumed that BCLink systems has already been an installed on a host machine. </p> Setting up a working directory <p>When running the ETL with the <code>bclink</code> backend, you must be the user <code>bcos_srv</code>, i.e. when ssh'd into the machine hosting <code>bclink</code>: <pre><code>ssh &lt;username&gt;@&lt;IP address of host machine&gt;\nsudo -s\nsu - bcos_srv\n</code></pre></p> <p>The best practise is to create a working directory in the following location: <pre><code>mkdir /usr/lib/bcos/MyWorkingDirectory/\ncd /usr/lib/bcos/MyWorkingDirectory/\n</code></pre></p> Installing carrot-cdm <p>It is also best practise to setup a virtual python environment and install the tool:</p> <pre><code>python3 -m venv automation\nsource automation/bin/activate\npip install pip --upgrade\npip install carrot-cdm\n</code></pre> <p>Check the version: <pre><code>carrot info version\n</code></pre> Which should display version <code>&gt;=0.5.0</code> for the automation to work.</p> <p>Detailed Installation Instructions</p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#2-setup-data","title":"2. Setup Data","text":"<p>Setup your inputs and obtain the <code>rules.json</code> for performing the transform (OMOP mapping).</p> Get inputs <p>We recommend that you download the test dataset from CO-CONNECT/demo-dataset, otherwise a smaller test dataset can be foud in the following location: <code>$(carrot info data_folder)/test/</code></p> <p>Note</p> <p>You will have to have <code>git</code> installed for this to work. You can do this while being <code>sudo</code> user, for example on CentOS: <pre><code>sudo yum install git\n</code></pre> If you are running from Windows, Mac or other forms of Linux and do not have <code>git</code> installed, you can either contact your system administrator, or follow the link to the github page, click \"code\" and then \"Download ZIP\".</p> <p><pre><code>git clone https://github.com/CO-CONNECT/demo-dataset.git\n</code></pre> checking the files: <pre><code>ls demo-dataset/data/\n</code></pre> There are two rules files, one yaml configuration (plus a tempate configuration file), and three folders for different data dumps. More information can be found in the demo-dataset README: <pre><code>config.yaml config-template.yaml part1  part2  part3  rules.json  rules_small.json\n</code></pre> We can check the rules.json file by testing displaying the first 15 liens of the <code>json</code> file: <pre><code>carrot display rules json demo-dataset/data/rules.json |&amp; head -15\n</code></pre> Outputs: <code>{   \"metadata\": {         \"date_created\": \"2022-02-11T12:22:48.465257\",         \"dataset\": \"FAILED: ExampleV4\"   },   \"cdm\": {         \"person\": {               \"MALE 3025\": {                     \"birth_datetime\": {                           \"source_table\": \"Demographics.csv\",                           \"source_field\": \"Age\",                           \"operations\": [                                 \"get_datetime_from_age\"                           ]                     },</code></p> <p>Note</p> <p>If you are following this guide with the example dataset, as long as you have copied or downloaded data as user <code>bcos_srv</code>, then all should be OK.</p> <p>To the run the tool and automatically upload data to <code>bclink</code>, you must be logged in as the user <code>bcos_srv</code>, therefore this user must have permissions to view the data. So double-check or grant access. </p> Granting data access to a user <p>There are many ways of doing this on CentOS via <code>chown</code> and/or <code>chmod</code>. You should contact your system administrator to do this if your are not experienced and/or don't have <code>root</code> access (that you may need). <pre><code>chown -R &lt;user/group&gt;:&lt;group&gt; &lt;path to data&gt;\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#3-setup-a-yaml-configuration-file","title":"3. Setup a <code>yaml</code> configuration file","text":"<p>The next step is to create and configure a <code>yaml</code> file for the tool to digest. This yaml file must contain the location of the \"rules\" <code>json</code> file, provided to you by the connect team, and the path of the input data.</p> Minimal YAML <p>Create a file called <code>config.yaml</code>, either as a new file or copy over the <code>cp demo-dataset/data/config-template.yaml config.yaml</code> and edit the values: <pre><code>settings:\n  clean: &lt;bool&gt;\n\nload: &amp;load-bclink\n  cache: &lt;str: cache folder&gt; \n  bclink:\n    dry_run: &lt;bool&gt;\n\ntransform:\n   settings: &amp;settings\n     output: *load-bclink\n     rules: &lt;str: rules .json file&gt;\n   data:\n     - input: &lt;str: first input data folder&gt;\n       &lt;&lt;: *settings\n     - input: &lt;str: additional input data folder&gt;\n       &lt;&lt;: *settings\n</code></pre></p> <p>Example template to use if you have copied over and are useing the demo-dataset:  </p> <pre><code>settings:\n  clean: true\nload: &amp;load-bclink\n  cache: /usr/lib/bcos/MyWorkingDirectory/cache/\n  bclink:\n    dry_run: false\ntransform:\n   settings: &amp;settings\n     output: *load-bclink\n     rules: /usr/lib/bcos/MyWorkingDirectory/demo-dataset/data/rules.json\n   data:\n     - input: /usr/lib/bcos/MyWorkingDirectory/demo-dataset/data/part1/\n       &lt;&lt;: *settings\n</code></pre> <p>Note</p> <p>If you are unfamilar with the Linux command-line and don't know how to create a file, try using <code>vim</code>, <code>pico</code> or <code>emacs</code> commands. Once you have created the file, you should save and quit.</p> YAML with multiple data folders <p>Similarly if you have multiple data dumps, you can configure the yaml like so: <pre><code>transform:\n   settings: ...\n   data:\n     - input: &lt;folder 1&gt;\n       &lt;&lt;: *settings\n   data:\n     - input: &lt;folder 2&gt;\n       &lt;&lt;: *settings\n</code></pre></p> <p>Note</p> <p>While the tool is running you can edit this file to append more data paths...</p> YAML to watch a directory for new data dumps <p>Specifying the data in the transform section as a dictionary will tell the tool to look inside the master folder for subfolders containing the data</p> <pre><code>transform:\n   settings: ...\n   data:\n     input: &lt;master folder&gt;\n     &lt;&lt;: *settings\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#4-setup-and-check-bclink-tables","title":"4. Setup and Check BCLink tables","text":"<p>By default, if the Carrot documentation for setting up BCLink has been followed correctly, you should already have tables created for the various bclink tables, e.g.:  </p> <ul> <li>person  </li> <li>condition_occurrence  </li> <li>measurement  </li> <li>observation  </li> <li>drug_exposure   </li> <li>etc..</li> </ul> Check if your tables exist <p>With a minimal <code>yaml</code> configuration, you can perform a check to see if the tables exist and that the tool is able to interact with them.</p> <p><pre><code>carrot etl --config config.yaml check-tables\n</code></pre> Example output: <pre><code>2022-03-15 12:56:02 - run_etl - INFO - running etl on config.yaml (last modified: 1647348863.9978611)\n...\n2022-03-15 12:56:02 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2022-03-15 12:56:02 - BCLinkHelpers - INFO - condition_occurrence (condition_occurrence) already exists --&gt; all good\n2022-03-15 12:56:02 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'drug_exposure' ) bclink\n2022-03-15 12:56:02 - BCLinkHelpers - INFO - drug_exposure (drug_exposure) already exists --&gt; all good\n...\n2022-03-15 12:56:03 - BCLinkHelpers - INFO - ======== BCLINK SUMMARY ========\n2022-03-15 12:56:03 - BCLinkHelpers - INFO - {\n  \"condition_occurrence\": {\n        \"bclink_table\": \"condition_occurrence\",\n        \"nrows\": \"63988\"\n  },\n  \"drug_exposure\": {\n        \"bclink_table\": \"drug_exposure\",\n        \"nrows\": \"33015\"\n  },\n...\n</code></pre></p> <p>If there is an error here, you may need to manually configure the table in the yaml file (see here)</p> If you need to create new tables via the GUI <p>If you want to do some testing on new tables</p> <p>Log in to the bclink web-gui as the 'data' user.</p> <p>Create a table (example <code>person</code>) </p> <p>Make a not of the dataset name and ID of the dataset i.e. <code>ds100421</code> </p> If you need to create new tables via the command line <p>Creating a person table called <code>person_test_data_v1</code> <pre><code>cd /usr/lib/bcos/OMOP\n$dataset_tool --create --form=PERSON --table=person_test_data_v1 --setname='PERSON_TEST_DATA_V1' --user=data bclink\nCreated dataset with table person_test_data_v1\n</code></pre></p> <p>You would then have to specify in the <code>yaml</code> configuration the name of these tables so the tool is able to know e.g. which table to insert the <code>person</code> into, for example: <pre><code>load: &amp;load-bclink\n  bclink:\n    tables:\n       person: person_test_data_v1\n    ...\n</code></pre></p> <p>CaRROT-CDM also has this feature to create tables based on what has been setup in the <code>yaml</code> configuration file <pre><code>carrot etl --config &lt;config&gt; create-tables\n</code></pre> Example output: <pre><code>2022-03-15 13:41:39 - run_etl - INFO - running etl on config.yaml (last modified: 1647351696.3011575)\n2022-03-15 13:41:39 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person_test_003' ) bclink\n2022-03-15 13:41:39 - BCLinkHelpers - NOTICE - dataset_tool --create --table=person_test_003 --setname=PERSON_TEST_003 --user=data --form=PERSON bclink\n2022-03-15 13:41:39 - BCLinkHelpers - INFO - Created dataset with table person_test_003\n--&gt; All done with success.\n...\n</code></pre></p> If you needed to create tables / if your table check fails <p>If you have had problems with your tables and need to overwrite the default behaviour of the tool, you can append to the <code>yaml</code> file the following configurtion, to force the tool to upload the output (destination) tables to specific BCLink tables <pre><code>bclink:\n  tables:\n    person: &lt;bclink id of person table&gt;\n    observation: &lt;bclink id of observation table&gt; \n    measurement: &lt;bclink id of measurement table&gt; \n    condition_occurrence: &lt;bclink id of condition_occurrence table&gt; \n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#5-clean-the-tables","title":"5. Clean the tables","text":"<p>Before you run the ETL (for the first time), it's important to make sure there's no existing data present in the tables you are uploading to.</p> From the Command Line <p><pre><code>carrot etl --config config.yaml clean-tables \n</code></pre> Example output: <pre><code>2022-03-15 13:42:51 - run_etl - INFO - running etl on config.yaml (last modified: 1647351753.2908654)\n2022-03-15 13:42:51 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2022-03-15 13:42:51 - BCLinkHelpers - INFO - condition_occurrence (condition_occurrence) already exists --&gt; all good\n...\n2022-03-15 13:42:51 - BCLinkHelpers - NOTICE - datasettool2 delete-all-rows condition_occurrence --database=bclink\n2022-03-15 13:42:53 - BCLinkHelpers - WARNING - Deleting all rows from dataset CONDITION_OCCURRENCE (condition_occurrence)\n2022-03-15 13:42:53 - BCLinkHelpers - WARNING - Deleted all 63988 rows from dataset CONDITION_OCCURRENCE (condition_occurrence)\n2022-03-15 13:42:53 - BCLinkHelpers - INFO - Cleaning table drug_exposure\n2022-03-15 13:42:53 - BCLinkHelpers - NOTICE - datasettool2 delete-all-rows drug_exposure --database=bclink\n...\n2022-03-15 13:43:07 - BCLinkHelpers - INFO - ======== BCLINK SUMMARY ========\n2022-03-15 13:43:07 - BCLinkHelpers - INFO - {\n\"condition_occurrence\": {\n        \"bclink_table\": \"condition_occurrence\",\n        \"nrows\": \"0\"\n  },\n  \"drug_exposure\": {\n        \"bclink_table\": \"drug_exposure\",\n        \"nrows\": \"0\"\n  },\n  \"measurement\": {\n        \"bclink_table\": \"measurement\", \n        \"nrows\": \"0\"\n  },\n  \"observation\": {\n        \"bclink_table\": \"observation\", \n        \"nrows\": \"0\"\n  },\n</code></pre></p> Specify within the YAML <p>Alternatively you can tell the tool to do this automatically by specifying it in the <code>yaml</code> configuration file, by appending the configuration. Everytime <code>carrot etl</code> is executed, the tables present in BCLink will be cleaned: <pre><code>settings:\n   clean: true\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#6-run-the-etl","title":"6. Run the ETL","text":"<p>Finally you are ready to execute the ETL...</p> Start the carrot ETL <p><pre><code>carrot etl --config config.yaml\n</code></pre> Example output:</p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#start-of-the-process","title":"Start of the Process","text":"<pre><code>2022-03-15 13:47:57 - run_etl - INFO - running etl on config.yaml (last modified: 1647352068.9562533)\n2022-03-15 13:47:57 - LocalDataCollection - INFO - DataCollection Object Created\n2022-03-15 13:47:57 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x7f0c5a82a5c0&gt;]\n2022-03-15 13:47:57 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x7f0c5a82a9b0&gt;]\n...\n2022-03-15 13:47:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2022-03-15 13:47:57 - BCLinkHelpers - INFO - condition_occurrence (condition_occurrence) already exists --&gt; all good\n...\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#building-the-model","title":"Building the Model","text":"<pre><code>...\n2022-03-15 13:47:58 - BCLinkDataCollection - INFO - DataCollection Object Created\n2022-03-15 13:47:58 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with carrot-cdm version 0.0.0\n2022-03-15 13:47:58 - CommonDataModel - INFO - Running with an DataCollection object\n2022-03-15 13:47:58 - CommonDataModel - INFO - Turning on automatic cdm column filling\n...\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#extracting-data","title":"Extracting data","text":"<pre><code>...\n2022-03-15 13:47:58 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT * FROM person_ids  bclink\n2022-03-15 13:47:58 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence bclink\n...\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#adding-health-data-element","title":"Adding Health Data Element","text":"<pre><code>2022-03-15 13:47:58 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-03-15 13:47:58 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-03-15 13:47:58 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n</code></pre>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#transforming-data","title":"Transforming data","text":"<p>Preparing everything: <pre><code>...\n2022-03-15 13:47:58 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-03-15 13:47:58 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n  \"person\": 2,\n  \"observation\": 4,\n  \"condition_occurrence\": 12,\n  \"drug_exposure\": 5\n}\n2022-03-15 13:47:58 - CommonDataModel - INFO - for person: found 2 objects\n2022-03-15 13:47:58 - CommonDataModel - INFO - working on person\n2022-03-15 13:47:58 - CommonDataModel - INFO - starting on MALE 3025\n2022-03-15 13:47:58 - Person - INFO - Called apply_rules\n2022-03-15 13:47:58 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n...\n</code></pre> Applying rules to build the health data element and saving the file to the cache folder: <pre><code>...\n2022-03-15 13:48:05 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-03-15 13:48:05 - Person - INFO - Called apply_rules\n2022-03-15 13:48:06 - Person - INFO - Mapped birth_datetime\n2022-03-15 13:48:06 - Person - INFO - Mapped gender_concept_id\n2022-03-15 13:48:06 - Person - INFO - Mapped gender_source_concept_id\n2022-03-15 13:48:06 - Person - INFO - Mapped gender_source_value\n2022-03-15 13:48:06 - Person - INFO - Mapped person_id\n2022-03-15 13:48:06 - Person - WARNING - Requiring non-null values in gender_concept_id removed 55732 rows, leaving 44268 rows.\n2022-03-15 13:48:06 - Person - INFO - Automatically formatting data columns.\n2022-03-15 13:48:06 - Person - INFO - created df (0x7f0c57843908)[FEMALE_3026]\n2022-03-15 13:48:06 - CommonDataModel - INFO - finished FEMALE 3026 (0x7f0c57843908) ... 2/2 completed, 44268 rows\n...\n2022-03-15 13:48:09 - BCLinkDataCollection - INFO - saving person.FEMALE_3026.0x7f0c57843908.2022-03-15T134809 to /usr/lib/bcos/MyWorkingDirectory/Temp/cache//person.FEMALE_3026.0x7f0c57843908.2022-03-15T134809.tsv\n...\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#loading-data","title":"Loading Data","text":"<p>Making jobs to load the data <pre><code>...\n2022-03-15 13:48:09 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person --user=data --data_file=/usr/lib/bcos/MyWorkingDirectory/Temp/cache//person.FEMALE_3026.0x7f0c57843908.2022-03-15T134809.tsv --support --bcqueue bclink\n2022-03-15 13:48:10 - BCLinkHelpers - INFO - submitted job to bclink queue: link-test-dt:bcos_srv-336\n2022-03-15 13:48:10 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person --user=data --database=bclink\n2022-03-15 13:48:13 - BCLinkHelpers - INFO - running job 10983\n...\n</code></pre> checking to see if upload jobs have finished: <pre><code>2022-03-15 13:56:19 - BCLinkHelpers - NOTICE - cat /data/var/lib/bcos/download/data/job11043/cover.11043\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - Job #11042    Tue Mar 15 13:56:06 2022    BC|SNPmax 6.0.0-rc21\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ### Application: supp-dataload-batch\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ### User:        data\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ### Database:  bclink\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ### Run on local/localhost.localdomain\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - Job description: Upload of file /usr/lib/bcos/MyWorkingDirectory\n/Temp/cache//drug_exposure.COVID_19_vaccine_3036.0x7f0c55b86390.2022-03-15T135606.tsv\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ================================================================\n========\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - --&gt; No data converter selected\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - --&gt; Adding new data, duplicates are discarded and reported\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ===Summary report===\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - --&gt; OK\n2022-03-15 13:56:19 - BCLinkHelpers - CRITICAL - 110 data row(s) discarded, 23389 new row(s) inserted\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ---&gt; See attachments\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ===Detailed report===\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -  \n...\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/ETL-Guide/#finished","title":"Finished!","text":"<pre><code>...\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - --&gt; OK\n2022-03-15 13:56:19 - BCLinkHelpers - CRITICAL - 115 data row(s) discarded, 23384 new row(s) inserted\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ---&gt; See attachments\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT - ===Detailed report===\n2022-03-15 13:56:19 - BCLinkHelpers - TEXT -  \n2022-03-15 13:56:19 - BCLinkDataCollection - INFO - done!\n</code></pre> <p>If the <code>yaml</code> contains: <pre><code>settings:\n   listen_for_changes: true\n</code></pre></p> <p>Instead you will see the message: <pre><code>2022-03-15 13:59:58 - run_etl - INFO - Finished!... Listening for changes every 5 seconds to data in config.yaml\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Extract/","title":"Extract","text":"<p>As described on https://co-connect.ac.uk, the input data for the ETL process must be in the form of <code>csv</code> data dumps, that are formatted to a particular data-standard.</p> <p> Data Standards </p> <p>This part of the ETL process must be performed manually by the data-partner.</p> <p>The 2<sup>nd</sup> part of the extract process (pseudonymisation) is optionally automated as part of the ETL.</p>"},{"location":"CaRROT-CDM/ETL/Load/","title":"Load","text":""},{"location":"CaRROT-CDM/ETL/Load/#via-the-gui","title":"Via the GUI","text":"<p>While logged in to the bclink web-gui as the 'data' user.</p>"},{"location":"CaRROT-CDM/ETL/Load/#create-a-new-person-table-optional","title":"Create a new person table [optional]","text":"<p>Viewing the info: </p>"},{"location":"CaRROT-CDM/ETL/Load/#upload-data","title":"Upload data","text":""},{"location":"CaRROT-CDM/ETL/Load/#via-the-cli","title":"Via the CLI","text":""},{"location":"CaRROT-CDM/ETL/Load/#ssh-into-the-vm","title":"ssh into the VM","text":"<p>Log in to the VM hosting bclink and switch to the user <code>bcos_srv</code>: <pre><code>ssh XX.YY.ZZ\nsudo -s\nsu - bcos_srv\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Load/#setup-a-working-directory","title":"Setup a working directory","text":"<p>Create a new working directory <pre><code>mkdir /usr/lib/bcos/MyWorkingDirectory/\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Load/#prepare-files","title":"Prepare files","text":"<p>Move the <code>tsv</code> files created by the carrot-cdm transform into this directory, e.g. in a subfolder <code>mapped_data</code>: <pre><code>mapped_data/\ntest/\n\u251c\u2500\u2500 condition_occurrence.tsv\n\u251c\u2500\u2500 masked_person_ids.csv\n\u251c\u2500\u2500 observation.tsv\n\u2514\u2500\u2500 person.tsv\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Load/#get-the-names-of-the-tables","title":"Get the name(s) of the tables","text":"<p>The default bclink table for \"person\" will be named <code>person</code>. However, you may want to create your own tables via the command line or via the GUI. As in the previous example, a <code>person</code> table could be created with the dataset id <code>ds100394</code>.</p>"},{"location":"CaRROT-CDM/ETL/Load/#clean-the-database-optional","title":"Clean the database [optional]","text":"<pre><code>datasettool2 delete-all-rows ds100394 --database=bclink\n</code></pre>"},{"location":"CaRROT-CDM/ETL/Load/#load-the-data","title":"Load the data","text":"<p>Use <code>dataset_tool</code> to load <code>tsv</code> files into the database (default name is bclink) as the user <code>data</code> (so you are able to view the upload jobs via the web GUI.</p> <p>The flags <code>--support</code> &amp; <code>--bcqueue</code> make sure the data file is uploaded as a job to the bclink batch queue.</p> <p>Example: <pre><code>dataset_tool --load \\\n             --table=ds100123 \\\n         --user=data\n         --data_file=mapped_data/person.tsv \\\n         --support --bcqueue \\\n         bclink\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Pseudonymisation/","title":"Pseudonymisation","text":"<p>Danger</p> <p>This section is decrepid, please instead use co-connect-pseudonymise instead.</p> <p>Warning</p> <p>Please use the command: <pre><code>pseudonymise csv --help\n</code></pre> That comes with the co-connect-pseudonymise package, and is installed when you install <code>carrot-cdm</code>.</p>"},{"location":"CaRROT-CDM/ETL/Pseudonymisation/#decrepid-tool","title":"Decrepid Tool","text":"<p>A dataset can be pseudonymised in-house, or via the use of the carrot-cdm <code>pseudonymise</code> command (also the command executed in the ETL automation process)</p> <pre><code>$ carrot pseudonymise --help\nUsage: carrot pseudonymise [OPTIONS] INPUT\n\n  Command to help pseudonymise data.\n\nOptions:\n  -s, --salt TEXT             salt hash  [required]\n  -i, --person-id, --id TEXT  name of the person_id  [required]\n  -o, --output-folder TEXT    path of the output folder  [required]\n  --chunksize INTEGER         set the chunksize when loading data\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"CaRROT-CDM/ETL/Pseudonymisation/#example","title":"Example","text":"<pre><code>carrot pseudonymise -s 123456 -i PersonID -o pseudoynmised_data $(carrot info data_folder)/test/inputs/original/Demographics.csv                  \n</code></pre> <p>Which outputs:</p> <pre><code>2021-10-07 14:55:55 - pseudonymise - INFO - Working on file /Users/calummacdonald/Usher/CO-CONNECT/Software/carrot-cdm/carrot/data/test/inputs/original/Demographics.csv, pseudonymising column 'PersonID' with salt '123456'\n2021-10-07 14:55:55 - pseudonymise - INFO - Saving new file to pseudoynmised_data/Demographics.csv\n2021-10-07 14:55:55 - InputData - INFO - InputData Object Created\n2021-10-07 14:55:56 - InputData - INFO - Registering  /Users/calummacdonald/Usher/CO-CONNECT/Software/carrot-cdm/carrot/data/test/inputs/original/Demographics.csv [&lt;class 'pandas.core.frame.DataFrame'&gt;]\n2021-10-07 14:55:56 - pseudonymise - INFO - 0      16dc368a89b428b2485484313ba67a3912ca03f2b2b424...\n1      37834f2f25762f23e1f74a531cbe445db73d6765ebe608...\n2      454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51...\n3      5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f...\n4      1253e9373e781b7500266caa55150e08e210bc8cd8cc70...\n                             ...                        \n995    8352dd9eb8b64669e0a8347fd37ae6e5cd67c817f2b4b1...\n996    235aa062e6372588dbae00552abf36b8ff9c315e3da56c...\n997    4aec429ac0bfafdbb8dab14f41d1b7a98dacf1ce3478b7...\n998    330e14d4ae80612334d94c488d29eb469626b476864abd...\n999    ab9828ca390581b72629069049793ba3c99bb8e5e9e7b9...\nName: PersonID, Length: 1000, dtype: object\n2021-10-07 14:55:56 - pseudonymise - INFO - Done!\n</code></pre>"},{"location":"CaRROT-CDM/ETL/Rules/","title":"Rules","text":"<p>The transform (mapping) rules <code>json</code> file encodes rules on how to create CDM objects.</p> <p>For co-connect datapartners, you will be supplied this <code>json</code> file by the co-connect team, who create this file based on a WhiteRabbit ScanReport uploaded and then mapped on the Carrot-Mapper mapping website.</p>"},{"location":"CaRROT-CDM/ETL/Rules/#cli-helper","title":"CLI helper","text":"<p>The co-connect package comes with a CLI helper for displaying/manipulating rules files <pre><code>carrot display rules --help\n</code></pre> <pre><code>Usage: carrot display rules [OPTIONS] COMMAND [ARGS]...\n\n  Commands for displaying json rules in various ways.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  dag      Display the OMOP mapping json as a DAG\n  delta    display a delta of two rules files\n  flatten  flattern a rules json file\n  json     Show the OMOP mapping json\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Rules/#json-structure","title":"JSON Structure","text":"<pre><code>{\n      \"metadata\": { ... },\n      \"cdm\": {\n             \"&lt;cdm table e.g. 'person' &gt;\": {\n                  \"&lt;object name: e.g 'MALE' &gt;\": {\n                        \"&lt;cdm field: e.g. 'gender_concept_id'&gt;\": {\n                              \"source_table\": \"&lt;name of input .csv&gt;\",\n                              \"source_field\": \"&lt;name of field in input .csv&gt;\",\n                              \"term_mapping\": {\n                                    \"&lt;name of value in field&gt;\": \"&lt;ohdsi concept id e.g. 8507 for 'MALE' &gt;\"\n                              }\n                        },\n            \"&lt;next cdm field&gt;\": {...},\n            ...\n              },\n          \"&lt;next object name: e.g. 'FEMALE'&gt;\" : {...}\n         },\n         \"&lt;next cdm table e.g. 'condition_occurrence'&gt;\" : {...}\n      }\n }\n</code></pre>"},{"location":"CaRROT-CDM/ETL/Rules/#full-json-example","title":"Full json example","text":"<p>An example <code>json</code> file can be found in the following location: <pre><code>echo $(carrot info data_folder)/test/rules/rules_14June2021.json\n</code></pre></p> <p>It can be displayed to the terminal (Unix): <pre><code>carrot display rules json $(carrot info data_folder)/test/rules/rules_14June2021.json\n</code></pre></p> <pre><code>{\n      \"metadata\": {\n            \"date_created\": \"2021-06-14T15:27:37.123947\",\n            \"dataset\": \"Test\"\n      },\n      \"cdm\": {\n            \"observation\": {\n                  \"observation_0\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Asian\": 35825508\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Asian\": 35825508\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_1\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Bangladeshi\": 35825531\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Bangladeshi\": 35825531\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_2\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Indian\": 35826241\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Indian\": 35826241\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_3\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White\": 35827394\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White\": 35827394\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_4\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Black\": 35825567\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Black\": 35825567\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_5\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White and Asian\": 35827395\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White and Asian\": 35827395\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"condition_occurrence\": {\n                  \"condition_occurrence_0\": {\n                        \"condition_concept_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\",\n                              \"term_mapping\": {\n                                    \"Y\": 254761\n                              }\n                        },\n                        \"condition_end_datetime\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"visit_date\"\n                        },\n                        \"condition_source_concept_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\",\n                              \"term_mapping\": {\n                                    \"Y\": 254761\n                              }\n                        },\n                        \"condition_source_value\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\"\n                        },\n                        \"condition_start_datetime\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"visit_date\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"person\": {\n                  \"female\": {\n                        \"birth_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"gender_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"F\": 8532\n                              }\n                        },\n                        \"gender_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"F\": 8532\n                              }\n                        },\n                        \"gender_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"male\": {\n                        \"birth_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"gender_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"M\": 8507\n                              }\n                        },\n                        \"gender_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"M\": 8507\n                              }\n                        },\n                        \"gender_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"measurement\": {\n                  \"covid_antibody\": {\n                        \"value_as_number\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\"\n                        },\n                        \"measurement_source_value\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\"\n                        },\n                        \"measurement_concept_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\",\n                              \"term_mapping\": 37398191\n                        },\n                        \"measurement_source_concept_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\",\n                              \"term_mapping\": 37398191\n                        },\n                        \"measurement_datetime\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"date\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            }\n      }\n}\n</code></pre>"},{"location":"CaRROT-CDM/ETL/Rules/#demo-dataset","title":"Demo Dataset","text":"<p>If you have downloaded the demo-dataset you will find two rules json files: <pre><code>ls demo-dataset/data/*json\n</code></pre> <pre><code>demo-dataset/data/rules.json       demo-dataset/data/rules_small.json\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Rules/#dag-display","title":"DAG Display","text":"<p>Example: <pre><code>carrot display rules dag demo-dataset/data/rules_small.json\ncarrot display rules dag demo-dataset/data/rules.json\n</code></pre></p> <p> </p>"},{"location":"CaRROT-CDM/ETL/Rules/#json-delta","title":"JSON Delta","text":"<p>You may want to extract a difference between two <code>json</code> rules files, for which the command <code>display rules delta</code> is appropriate.</p> <p>Extract the delta between two rules files (a small old, and bigger new), pipeing the output to a file: <pre><code>carrot display rules delta demo-dataset/data/rules_small.json demo-dataset/data/rules.json | tee rules_delta.json\n</code></pre> Output example... <pre><code>2022-03-16 10:35:09 - load_json_delta - INFO - loading a json from 'demo-dataset/data/rules.json' as a delta\n2022-03-16 10:35:09 - load_json_delta - INFO - Original JSON date: 2022-02-11T12:22:48.465257\n2022-03-16 10:35:09 - load_json_delta - INFO - New JSON date: 2022-02-11T12:22:48.465257\n...\n2022-03-16 10:34:42 - load_json_delta - INFO - Detected a new rule for Hypertensive disorder 3050\n2022-03-16 10:34:42 - load_json_delta - INFO - Detected a new rule for COVID-19 vaccine 3034\n2022-03-16 10:34:42 - load_json_delta - INFO - Detected a new rule for COVID-19 vaccine 3036\n2022-03-16 10:34:42 - load_json_delta - INFO - Detected a new rule for SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-03-16 10:34:42 - load_json_delta - INFO - Detected a new rule for SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041~\n...\n },\n      \"cdm\": {\n            \"observation\": {\n                  \"H/O: heart failure 3043\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Hospital_Visit.csv\",\n                              \"source_field\": \"reason\",\n                              \"term_mapping\": {\n                                    \"Heart Attack\": 4059317\n                              }\n                        },\n...\n</code></pre></p> <p>And displaying the delta of the rules created: <pre><code>carrot display rules dag rules_delta.json\n</code></pre> </p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/","title":"Transform GUI","text":"<p>Caution</p> <p>On macOS the GUI only works with <code>python &gt;= 3.9.0</code></p> <p>A simple Graphical User Interface (GUI) is implemented in carrot-cdm in order to run the mapping.</p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/#1-prepare-the-inputs","title":"1. Prepare the inputs","text":"<p>In this example, we use the test data that is available in the tool (see <code>carrot info</code>) We have:  </p> <ul> <li>A <code>rules.json</code> file for the mapping-rules  </li> <li>A folder containing the input data  </li> </ul> <p></p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/#2-start-the-gui","title":"2. Start the GUI","text":"<p>Assuming you have successfully installed the packaged, you need to enter the command line to start the GUI:</p> Option 1Option 2 <pre><code>etl-gui\n</code></pre> <pre><code>carrot map gui\n</code></pre> <p>Note</p> <p>This can be started from any directory, as soon as you open up the command line. You do not need to be in the same directory as all your files.</p> <p>Which pops open the GUI.. </p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/#3-select-files","title":"3. Select Files","text":"<p>Use the browse buttons to select the rules.json file as well as the input <code>.csv</code> files </p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/#4-run","title":"4. Run","text":"<p>Click the \"Run\" button to execute running the tools on the selected files.  </p>"},{"location":"CaRROT-CDM/ETL/Transform-GUI/#5-inspect-the-output","title":"5. Inspect the output","text":"<p>You will see outputs appear on the command line. A pop-up box will appear saying 'Done' when the tool has finished running. </p> <p>Now the folder will contain the output <code>tsv</code> files which contain the mapped CDM tables. In this example these are:  </p> <ul> <li><code>person.tsv</code> </li> <li><code>condition_occurrence.tsv</code> </li> <li><code>observation.tsv</code> </li> </ul> <p></p>"},{"location":"CaRROT-CDM/ETL/Transform/","title":"Transform","text":"<p>This section describes how CO-CONNECT-Tools can be used to manually perform the Transform part of the co-connect ETL (Extract, Transform, Load). By transforming (mapping) a dataset (series of input <code>.csv</code> files) given a set of rules defined in a <code>json</code> file.</p> <p>The following guide will take you through the main steps to make sure the tool is installed correctly and that the ETL is performed correctly.</p> <p>Note</p> <p>Running the transform part of the ETL process with <code>carrot run map OPTIONS</code> instead of <code>carrot etl --config &lt;file&gt;</code> gives you a lot more control over the command line options, although all options can be parsed as key-word-arguments in the config <code>yaml</code> used with the <code>carrot etl</code></p>"},{"location":"CaRROT-CDM/ETL/Transform/#1-install-the-tool","title":"1. Install the tool","text":"<p>Follow the install guide here:</p> <p>Install Guide</p>"},{"location":"CaRROT-CDM/ETL/Transform/#2-gather-inputs","title":"2. Gather Inputs","text":"<p>To run the transformation to CDM you will need:   </p> <ol> <li>Input Data, in the form of <code>csv</code> files</li> <li><code>json</code> file containing the \"mapping rules\"</li> </ol>"},{"location":"CaRROT-CDM/ETL/Transform/#3-check-inputs","title":"3. Check Inputs","text":"<p>Input data is expected in <code>csv</code> format, </p> <p>It is possible to do a quick check to display the first 10 rows of an input <code>csv</code>. Run: <pre><code>carrot display dataframe --head 10 &lt;input data csv file&gt; \n</code></pre></p> <p>Example</p> Unix UsersWindows Users <p>A test dataset is located in the install folder (<code>carrot info install_folder</code>) <pre><code>carrot display dataframe --head 10 $(carrot info install_folder)/data/test/inputs/Demographics.csv\n</code></pre></p> <p>A test dataset is located in the install folder (<code>carrot info install_folder</code>), by listing this directory (<code>dir</code>) you can find the full path to an example file to test inplace of <code>&lt;input data csv file&gt;</code></p> <p>With a <code>json</code> file for the rules, you can quickly check the tool is able to read and display them via: <pre><code>carrot display json rules.json\n</code></pre></p> <p>Example</p> Unix UsersWindows Users <p>As with the input <code>csv</code> files, the test dataset comes packaged with a rules <code>json</code> file. <pre><code>carrot display json  $(carrot info install_folder)/data/test/rules/rules_14June2021.json\n</code></pre></p> <p>As with the input <code>csv</code> files, the test dataset comes packaged with a rules <code>json</code> file, which can be found via the folder <code>carrot info install_folder</code> followed by <code>\\data\\test\\rules\\</code>.</p>"},{"location":"CaRROT-CDM/ETL/Transform/#4-run-the-tool","title":"4. Run The Tool","text":"<p>The synthax for running the tool can be seen from using <code>--help</code>: <pre><code>carrot run map --help\n</code></pre> <pre><code>Usage: carrot run map [OPTIONS] [INPUTS]...\n\n  Perform OMOP Mapping given an json file and a series of input files\n\n  INPUTS should be a space separated list of individual input files or\n  directories (which contain .csv files)\n\nOptions:\n  --rules TEXT                    input json file containing all the mapping\n                                  rules to be applied  [required]\n  --indexing-conf TEXT            configuration file to specify how to start\n                                  the indexing\n  --csv-separator [;|:| |,| ]     choose a separator to use when dumping\n                                  output csv files\n  --use-profiler                  turn on saving statistics for profiling CPU\n                                  and memory usage\n  --format-level [0|1|2]          Choose the level of formatting to apply on\n                                  the output data. 0 - no formatting. 1 -\n                                  automatic formatting. 2 (default) - check\n                                  formatting (will crash if input data is not\n                                  already formatted).\n  --output-folder TEXT            define the output folder where to dump csv\n                                  files to\n  --write-mode [w|a]              force the write-mode on existing files\n  --split-outputs                 force the output files to be split into\n                                  separate files\n  --allow-missing-data            don't crash if there is data tables in rules\n                                  file that hasnt been loaded\n  --database TEXT                 define the output database where to insert\n                                  data into\n  -nc, --number-of-rows-per-chunk INTEGER\n                                  Choose the number of rows (INTEGER) of input\n                                  data to load (chunksize). The option 'auto'\n                                  will work out the ideal chunksize. Inputing\n                                  a value &lt;=0 will turn off data chunking\n                                  (default behaviour).\n  -np, --number-of-rows-to-process INTEGER\n                                  the total number of rows to process\n  --person-id-map TEXT            pass the location of a file containing\n                                  existing masked person_ids\n  --db TEXT                       instead, pass a connection string to a db\n  --merge-output                  merge the output into one file\n  --parse-original-person-id      turn off automatic conversion (creation) of\n                                  person_id to (as) Integer\n  --no-fill-missing-columns       Turn off automatically filling missing CDM\n                                  columns\n  --log-file TEXT                 specify a path for a log file\n  --max-rules INTEGER             maximum number of rules to process\n  --object TEXT                   give a list of objects by name to process\n  --table TEXT                    give a list of tables by name to process\n  --help                          Show this message and exit.\n</code></pre></p> <p>The tool requires you to pass a <code>.json</code> file for the rules, as well as space separated list of <code>.csv</code> files </p> <pre><code>carrot run map --rules &lt;.json file for rules&gt; &lt;csv file 1&gt; &lt;csv file 2&gt; &lt;csv file 3&gt; ...\n</code></pre> <p>Example</p> Unix UsersWindows Users <p>For macOS/Ubuntu/Centos etc. users, you can easily run from the CLI with a wildcard. Assuming your input data is located in the folder <code>data/</code> you can run:</p> <pre><code>carrot run map --rules $(carrot info install_folder)/data/test/rules/rules_14June2021.json  $(carrot info install_folder)/data/test/inputs/*.csv\n</code></pre> <p>The tool has the capability to also run on a folder containing the <code>.csv</code> files. The tool will look in the folder for <code>.csv</code> files and load them:</p> <pre><code>carrot run map --rules $(carrot info install_folder)/data/test/rules/rules_14June2021.json  $(carrot info install_folder)/data/test/inputs/\n</code></pre> <p>For Windows users, you can run by passing the full-path to the folder containing your <code>.csv</code> input files.</p> <pre><code>carrot run map --rules rules.json D:\\Foo\\Bar\\data\n</code></pre> <p>Or by manually passing the individual input csv files:</p> <pre><code>carrot run map --rules rules.json D:\\Foo\\Bar\\data\\file_1.csv D:\\Foo\\Bar\\data\\file_2.csv\n</code></pre> <p>Wildcards for inputs ....  THIS NEEDS INSTRUCTIONS FOR WINDOWS USERS </p>"},{"location":"CaRROT-CDM/ETL/Transform/#5-check-the-output","title":"5. Check The Output","text":"<p>By default, mapped <code>tsv</code> files are created in the folder <code>output_data</code> within your current working directory.</p> <p>Tip</p> <p>To specify a different output folder, use the command line argument <code>--output-folder</code> when running <code>carrot map run</code></p> <p>Log files are also created in a subdirectory of the output folder, for example: <pre><code>output_data/\n\u251c\u2500\u2500 condition_occurrence.tsv\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2021-09-13T101629_slice_0.json\n\u251c\u2500\u2500 observation.tsv\n\u2514\u2500\u2500 person.tsv\n</code></pre></p> <p>Other than opening up the output csv in your favourite viewer, you can also use the command line tools to display a simple dataframe <pre><code>carrot display dataframe --drop-na output_data/condition_occurrence.tsv\n</code></pre> <pre><code>       condition_occurrence_id  person_id  condition_concept_id  ... condition_end_datetime condition_source_value  condition_source_concept_id\n0                            1          9                312437  ...    2020-04-10 00:00:00                      1                       312437\n1                            2         18                312437  ...    2020-04-11 00:00:00                      1                       312437\n2                            3         28                312437  ...    2020-04-10 00:00:00                      1                       312437\n3                            4         38                312437  ...    2020-04-10 00:00:00                      1                       312437\n4                            5         44                312437  ...    2020-04-10 00:00:00                      1                       312437\n</code></pre></p> <p>Markdown format can be outputed for convenience too: <pre><code>carrot display dataframe --markdown --drop-na output_data/person.tsv\n</code></pre></p> person_id gender_concept_id birth_datetime gender_source_value gender_source_concept_id 0 101 8507 1951-12-25 00:00:00 M 8507 1 102 8507 1981-11-19 00:00:00 M 8507 2 103 8532 1997-05-11 00:00:00 F 8532 3 104 8532 1975-06-07 00:00:00 F 8532 4 105 8532 1976-04-23 00:00:00 F 8532 5 106 8507 1966-09-29 00:00:00 M 8507 6 107 8532 1956-11-12 00:00:00 F 8532 7 108 8507 1985-03-01 00:00:00 M 8507 8 109 8532 1950-10-31 00:00:00 F 8532 9 110 8532 1993-09-07 00:00:00 F 8532 <p>Note</p> <p>The argument <code>--drop-na</code> removes columns that are entirely <code>nan</code>/<code>null</code> in order to get a better display of the mapped CDM table, this is because many of the CDM table columns are not used in the examples.</p>"},{"location":"CaRROT-CDM/ETL/Transform/#6-additional-options","title":"6. Additional Options","text":"<p>To override the default data chunking options and process a dataset in specific chunks, you can use the following flag:</p> <pre><code>  -nc, --number-of-rows-per-chunk INTEGER\n                                  choose to chunk running the data into nrows\n</code></pre> <p>For a dataset of order millions of records, it is advised that you run with <code>-nc 100000</code> which will process the data in 100k row chunks. </p> <p>For testing or debugging purposes, especially if you are transforming a large dataset, you can use the option <code>-np</code> to only run on a certain number of initial rows of the input <code>csv</code> files.  <pre><code>  -np, --number-of-rows-to-process INTEGER\n                                  the total number of rows to process\n</code></pre></p> <p>If you wish to profile the CPU/memory usage as a function of time, you can run with the following flag: <pre><code>  --use-profiler                  turn on saving statistics for profiling CPU\n                                  and memory usage\n</code></pre></p> <p>Which will additionally save and output a time series from the start of executing to the end of executing the ETL. <pre><code>2021-07-27 10:31:48 - Profiler - INFO -      time[s]  memory[GB]  cpu[%]\n0   0.000384    0.056290   0.000\n1   0.104976    0.057865  24.650\n2   0.205735    0.058556  23.325\n3   0.308194    0.060932  24.625\n4   0.415116    0.061394  24.650\n...\n2021-07-27 10:31:48 - Profiler - INFO - finished profiling\n2021-07-27 10:31:48 - CommonDataModel - INFO - Writen the memory/cpu statistics to /Users/calummacdonald/Usher/CO-CONNECT/Software/carrot-cdm/alspac/output_data//logs//statistics_2021-07-27T093143.csv\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/","title":"Yaml","text":"<p>The following guide documents how you can use the co-connect command line tools to automate the ETL process for by defining a <code>yaml</code> configuration file.</p> <p>ETL workflows, configured with a <code>yaml</code> file are executed with the synthax <code>carrot etl --config &lt;path to config file&gt; [optional additional COMMAND]</code> </p> <p><pre><code>carrot etl --config config.yml --help\n</code></pre> <pre><code>Usage: carrot etl [OPTIONS] COMMAND [ARGS]...\n\n  Command group for running the full ETL of a dataset\n\nOptions:\n  --config, --config-file TEXT  specify a yaml configuration file\n  -d, --daemon                  run the ETL as a daemon process\n  -l, --log-file TEXT           specify the log file to write to\n  --help                        Show this message and exit.\n\nCommands:\n  check-tables   check tables\n  clean-table    clean (delete all rows) of a given table name\n  clean-tables   clean (delete all rows) in the tables defined in the...\n  create-tables  create new bclink tables\n  delete-tables  delete some tables\n</code></pre></p> <p>To run the full ETL you need a <code>.yml</code>(or <code>.yaml</code>) file to configure various settings.</p> <p> Example yamls </p>"},{"location":"CaRROT-CDM/ETL/Yaml/#setup-your-yaml-config","title":"Setup your YAML config","text":"<p>Here are some details on how you can setup a yaml configuration file</p>"},{"location":"CaRROT-CDM/ETL/Yaml/#get-to-the-point","title":"Get to the point...","text":"<p>\"TL;DR, I want a yaml configuration file that I can run out the box...\"</p> Locally <p>Run on some input data and perform a local load by merging the split outputs (e.g. create one file called <code>person.tsv</code> in the output folder) <pre><code>settings:\n    listen_for_changes: true\n\nload: &amp;load-local\n    output: cache/\n    merge_output: true\n\ntransform:\n    settings: &amp;settings\n        rules: demo-dataset/data/rules_small.json\n    data:\n        - input: demo-dataset/data/part1/\n          &lt;&lt;: *settings\n          &lt;&lt;: *load-local\n</code></pre></p> <p>Warning</p> <p>The default behaviour is to load all data into memory and begin processing. This behaviour is not optimal for very large datasets, or if your computing environment has a small amount of memory (or you have a crash due to memory usage when running).</p> <p>To change the chunking of data, you can tell the tool to only read in X number of rows at a time, like so: <pre><code>- input: ...\n  ...\n  number_of_rows_per_chunk: 10000\n</code></pre></p> <p>The names of these can be found in the source documentation, corresponding to the options you will see via the command <code>carrot run map --help</code>. For example, to not perform any column formatting and to not automatically fill missing columns (e.g. <code>year_of_birth</code> in the person table): <pre><code>- input: ...\n  ...\n  dont_automatically_fill_missing_columns: true\n  format_level: 0\n</code></pre></p> BCLink <p>Configuration to upload to BCLink. Will wait for changes (additional data added to the transform section, changes to the rules file etc...) <pre><code>settings:\n    listen_for_changes: true\n    clean: true\nload: &amp;load-bclink\n    cache: /usr/lib/bcos/MyWorkingDirectory/Temp/cache/\n    bclink:\n        dry_run: false\ntransform:\n    settings: &amp;settings\n        output: *load-bclink\n        rules: /usr/lib/bcos/MyWorkingDirectory/Temp/demo-dataset/data/rules.json\n    data:\n    - input: /usr/lib/bcos/MyWorkingDirectory/Temp/demo-dataset/data/part1/\n      &lt;&lt;: *settings\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#transform-tab","title":"Transform Tab","text":"<p>This tab specifies how the data is going to be transformed.</p> Configuration <p>The following is the most basic example of a configuration, where the rules, input data, output data folder and tables names are specified as such: <pre><code>transform:\n   data:\n     - input: &lt;input data folder&gt;\n       output: &lt;output folder location&gt;\n       rules: &lt;rules.json location&gt;\n</code></pre> When this configuration is executed, a local storage folder is used to write the files, i.e. there is no load performed. Effectively this is just the T in the ETL.</p> <p>Example: <pre><code>transform:\n   data:\n     - input: /usr/lib/bcos/MyWorkingDirectory/Temp/demo-dataset/data/part1/\n       rules: /usr/lib/bcos/MyWorkingDirectory/Temp/demo-dataset/data/rules.json\n       output: /usr/lib/bcos/MyWorkingDirectory/Temp/cache/basic/\n</code></pre></p> <p>Additional valid keyword arguments to the <code>map</code> function (which can be found in in <code>def map</code>), can be passed here: <pre><code>transform:\n   data:\n     - input: &lt;input data folder&gt;\n       output: &lt; output folder location&gt;\n       rules: &lt; rules.json location&gt;\n       split_outputs: &lt;true/false if to save the outputs as one file e.g. person.tsv or not&gt;\n       number_of_rows_to_process: &lt;number of rows of data to run over&gt;\n       dont_automatically_fill_missing_columns: &lt;true/false dont automatically fill missing columns e.g. year_of_birth from birth_datetime&gt;\n</code></pre></p> <p>Multiple input folders can be added: <pre><code>transform:\n   data:\n     - input: &lt; input data folder&gt;\n       output: &lt; output folder location&gt;\n       rules: &lt; rules.json location&gt;\n     - input: &lt; another input data folder&gt;\n       output: &lt; output folder location&gt;\n       rules: &lt; rules.json location&gt;\n</code></pre> Anchors can be used to reduce the number of repeats: <pre><code>transform:\n   settings: &amp;settings\n     output: &lt; output folder location&gt;\n     rules: &lt; rules.json location&gt;\n   data:\n     - input: &lt; input data folder&gt;\n       &lt;&lt;:*settings\n     - input: &lt; another input data folder&gt; \n       &lt;&lt;:*settings\n</code></pre></p> <p>Alternatively the data tab can be a master/main folder, containing subfolders of multiple data dumps. If specified as follows, the code will use the main folder to look for subfolders to process. <pre><code>transform:\n   data:\n     input: &lt; input main folder&gt;\n     output: &lt; output folder location&gt;\n     rules: &lt; rules.json location&gt;\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#load-tab","title":"Load Tab","text":"Local Configuration <p>A very basic example shows how the load tab can be used to do the same as above (load/dump to a local folder i.e. <code>LocalDataCollection</code>) <pre><code>load: &amp;load-local\n   output: &lt; folder for the output&gt;\ntransform:\n   settings: &amp;settings\n     output: *load-local\n     rules: &lt; rules.json location&gt;\n   data:\n     - input: &lt; input data folder&gt;\n       &lt;&lt;:*settings\n</code></pre></p> BCLink Configuration <p>To specify we want to load to BCLink instead, we can create a load tab section as so: <pre><code>load: &amp;load-bclink\n  cache: /usr/lib/bcos/MyWorkingDirectory/Temp/cache/\n  bclink:\n    dry_run: false\ntransform:\n   settings: &amp;settings\n     output: *load-bclink\n     rules: &lt; rules.json location&gt;\n   data:\n     - input: &lt; input data folder&gt;\n       &lt;&lt;:*settings\n</code></pre> By specifying that the output should go to the <code>load-bclink</code> configuration, the tool will be able to write the output to a <code>BCLinkDataCollection</code>.</p> <p>The default behaviour is to assume that the destination table (e.g. <code>person</code> table) should be loaded to a BCLink table which exactly the same name.  If this is not the case, you must manually specify the names of the tables so the tool knows which BCLink table it should upload to.</p> <pre><code>load: &amp;load-bclink\n  cache: &lt; cache directory for writing files&gt;\n  bclink:\n     tables:\n        &lt;cdm table&gt;: &lt;bclink table&gt;\n        ...\n</code></pre> <p>Example: <pre><code>load: &amp;load-bclink\n  cache: /usr/lib/bcos/MyWorkingDirectory/Temp/cache/\n  bclink:\n     tables:\n        person: ds100000\n        observation: ds100001\n        measurement: ds100003\n        .....\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#extract-tab","title":"Extract Tab","text":"<p>By defining an extract tab in the <code>yaml</code> you are able to execute code before the transform is run.</p> Configuration <p>The following shows how a bash command can be defined to run on an input and extract an output. It's important that you define an extract job that will run on a single input and output a single output. The configuration must specify a <code>{input}</code> and a  <code>{output}</code>.</p> <p>Any configuration must be also hooked up with the transform part, so the tool knows that it should perform the extract before processing..</p> <p>For example... <pre><code>extract: &amp;pseudonymise\n    bash: |\n        echo \"Going to run {input} and put output in {output}\"\n        pseudonymise csv --salt 12345 --id ID --output-folder {output} {input}\n...\ntransform:\n    data:\n        - input:\n            &lt;&lt;: *pseudonymise\n            input: demo-dataset/data/part1/\n            output: data_pseudonymised/\n          output: output/\n          rules: demo-dataset/data/rules_small.json\n</code></pre> As can be seen, in the transform tab, the <code>data.input</code> now itself specifies an <code>input</code>/<code>output</code> because the <code>*pseudonymise</code> specifies how a pre-processing on the <code>input</code> should be run to produce an <code>output</code> - which is then used as the input for transform.</p> <p>Example output, you will see the <code>pseudonymise csv</code> start to run before the transform. <pre><code>2022-03-16 11:35:46 - run_etl - INFO - running etl on config.yaml (last modified: 1647430135.1740856)\n\"Going to run demo-dataset/data/part1/Symptoms.csv and put output in data_pseudonymised/\"\n\n2022-03-16 11:35:46.656 | INFO     | cli.cli:csv:16 - Working on file demo-dataset/data/part1/Symptoms.csv, pseudonymising columns '['ID']' with salt '12345'\n2022-03-16 11:35:46.656 | INFO     | cli.cli:csv:22 - Saving new file to data_pseudonymised//Symptoms.csv\n2022-03-16 11:35:46.761 | DEBUG    | cli.cli:csv:32 - 0        e428669397a3d0c72d46f6d5afe9a8ae20ea675883c0e7...\n1        a37861e3f9bb0fd4385b7c6fddcf6d4ba366a4f3c9b17b...\n2        a37861e3f9bb0fd4385b7c6fddcf6d4ba366a4f3c9b17b...\n3        a37861e3f9bb0fd4385b7c6fddcf6d4ba366a4f3c9b17b...\n4        a37861e3f9bb0fd4385b7c6fddcf6d4ba366a4f3c9b17b...\n...\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#running-the-etl","title":"Running the ETL","text":"<p>To run the ETL-Tool given a configuration file, simply run: <pre><code>carrot etl --config config.yaml\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#daemon-mode","title":"Daemon Mode","text":"<p>To run as a background process (more specifically as a \"daemon\"), that watches for changes in a directory (ideal for data dumps of regularly updated data) the tool can be started with the flag <code>-d</code> or <code>--daemon</code>.</p>"},{"location":"CaRROT-CDM/ETL/Yaml/#start-the-etl-from-a-yaml-file","title":"Start the ETL from a yaml file","text":"<p>Start the ETL process with the following command: <pre><code>carrot etl --config config.yaml --daemon\n</code></pre> You'll see an output like: <pre><code>2022-03-16 11:10:30 - etl - INFO - running as a daemon process, logging to carrot.log\n2022-03-16 11:10:30 - etl - INFO - process_id in &lt;TimeoutPIDLockFile: 'etl.pid' -- 'etl.pid'&gt;\n</code></pre></p> <p>The file <code>etl.pid</code> will exist as long as the process is running, if you dont see this (while you specified to listen for changes), something when wrong and you should check the logs.</p>"},{"location":"CaRROT-CDM/ETL/Yaml/#check-the-logs","title":"Check the logs","text":"<p>The yaml file configures where log messages are saved. For example, you can <code>tail</code> the last two lines of the log to see the output: <pre><code>tail carrot.log\n</code></pre> <pre><code>2022-03-16 11:14:14 - CommonDataModel - INFO - working on drug_exposure\n2022-03-16 11:14:14 - CommonDataModel - INFO - starting on drug_exposure.COVID_19_vaccine_3035.0x117dff910.2022-03-16T111409\n2022-03-16 11:14:14 - CommonDataModel - INFO - finished drug_exposure.COVID_19_vaccine_3035.0x117dff910.2022-03-16T111409 (0x119468eb0) ... 1/1 completed, 23522 rows\n2022-03-16 11:14:14 - CommonDataModel - INFO - saving dataframe (0x11ba44ca0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x117dfffd0&gt;\n2022-03-16 11:14:14 - LocalDataCollection - INFO - saving drug_exposure to cache//drug_exposure.tsv\n2022-03-16 11:14:14 - LocalDataCollection - INFO - finished save to file\n2022-03-16 11:14:14 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 23522 rows from 1 tables\n2022-03-16 11:14:14 - LocalDataCollection - INFO - Getting next chunk of data\n2022-03-16 11:14:14 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-03-16 11:14:14 - run_etl - INFO - Finished!... Listening for changes every 5 seconds to data in config.yaml\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#find-the-process-id","title":"Find the process ID","text":"<p>By default a (lock) file <code>etl.pid</code> is created while the ETL process is running as a background process. The PID (process ID) is saved inside the file, e.g.: <pre><code>$ cat etl.pid \n75107\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#kill-the-daemon","title":"Kill the daemon","text":"<p>To stop the background process, you can do: <pre><code>kill -9 $(cat etl.pid)\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#additional-commands","title":"Additional Commands","text":""},{"location":"CaRROT-CDM/ETL/Yaml/#check-tables","title":"Check Tables","text":"<p>A simple command that only works if you have BCLink options setup in your <code>yaml</code> file <pre><code>carrot etl --config config.yaml check-tables\n</code></pre> The command will display information about the number of rows in the BCLink tables the configuration file configures for.</p>"},{"location":"CaRROT-CDM/ETL/Yaml/#delete-tables","title":"Delete Tables","text":"<p>The following command can be run even when the ETL tool is already running as a background process <pre><code>carrot etl --config config.yaml delete-tables \n</code></pre> Starting the command provides you with a prompt, asking which files you want to delete <pre><code>2022-03-16 11:17:10 - run_etl - INFO - running etl on config.yaml (last modified: 1647356757.6764326)\n[?] Which tables do you want to delete? ... : \n &gt; o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/person_ids.0x7f0c5783dcf8.2022-03-15T134800.tsv\n   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/person.MALE_3025.0x7f0c57843668.2022-03-15T134803.tsv\n   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/person_ids.0x7f0c56ad6ac8.2022-03-15T134807.tsv\n   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/person.FEMALE_3026.0x7f0c57843908.2022-03-15T134809.tsv\n   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/observation.Antibody_3027.0x7f0c56ad6908.2022-03-15T134814.tsv\n   ...\n</code></pre></p> <p>For example, selecting one file (by moving your keyboard arrows to navigate and select): <pre><code>   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/observation.2019_nCoV_3044.0x7f0c563fa470.2022-03-15T135156.tsv\n &gt; X /usr/lib/bcos/MyWorkingDirectory/Temp/cache/observation.Cancer_3045.0x7f0c563fa278.2022-03-15T135200.tsv\n   o /usr/lib/bcos/MyWorkingDirectory/Temp/cache/condition_occurrence.Headache_3028.0x7f0c571c3320.2022-03-15T135459.tsv\n</code></pre> Starts the process to delete this file (also deletes it from BCLink if it has been uploaded there): <pre><code>...\n2022-03-16 11:19:42 - BCLinkHelpers - INFO - Called remove_table on /usr/lib/bcos/MyWorkingDirectory/Temp/cache/observation.Cancer_3045.0x7f0c563fa278.2022-03v\n2022-03-16 11:19:42 - LocalDataCollection - INFO - DataCollection Object Created\n2022-03-16 11:19:42 - LocalDataCollection - INFO - Using a chunksize of '1000' nrows\n2022-03-16 11:19:42 - LocalDataCollection - INFO - Registering  observation [&lt;carrot.io.common.DataBrick object at 0x7fa2a0471630&gt;]\n2022-03-16 11:19:42 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'obsek\n2022-03-16 11:19:42 - BCLinkHelpers - INFO - got pk observation_id\n2022-03-16 11:19:42 - LocalDataCollection - INFO - Retrieving initial dataframe for 'observation' for the first time\n2022-03-16 11:19:42 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=DELETE FROM observation WHERE observation_id IN (82700,82701,82703,82704,827k\n2022-03-16 11:19:42 - LocalDataCollection - INFO - Getting next chunk of data\n2022-03-16 11:19:42 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'observation'\n2022-03-16 11:19:42 - LocalDataCollection - INFO - --&gt; Got 1000 rows\n...\n2022-03-16 11:19:44 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-03-16 11:19:44 - delete_tables - WARNING - removing /usr/lib/bcos/MyWorkingDirectory/Temp/cache/observation.Cancer_3045.0x7f0c563fa278.2022-03-15T135200\n</code></pre></p>"},{"location":"CaRROT-CDM/ETL/Yaml/#reference","title":"Reference","text":"<p>The following </p>"},{"location":"CaRROT-CDM/ETL/Yaml/#bclink","title":"BCLink","text":""},{"location":"CaRROT-CDM/Performance/cProfile/","title":"cProfile","text":"<p>The command-line ETL (<code>coconnect map run</code>) can be ran as a module via <code>-m coconnect.cli.cli map run</code>. To include <code>cProfile</code>, the following command can be executed to run the ETL-Tool while using <code>cProfile</code>: <pre><code>python3 -m cProfile -o profile.prof -m coconnect.cli.cli map run --rules rules.json input_data/\n</code></pre></p> <p>Which dumps the profiling output to the file <code>profile.prof</code>.</p>"},{"location":"CaRROT-CDM/notebooks/Generating%20Synthetic%20Data%20By%20Hand/","title":"Generating Synthetic Data By Hand","text":"<pre><code>from carrot.cdm.objects.common import DestinationTable, DestinationField\n\nclass Demographics(DestinationTable):\n    name = 'Demo'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.Age = DestinationField(dtype=\"Integer\", required=False)\n        self.Sex = DestinationField(dtype=\"Text50\", required=False )\n        super().__init__(self.name,type(self).__name__)\n\nclass Symptoms(DestinationTable):\n    name = 'Symptoms'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.date_occurrence = DestinationField(dtype=\"Timestamp\", required=False)                                                                                     \n        self.Headache = DestinationField(dtype=\"Text50\", required=False )\n        self.Fatigue = DestinationField(dtype=\"Text50\", required=False )\n        self.Dizzy = DestinationField(dtype=\"Text50\", required=False )\n        self.Cough = DestinationField(dtype=\"Text50\", required=False )\n        self.Fever = DestinationField(dtype=\"Text50\", required=False )\n        self.Muscle_Pain = DestinationField(dtype=\"Text50\", required=False )\n        super().__init__(self.name,type(self).__name__)\n\nclass GP_Records(DestinationTable):\n    name = 'GP_Records'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.date_of_visit = DestinationField(dtype=\"Timestamp\", required=False)                                                                                     \n        self.comorbidity = DestinationField(dtype=\"Text50\", required=False )\n        self.comorbidity_value = DestinationField(dtype=\"Float\", required=False )\n        super().__init__(self.name,type(self).__name__)\n\nclass Hospital_Visit(DestinationTable):\n    name = 'Hospital_Visit'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.admission_date = DestinationField(dtype=\"Timestamp\", required=False)   \n        self.reason = DestinationField(dtype=\"Text50\", required=False )\n        super().__init__(self.name,type(self).__name__)\n\nclass Blood_Test(DestinationTable):\n    name = 'Blood_Test'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.date_taken = DestinationField(dtype=\"Timestamp\", required=False)   \n        self.location = DestinationField(dtype=\"Text50\", required=False )\n        self.quantity = DestinationField(dtype=\"Float\", required=False )\n        super().__init__(self.name,type(self).__name__)\n\nclass Vaccinations(DestinationTable):\n    name = 'Vaccinations'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.date_of_vaccination = DestinationField(dtype=\"Timestamp\", required=False)                                                                                     \n        self.type = DestinationField(dtype=\"Text50\", required=False)\n        self.stage = DestinationField(dtype=\"Integer\", required=False)\n        super().__init__(self.name,type(self).__name__) \n\n\nclass Serology(DestinationTable):\n    name = 'Serology'\n    def __init__(self,name=None,**kwargs):\n        self.ID = DestinationField(dtype=\"Text50\", required=True)\n        self.Date = DestinationField(dtype=\"Timestamp\", required=True)\n        self.IgG = DestinationField(dtype=\"Float\", required=False )\n        super().__init__(self.name,type(self).__name__)\n</code></pre> <p>Then build a total model(dataset) based upon these tables, creating this with 50k people</p> <pre><code>import pandas as pd\nimport numpy as np\nimport datetime\nimport time\nimport io\nimport carrot\nfrom carrot.cdm import CommonDataModel\nfrom carrot.cdm import define_table\n\ndef create_gaus_time_series(mu,sigma,n):\n    mu = time.mktime(mu.timetuple())\n    sigma = (datetime.timedelta(**sigma)).total_seconds()\n    return pd.Series([datetime.date.fromtimestamp(x) for x in np.random.normal(mu,sigma,n)])\n\nclass ExampleCovid19DataSet(CommonDataModel):\n    def __init__(self):\n        \"\"\"                                                                                                                                                    \n        initialise the inputs and setup indexing                                                                                                               \n        \"\"\"  \n        #50k people\n        n = 50000\n\n\n        outputs = carrot.tools.create_sql_store(connection_string=\"postgresql://localhost:5432/ExampleCOVID19DataSet\",\n                                          drop_existing=True)\n        super().__init__(format_level=0,outputs=outputs)\n\n        #create people indexes that we can use in the different tables\n        self.people = pd.DataFrame([f'pk{i}' for i in range(1,n+1)],columns=['pks'])\n\n        #set the processing order, e.g we want to build demographics table first\n        #so that the values recorded in other tables can be demographically dependent \n        self.set_execution_order([\n            'Demographics', \n            'GP_Records', \n            'Vaccinations',\n            'Serology',\n            'Symptoms',\n            'Hospital_Visit',\n            'Blood_Test'\n        ])\n        self.process()\n\n    @define_table(Demographics)\n    def demo(self):  \n        \"\"\"\n        Straight foreward demographics\n        \"\"\"\n        self.ID.series = self.cdm.people['pks']\n        self.n = len(self.ID.series)\n        self.Age.series = pd.Series(np.random.normal(60,20,self.n)).astype(int)\n        self.Age.series = self.Age.series.mask(self.Age.series &amp;lt; 0 , None)\n        self.Sex.series = pd.Series(np.random.choice(['Male','Female',None],size=self.n,p=[0.55,0.445,0.005]))\n\n\n    @define_table(Symptoms)\n    def symptoms(self):\n        npeople = self.cdm.demo.n\n        nsymptoms = npeople*5\n\n        ID = self.cdm.demo.ID.series\n\n        self.ID.series = ID.sample(int(npeople*0.8))\\\n            .sample(nsymptoms,replace=True)\\\n            .sort_values().reset_index(drop=True)  \n\n        self.date_occurrence.series = create_gaus_time_series(mu=datetime.datetime(2021,1,1),\n                                                              sigma={'days':365},\n                                                              n=nsymptoms)\n\n        self.date_occurrence.series.loc[self.date_occurrence.series.sample(frac=0.005).index] = np.nan\n\n        syms_probs = {'Headache':0.8,'Fatigue':0.7,'Dizzy':0.4,'Cough':0.7,'Fever':0.2,'Muscle_Pain':0.1}\n        for key,p in syms_probs.items():\n            series = pd.Series(np.random.choice(['Yes','No'],size=nsymptoms,p=[p,1-p]))\n            setattr(getattr(self,key),'series',series)\n\n    @define_table(Serology)\n    def serology(self):\n\n        def calc_IgG(age,sex,nrisks):\n            scale = 50*(1 - age/200)*(1.1 if sex=='Female' else 1.0)*(1/nrisks)\n            return np.random.exponential(scale=scale)\n\n        df_gp = self.cdm.gp.get_df()\n        df_nrisks = df_gp['comorbidity'].groupby(df_gp.index)\\\n                    .count()\n        df_nrisks.name ='nrisks'\n\n        df = self.cdm.demo.get_df().join(df_nrisks).reset_index()\n        df['nrisks'] = df['nrisks'].fillna(1)\n\n        df = df[df['Age']&amp;gt;18].sample(frac=0.3)\n        nstudies = len(df)\n\n        df = df.sample(frac=1.4,replace=True).reset_index()\n\n        df['IgG'] = df.apply(lambda x : calc_IgG(x.Age,x.Sex,x.nrisks),axis=1)\n        df.sort_values('ID',inplace=True)\n\n        self.IgG.series = df['IgG']\n        self.ID.series = df['ID']\n        self.Date.series = create_gaus_time_series(mu=datetime.datetime(2021,5,1),\n                                                              sigma={'days':365},\n                                                              n=len(df))\n\n    @define_table(GP_Records)\n    def gp(self):\n\n        def calc_comoribidites(age):\n            if pd.isna(age):\n                return []   \n            comorbidities = {\n                'Mental Health':0.3*(1 + age/90) ,\n                'Diabetes Type-II':0.15*(1 + age/70) ,\n                'Heart Condition':0.1*(1 + age/50) ,\n                'High Blood Pressure':0.07*(1 + age/60),\n                'BMI': 1\n            }\n            return [x for x,p in comorbidities.items() if np.all(np.random.uniform() &amp;lt; p) ]\n\n        #90% of people have a GP visit record\n        df = self.cdm.demo.get_df().sample(frac=0.9).reset_index()\n\n        df['comorbidity'] = df.apply(lambda x: calc_comoribidites(x.Age),axis=1)\n        df['date_of_observation'] = create_gaus_time_series(mu=datetime.datetime(2010,5,1),\n                                                              sigma={'days':700},\n                                                              n=len(df))\n\n        df = df.explode('comorbidity').set_index('ID').sort_index()\n\n        self.ID.series = df.index.to_series()\n        self.comorbidity.series = df['comorbidity']\n        self.comorbidity_value.series = df['comorbidity'].apply(lambda x: np.random.exponential(scale=20)\n                                                                if x == 'BMI' else 1)\n        self.date_of_visit.series = df['date_of_observation']\n\n    @define_table(Hospital_Visit) \n    def hospital(self):\n\n        n = len(self.cdm.demo.ID.series)\n\n        #5% of people have had a hospital visit\n        #some of those have multiple visists\n        self.ID.series = self.cdm.demo.ID.series.sample(n)\\\n                        .sample(int(n*1.2),replace=True)\\\n                        .sort_values().reset_index(drop=True)  \n\n        n = len(self.ID.series)\n        self.admission_date.series = create_gaus_time_series(mu=datetime.datetime(2020,5,1),\n                                                              sigma={'days':300},\n                                                              n=n)\n\n        reasons = {\n            'Kidney Operation':0.1,\n            'Appendix Operation':0.1,\n            'Heart Attack':0.2,\n            'COVID-19':0.15,\n            'Pneumonia':0.15,\n            'Cancer':0.3\n        }\n\n        self.reason.series = pd.Series(np.random.choice(list(reasons.keys()),size=n,p=list(reasons.values())))\n\n    @define_table(Blood_Test)\n    def bloods(self):\n        #half of the people with hospital visits have blood taken\n        df_hospital = self.cdm.hospital.get_df().sample(frac=0.5).reset_index()\n\n        self.ID.series = df_hospital['ID']\n        self.date_taken.series = pd.to_datetime(df_hospital['admission_date']) \\\n                               + datetime.timedelta(days=np.random.uniform(0,5))\n\n        n = len(df_hospital)\n        self.location.series = pd.Series(np.random.choice(['Right Arm','Left Arm','Small Intestine','Abdominal Wall'],\n                                                   size=n,\n                                                   p=[0.3,0.3,0.2,0.2]))\n        self.quantity.series = pd.Series((np.random.exponential(scale=1.5) for _ in range(0,n)))\n\n    @define_table(Vaccinations)\n    def first_covid_vaccination(self):\n\n        def calc_date_of_vacc(age):\n            if pd.isna(age):\n                return np.nan\n            start_date = datetime.datetime(2021,1,1)\n            tdelta = datetime.timedelta(days=(300-age*2)+np.random.uniform(0,50))\n\n            return start_date + tdelta\n\n        #95% of people have had a vaccination\n        df = self.cdm.demo.get_df().sample(frac=0.9).reset_index()\n\n        self.ID.series = df['ID']\n        self.date_of_vaccination.series =  df.apply(lambda x : calc_date_of_vacc(x.Age),axis=1)\n        n = len(self.ID.series)\n        self.type.series = pd.Series(np.random.choice(['Moderna','AstraZenica','Pfizer'],size=n,p=[0.34,0.33,0.33]))\n        self.stage.series = pd.Series((0 for _ in range(0,n)))\n\n    @define_table(Vaccinations)\n    def second_covid_vaccination(self):\n\n        def calc_date_of_vacc(age):\n            if pd.isna(age):\n                return np.nan\n            start_date = datetime.datetime(2021,1,1)\n            tdelta = datetime.timedelta(days=(300-age*2)+np.random.uniform(0,50))\n\n            return start_date + tdelta\n\n        #80% of people who had 1st had 2nd\n        df = self.cdm.first_covid_vaccination.get_df().sample(frac=0.8).reset_index()\n\n        self.ID.series = df['ID']\n        self.date_of_vaccination.series =  pd.to_datetime(df['date_of_vaccination']) \\\n                                           + datetime.timedelta(days=(50+np.random.uniform(0,50)))\n        n = len(self.ID.series)\n        self.type.series = pd.Series(np.random.choice(['Moderna','AstraZenica','Pfizer'],size=n,p=[0.34,0.33,0.33]))\n        self.stage.series = pd.Series((1 for _ in range(0,n)))\n</code></pre> <pre><code>model = ExampleCovid19DataSet()\nmodel\n</code></pre> <pre>\n<code>2022-06-17 15:03:24 - SqlDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:03:25 - SqlDataCollection - INFO - Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Turning on automatic cdm column filling\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added bloods of type Blood_Test\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added demo of type Demographics\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added first_covid_vaccination of type Vaccinations\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added gp of type GP_Records\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added hospital of type Hospital_Visit\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added second_covid_vaccination of type Vaccinations\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added serology of type Serology\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Added symptoms of type Symptoms\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Starting processing in order: ['Demographics', 'GP_Records', 'Vaccinations', 'Serology', 'Symptoms', 'Hospital_Visit', 'Blood_Test']\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - Number of objects to process for each table...\n{\n      \"Blood_Test\": 1,\n      \"Demographics\": 1,\n      \"Vaccinations\": 2,\n      \"GP_Records\": 1,\n      \"Hospital_Visit\": 1,\n      \"Serology\": 1,\n      \"Symptoms\": 1\n}\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - for Demographics: found 1 object\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - working on Demographics\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - starting on demo\n2022-06-17 15:03:25 - Demographics - INFO - Not formatting data columns\n2022-06-17 15:03:25 - Demographics - INFO - created df (0x1076b2a90)[demo]\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - finished demo (0x1076b2a90) ... 1/1 completed, 50000 rows\n2022-06-17 15:03:25 - ExampleCovid19DataSet - ERROR - Removed 49652 row(s) due to duplicates found when merging Demographics\n2022-06-17 15:03:25 - ExampleCovid19DataSet - WARNING - Example duplicates...\n2022-06-17 15:03:25 - ExampleCovid19DataSet - WARNING -        Age     Sex\nID                \npk1   62.0    Male\npk2   56.0  Female\npk3   37.0    Male\npk4   57.0  Female\npk5   55.0    Male\npk6   95.0    Male\npk7   49.0    Male\npk8   76.0    Male\npk9   90.0  Female\npk10  62.0    Male\n2022-06-17 15:03:25 - ExampleCovid19DataSet - INFO - saving dataframe (0x1076b22b0) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:03:25 - SqlDataCollection - INFO - updating Demographics in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:03:25 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:03:26 - ExampleCovid19DataSet - INFO - finalised Demographics on iteration 0 producing 50000 rows from 1 tables\n2022-06-17 15:03:26 - ExampleCovid19DataSet - INFO - for GP_Records: found 1 object\n2022-06-17 15:03:26 - ExampleCovid19DataSet - INFO - working on GP_Records\n2022-06-17 15:03:26 - ExampleCovid19DataSet - INFO - starting on gp\n2022-06-17 15:03:31 - GP_Records - INFO - Not formatting data columns\n2022-06-17 15:03:31 - GP_Records - INFO - created df (0x10bc8f070)[gp]\n2022-06-17 15:03:31 - ExampleCovid19DataSet - INFO - finished gp (0x10bc8f070) ... 1/1 completed, 96181 rows\n2022-06-17 15:03:31 - ExampleCovid19DataSet - ERROR - Removed 39336 row(s) due to duplicates found when merging GP_Records\n2022-06-17 15:03:31 - ExampleCovid19DataSet - WARNING - Example duplicates...\n2022-06-17 15:03:31 - ExampleCovid19DataSet - WARNING -         date_of_visit       comorbidity  comorbidity_value\nID                                                        \npk1        2007-10-21  Diabetes Type-II                1.0\npk10       2009-01-03     Mental Health                1.0\npk100      2010-04-11     Mental Health                1.0\npk100      2010-04-11  Diabetes Type-II                1.0\npk1000     2006-10-08     Mental Health                1.0\npk10000    2012-02-26     Mental Health                1.0\npk10001    2007-07-17   Heart Condition                1.0\npk10002    2012-12-29  Diabetes Type-II                1.0\npk10002    2012-12-29   Heart Condition                1.0\npk10003    2010-04-01     Mental Health                1.0\n2022-06-17 15:03:31 - ExampleCovid19DataSet - INFO - saving dataframe (0x10b690b50) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:03:31 - SqlDataCollection - INFO - updating GP_Records in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:03:37 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:03:37 - ExampleCovid19DataSet - INFO - finalised GP_Records on iteration 0 producing 96181 rows from 1 tables\n2022-06-17 15:03:37 - ExampleCovid19DataSet - INFO - for Vaccinations: found 2 objects\n2022-06-17 15:03:37 - ExampleCovid19DataSet - INFO - working on Vaccinations\n2022-06-17 15:03:37 - ExampleCovid19DataSet - INFO - starting on first_covid_vaccination\n2022-06-17 15:03:39 - Vaccinations - INFO - Not formatting data columns\n2022-06-17 15:03:39 - Vaccinations - INFO - created df (0x10c0998b0)[first_covid_vaccination]\n2022-06-17 15:03:39 - ExampleCovid19DataSet - INFO - finished first_covid_vaccination (0x10c0998b0) ... 1/2 completed, 45000 rows\n2022-06-17 15:03:39 - ExampleCovid19DataSet - INFO - starting on second_covid_vaccination\n2022-06-17 15:03:39 - Vaccinations - INFO - Not formatting data columns\n2022-06-17 15:03:39 - Vaccinations - INFO - created df (0x10bd35070)[second_covid_vaccination]\n2022-06-17 15:03:39 - ExampleCovid19DataSet - INFO - finished second_covid_vaccination (0x10bd35070) ... 2/2 completed, 36000 rows\n2022-06-17 15:03:39 - ExampleCovid19DataSet - ERROR - Removed 64 row(s) due to duplicates found when merging Vaccinations\n2022-06-17 15:03:39 - ExampleCovid19DataSet - WARNING - Example duplicates...\n2022-06-17 15:03:39 - ExampleCovid19DataSet - WARNING -                 type  stage\nID                         \npk40950  AstraZenica      0\npk44515  AstraZenica      0\npk33656  AstraZenica      0\npk23624  AstraZenica      0\npk49043      Moderna      0\npk12327  AstraZenica      0\npk45916       Pfizer      0\npk22064       Pfizer      0\npk23701      Moderna      0\npk2449   AstraZenica      0\n2022-06-17 15:03:39 - ExampleCovid19DataSet - INFO - saving dataframe (0x1076b29a0) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n</code>\n</pre> <pre>\n<code>2022-06-17 15:03:39 - SqlDataCollection - INFO - updating Vaccinations in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:03:48 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:03:48 - ExampleCovid19DataSet - INFO - finalised Vaccinations on iteration 0 producing 81000 rows from 2 tables\n2022-06-17 15:03:48 - ExampleCovid19DataSet - INFO - for Serology: found 1 object\n2022-06-17 15:03:48 - ExampleCovid19DataSet - INFO - working on Serology\n2022-06-17 15:03:48 - ExampleCovid19DataSet - INFO - starting on serology\n2022-06-17 15:03:49 - Serology - INFO - Not formatting data columns\n2022-06-17 15:03:49 - Serology - INFO - created df (0x10bd35be0)[serology]\n2022-06-17 15:03:49 - ExampleCovid19DataSet - INFO - finished serology (0x10bd35be0) ... 1/1 completed, 20591 rows\n2022-06-17 15:03:49 - ExampleCovid19DataSet - INFO - saving dataframe (0x10b6b6cd0) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:03:49 - SqlDataCollection - INFO - updating Serology in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:03:51 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:03:51 - ExampleCovid19DataSet - INFO - finalised Serology on iteration 0 producing 20591 rows from 1 tables\n2022-06-17 15:03:51 - ExampleCovid19DataSet - INFO - for Symptoms: found 1 object\n2022-06-17 15:03:51 - ExampleCovid19DataSet - INFO - working on Symptoms\n2022-06-17 15:03:51 - ExampleCovid19DataSet - INFO - starting on symptoms\n2022-06-17 15:03:53 - Symptoms - INFO - Not formatting data columns\n2022-06-17 15:03:53 - Symptoms - INFO - created df (0x1265ee490)[symptoms]\n2022-06-17 15:03:53 - ExampleCovid19DataSet - INFO - finished symptoms (0x1265ee490) ... 1/1 completed, 250000 rows\n2022-06-17 15:03:53 - ExampleCovid19DataSet - ERROR - Removed 193327 row(s) due to duplicates found when merging Symptoms\n2022-06-17 15:03:53 - ExampleCovid19DataSet - WARNING - Example duplicates...\n2022-06-17 15:03:53 - ExampleCovid19DataSet - WARNING -      date_occurrence Headache Fatigue Dizzy Cough Fever Muscle_Pain\nID                                                                 \npk1       2021-01-24      Yes     Yes    No   Yes    No         Yes\npk1       2019-05-30      Yes     Yes    No    No   Yes          No\npk1       2021-05-16      Yes      No    No    No   Yes          No\npk1       2022-06-11      Yes     Yes   Yes   Yes    No          No\npk1       2020-06-18      Yes     Yes   Yes   Yes   Yes         Yes\npk1       2021-02-04      Yes      No    No   Yes    No          No\npk10      2021-11-24      Yes     Yes    No   Yes   Yes          No\npk10      2018-12-15       No     Yes    No   Yes    No          No\npk10      2020-12-28      Yes      No    No   Yes    No          No\npk10      2023-04-08      Yes     Yes   Yes   Yes    No          No\n2022-06-17 15:03:53 - ExampleCovid19DataSet - INFO - saving dataframe (0x10bd3c220) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:03:53 - SqlDataCollection - INFO - updating Symptoms in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:04:00 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - finalised Symptoms on iteration 0 producing 250000 rows from 1 tables\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - for Hospital_Visit: found 1 object\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - working on Hospital_Visit\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - starting on hospital\n2022-06-17 15:04:00 - Hospital_Visit - INFO - Not formatting data columns\n2022-06-17 15:04:00 - Hospital_Visit - INFO - created df (0x12660f580)[hospital]\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - finished hospital (0x12660f580) ... 1/1 completed, 60000 rows\n2022-06-17 15:04:00 - ExampleCovid19DataSet - ERROR - Removed 51290 row(s) due to duplicates found when merging Hospital_Visit\n2022-06-17 15:04:00 - ExampleCovid19DataSet - WARNING - Example duplicates...\n2022-06-17 15:04:00 - ExampleCovid19DataSet - WARNING -         admission_date              reason\nID                                        \npk1         2020-03-20              Cancer\npk1000      2019-02-14           Pneumonia\npk10001     2019-10-10  Appendix Operation\npk10001     2021-01-13              Cancer\npk10001     2018-11-30            COVID-19\npk10001     2021-01-21            COVID-19\npk10002     2020-05-31              Cancer\npk10004     2020-02-12              Cancer\npk10004     2020-05-27    Kidney Operation\npk10005     2020-11-13           Pneumonia\n2022-06-17 15:04:00 - ExampleCovid19DataSet - INFO - saving dataframe (0x10bd3c280) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:04:00 - SqlDataCollection - INFO - updating Hospital_Visit in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:04:01 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:01 - ExampleCovid19DataSet - INFO - finalised Hospital_Visit on iteration 0 producing 60000 rows from 1 tables\n2022-06-17 15:04:01 - ExampleCovid19DataSet - INFO - for Blood_Test: found 1 object\n2022-06-17 15:04:01 - ExampleCovid19DataSet - INFO - working on Blood_Test\n2022-06-17 15:04:01 - ExampleCovid19DataSet - INFO - starting on bloods\n2022-06-17 15:04:02 - Blood_Test - INFO - Not formatting data columns\n2022-06-17 15:04:02 - Blood_Test - INFO - created df (0x126afcdf0)[bloods]\n2022-06-17 15:04:02 - ExampleCovid19DataSet - INFO - finished bloods (0x126afcdf0) ... 1/1 completed, 30000 rows\n2022-06-17 15:04:02 - ExampleCovid19DataSet - INFO - saving dataframe (0x10bd42ee0) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x107742100&gt;\n2022-06-17 15:04:02 - SqlDataCollection - INFO - updating Blood_Test in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:04:06 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:06 - ExampleCovid19DataSet - INFO - finalised Blood_Test on iteration 0 producing 30000 rows from 1 tables\n</code>\n</pre> <pre>\n<code>&lt;__main__.ExampleCovid19DataSet at 0x10b6280d0&gt;</code>\n</pre> <pre><code>model.keys()\n</code></pre> <pre>\n<code>dict_keys(['Demographics', 'GP_Records', 'Vaccinations', 'Serology', 'Symptoms', 'Hospital_Visit', 'Blood_Test'])</code>\n</pre> <p>retrieve the dataframes from the model</p> <pre><code>model['Demographics']\n</code></pre> Age Sex ID pk1 62.0 Male pk2 56.0 Female pk3 37.0 Male pk4 57.0 Female pk5 55.0 Male ... ... ... pk45025 124.0 Male pk47056 29.0 None pk49115 61.0 None pk49903 31.0 None pk49979 139.0 Male <p>348 rows \u00d7 2 columns</p> <pre><code>model['GP_Records']\n</code></pre> date_of_visit comorbidity comorbidity_value ID pk1 2007-10-21 Diabetes Type-II 1.000000 pk1 2007-10-21 Heart Condition 1.000000 pk1 2007-10-21 BMI 48.725715 pk10 2009-01-03 Mental Health 1.000000 pk10 2009-01-03 BMI 23.007635 ... ... ... ... pk9994 2009-08-09 BMI 58.675133 pk9995 2012-09-06 BMI 46.737545 pk9996 2010-04-13 BMI 1.468812 pk9998 2013-02-05 BMI 9.818068 pk9999 2011-05-23 BMI 17.832532 <p>56845 rows \u00d7 3 columns</p> <pre><code>model['Vaccinations']\n</code></pre> date_of_vaccination type stage ID pk32019 2021-07-18 21:19:36.566949 Moderna 0 pk32821 2021-09-07 23:08:09.986958 AstraZenica 0 pk40642 2021-08-17 22:21:06.467002 Pfizer 0 pk30449 2021-08-31 08:30:15.911471 AstraZenica 0 pk34545 2021-06-26 14:03:28.892069 Moderna 0 ... ... ... ... pk31142 2021-07-08 00:53:53.763942 Pfizer 1 pk22055 2021-08-17 01:57:34.407304 AstraZenica 1 pk38363 2021-11-21 07:48:46.199824 AstraZenica 1 pk38067 2021-10-21 18:15:40.375532 Moderna 1 pk39445 2021-11-05 17:19:06.735946 Pfizer 1 <p>80936 rows \u00d7 3 columns</p> <pre><code>model['Serology']\n</code></pre> Date IgG ID pk10001 2021-03-17 13.701676 pk10001 2021-07-29 1.077413 pk10005 2020-06-06 39.366639 pk10005 2020-03-02 56.358177 pk10009 2021-05-03 55.585361 ... ... ... pk997 2020-12-14 1.720815 pk9978 2020-11-10 52.239568 pk9982 2023-05-12 5.753619 pk9989 2022-12-16 37.616017 pk9989 2022-12-14 7.415778 <p>20591 rows \u00d7 2 columns</p> <pre><code>model['Symptoms']\n</code></pre> date_occurrence Headache Fatigue Dizzy Cough Fever Muscle_Pain ID pk1 2021-01-24 Yes Yes No Yes No Yes pk1 2019-05-30 Yes Yes No No Yes No pk1 2021-05-16 Yes No No No Yes No pk1 2022-06-11 Yes Yes Yes Yes No No pk1 2020-06-18 Yes Yes Yes Yes Yes Yes ... ... ... ... ... ... ... ... pk9992 2021-11-22 No No No No No Yes pk9992 2019-11-07 Yes Yes No Yes Yes Yes pk9992 2018-09-01 Yes Yes No No Yes Yes pk9993 2018-12-02 No No No No Yes No pk9996 2018-08-11 Yes Yes No Yes No Yes <p>56673 rows \u00d7 7 columns</p> <pre><code>model['Blood_Test']\n</code></pre> date_taken location quantity ID pk31048 2019-05-24 13:00:52.371957 Right Arm 0.858088 pk42771 2020-12-04 13:00:52.371957 Right Arm 0.362495 pk9294 2019-08-18 13:00:52.371957 Left Arm 0.674107 pk34653 2020-06-11 13:00:52.371957 Small Intestine 0.994321 pk34474 2020-06-13 13:00:52.371957 Left Arm 0.520345 ... ... ... ... pk2856 2021-03-03 13:00:52.371957 Left Arm 0.358316 pk46915 2020-01-19 13:00:52.371957 Left Arm 0.387136 pk32960 2019-10-12 13:00:52.371957 Left Arm 0.563794 pk30995 2018-05-29 13:00:52.371957 Abdominal Wall 0.665672 pk21561 2019-10-16 13:00:52.371957 Left Arm 2.318754 <p>30000 rows \u00d7 3 columns</p> <pre><code>model['Hospital_Visit']\n</code></pre> admission_date reason ID pk1 2020-03-20 Cancer pk1000 2019-02-14 Pneumonia pk10001 2019-10-10 Appendix Operation pk10001 2021-01-13 Cancer pk10001 2018-11-30 COVID-19 ... ... ... pk9843 2018-10-04 COVID-19 pk9904 2018-05-15 Appendix Operation pk9913 2022-06-14 Pneumonia pk9915 2021-12-24 COVID-19 pk9926 2017-11-18 Cancer <p>8710 rows \u00d7 2 columns</p> <pre><code>df_gp = model['GP_Records']\ndf_nrisks = df_gp.groupby(df_gp.index)['comorbidity'].count() \ndf_nrisks.name = 'nrisks'\ndf_nrisks.value_counts().to_frame().sort_index()\n</code></pre> nrisks 0 40 1 35767 2 6920 3 1927 4 313 5 33 <p>build a dataframe for analysis by getting the serology data + demographics data + the number of risks</p> <pre><code>df = model['Serology'].join(model['Demographics']).join(df_nrisks).fillna(0)\ndf\n</code></pre> Date IgG Age Sex nrisks ID pk10001 2021-03-17 13.701676 0.0 0 2.0 pk10001 2021-07-29 1.077413 0.0 0 2.0 pk10005 2020-06-06 39.366639 0.0 0 3.0 pk10005 2020-03-02 56.358177 0.0 0 3.0 pk10009 2021-05-03 55.585361 0.0 0 1.0 ... ... ... ... ... ... pk997 2020-12-14 1.720815 0.0 0 1.0 pk9978 2020-11-10 52.239568 0.0 0 1.0 pk9982 2023-05-12 5.753619 0.0 0 1.0 pk9989 2022-12-16 37.616017 0.0 0 2.0 pk9989 2022-12-14 7.415778 0.0 0 2.0 <p>20591 rows \u00d7 5 columns</p> <p>Produce some plots show how there is a difference in the IgG response for different age and risk groups</p> <pre><code>import matplotlib.pyplot as plt\nfig,axs = plt.subplots(2)\nax = axs[0]\n\ndf['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),histtype='step',lw=2,density=True,label='all')\ndf[df['Age']&amp;gt;50]['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),density=True,histtype='step',lw=2,label='Age &amp;gt; 50')\ndf[df['Age']&amp;lt;50]['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),density=True,histtype='step',lw=2,label='Age &amp;lt; 50')\nax.set_yscale('log')\nax.set_xlabel('IgG measurement')\nax.legend()\n\nax = axs[1]\ndf[df['nrisks']&amp;lt;1]['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),density=True,histtype='step',lw=2,label='nrisk=0')\ndf[df['nrisks']==1]['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),density=True,histtype='step',lw=2,label='nrisk=1')\ndf[df['nrisks']&amp;gt;1]['IgG'].plot.hist(ax=ax,bins=10,range=(0,150),density=True,histtype='step',lw=2,label='nrisk&amp;gt;1')\n\nax.set_yscale('log')\nax.set_xlabel('IgG measurement')\nax.legend()\nplt.show();\n</code></pre>"},{"location":"CaRROT-CDM/notebooks/Generating%20Synthetic%20Data%20By%20Hand/#define-a-model","title":"Define a Model","text":"<p>We can use carrot.cdm to create destination tables and fields for our synthetic data model</p>"},{"location":"CaRROT-CDM/notebooks/Generating%20Synthetic%20Data%20By%20Hand/#create-a-model","title":"Create a Model","text":"<p>Create and run the model, for a dataset so large and complex, this can take some time</p>"},{"location":"CaRROT-CDM/notebooks/Generating%20Synthetic%20Data%20By%20Hand/#viewing-the-model","title":"Viewing the Model","text":"<p>Print to see what output data tables the model contains</p>"},{"location":"CaRROT-CDM/notebooks/Generating%20Synthetic%20Data%20By%20Hand/#perform-analysis","title":"Perform analysis","text":"<p>Firstly get the the GP records, and count the number of comorbidities a patient has, to create risk groups</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/","title":"Introduction   Using the ETLTool","text":"<pre><code>!pip3 install carrot-cdm -q\n</code></pre> <pre><code>!carrot --version\n</code></pre> <pre>\n<code>0.6.2\n</code>\n</pre> <pre><code>import carrot.tools\nimport json\nimport os\n\ncarrot.data_folder = os.path.join(os.path.dirname(carrot.__file__),'data')\n\nrules = carrot.tools.load_json(f'{carrot.data_folder}/test/rules/rules_14June2021.json')\nprint(json.dumps(rules, indent=6))\n</code></pre> <pre>\n<code>{\n      \"metadata\": {\n            \"date_created\": \"2021-06-14T15:27:37.123947\",\n            \"dataset\": \"Test\"\n      },\n      \"cdm\": {\n            \"observation\": {\n                  \"observation_0\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Asian\": 35825508\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Asian\": 35825508\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_1\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Bangladeshi\": 35825531\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Bangladeshi\": 35825531\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_2\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Indian\": 35826241\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Indian\": 35826241\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_3\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White\": 35827394\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White\": 35827394\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_4\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Black\": 35825567\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"Black\": 35825567\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"observation_5\": {\n                        \"observation_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White and Asian\": 35827395\n                              }\n                        },\n                        \"observation_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"observation_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\",\n                              \"term_mapping\": {\n                                    \"White and Asian\": 35827395\n                              }\n                        },\n                        \"observation_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"ethnicity\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"condition_occurrence\": {\n                  \"condition_occurrence_0\": {\n                        \"condition_concept_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\",\n                              \"term_mapping\": {\n                                    \"Y\": 254761\n                              }\n                        },\n                        \"condition_end_datetime\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"visit_date\"\n                        },\n                        \"condition_source_concept_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\",\n                              \"term_mapping\": {\n                                    \"Y\": 254761\n                              }\n                        },\n                        \"condition_source_value\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"symptom1\"\n                        },\n                        \"condition_start_datetime\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"visit_date\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Symptoms.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"person\": {\n                  \"female\": {\n                        \"birth_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"gender_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"F\": 8532\n                              }\n                        },\n                        \"gender_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"F\": 8532\n                              }\n                        },\n                        \"gender_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  },\n                  \"male\": {\n                        \"birth_datetime\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"date_of_birth\"\n                        },\n                        \"gender_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"M\": 8507\n                              }\n                        },\n                        \"gender_source_concept_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\",\n                              \"term_mapping\": {\n                                    \"M\": 8507\n                              }\n                        },\n                        \"gender_source_value\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"sex\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"Demographics.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            },\n            \"measurement\": {\n                  \"covid_antibody\": {\n                        \"value_as_number\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\"\n                        },\n                        \"measurement_source_value\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\"\n                        },\n                        \"measurement_concept_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\",\n                              \"term_mapping\": 37398191\n                        },\n                        \"measurement_source_concept_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"IgG\",\n                              \"term_mapping\": 37398191\n                        },\n                        \"measurement_datetime\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"date\"\n                        },\n                        \"person_id\": {\n                              \"source_table\": \"covid19_antibody.csv\",\n                              \"source_field\": \"PersonID\"\n                        }\n                  }\n            }\n      }\n}\n</code>\n</pre> <pre><code>f_map = carrot.tools.get_file_map_from_dir(f'{carrot.data_folder}/test/inputs/')\nprint (json.dumps(f_map,indent=6))\n</code></pre> <pre>\n<code>{\n      \"Symptoms.csv\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/data/test/inputs/Symptoms.csv\",\n      \"Covid19_test.csv\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/data/test/inputs/Covid19_test.csv\",\n      \"covid19_antibody.csv\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/data/test/inputs/covid19_antibody.csv\",\n      \"vaccine.csv\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/data/test/inputs/vaccine.csv\",\n      \"Demographics.csv\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/data/test/inputs/Demographics.csv\"\n}\n</code>\n</pre> <p>use the <code>f_map</code> to load all the inputs into a map between the file name and a dataframe object. This can be created manually via any prefered method.</p> <pre><code>inputs = carrot.tools.load_csv(f_map)\ninputs\n</code></pre> <pre>\n<code>2022-06-17 14:46:49 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x10df1b040&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Covid19_test.csv [&lt;carrot.io.common.DataBrick object at 0x10df1b0d0&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  covid19_antibody.csv [&lt;carrot.io.common.DataBrick object at 0x10df1b310&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  vaccine.csv [&lt;carrot.io.common.DataBrick object at 0x110d473d0&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x10deb5dc0&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x10deb5d30&gt;</code>\n</pre> <pre><code>inputs.keys()\n</code></pre> <pre>\n<code>dict_keys(['Symptoms.csv', 'Covid19_test.csv', 'covid19_antibody.csv', 'vaccine.csv', 'Demographics.csv'])</code>\n</pre> <pre><code>inputs['Symptoms.csv']\n</code></pre> <pre>\n<code>2022-06-17 14:46:49 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n</code>\n</pre> PersonID visit_date symptom1 symptom2 symptom3 0 16dc368a89b428b2485484313ba67a3912ca03f2b2b424... 2020-11-15 00:00:00.000000 Y Y Y 1 37834f2f25762f23e1f74a531cbe445db73d6765ebe608... 2020-01-04 00:00:00.000000 Y Y Y 2 454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51... 2020-03-27 00:00:00.000000 Y Y Y 3 5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f... 2020-06-24 00:00:00.000000 N N N 4 1253e9373e781b7500266caa55150e08e210bc8cd8cc70... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 795 62f6d46c48c7d9ff3d09a408d0ec880f167a5dc9c8fd34... 2020-11-04 00:00:00.000000 N Y N 796 c62510afc57db491f9f993387b76dd9a7d08f09c013269... 2020-07-27 00:00:00.000000 Y Y Y 797 bdc5d8a48c23897906b09a9a3680bd2e9c8b3121edbda3... 2020-03-27 00:00:00.000000 Y Y Y 798 fa88d374b9cf5e059fad4a2fe406feae4c49cbf4803083... 2020-12-24 00:00:00.000000 N N N 799 6a97982dccf77dd3dafa27fcbdf75c017301f730ba186b... 2020-11-15 00:00:00.000000 Y Y Y <p>800 rows \u00d7 5 columns</p> <pre><code>inputs_chunked = carrot.tools.load_csv(f_map,chunksize=100)\ninputs_chunked['Symptoms.csv']\n</code></pre> <pre>\n<code>2022-06-17 14:46:49 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Using a chunksize of '100' nrows\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x10deb5850&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Covid19_test.csv [&lt;carrot.io.common.DataBrick object at 0x111d3c490&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  covid19_antibody.csv [&lt;carrot.io.common.DataBrick object at 0x111d3c580&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  vaccine.csv [&lt;carrot.io.common.DataBrick object at 0x111d3c970&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x111d3cc40&gt;]\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n</code>\n</pre> PersonID visit_date symptom1 symptom2 symptom3 0 16dc368a89b428b2485484313ba67a3912ca03f2b2b424... 2020-11-15 00:00:00.000000 Y Y Y 1 37834f2f25762f23e1f74a531cbe445db73d6765ebe608... 2020-01-04 00:00:00.000000 Y Y Y 2 454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51... 2020-03-27 00:00:00.000000 Y Y Y 3 5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f... 2020-06-24 00:00:00.000000 N N N 4 1253e9373e781b7500266caa55150e08e210bc8cd8cc70... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 95 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-11-04 00:00:00.000000 N Y N 96 8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f6... 2020-07-27 00:00:00.000000 Y Y Y 97 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-01-04 00:00:00.000000 Y Y Y 98 5a39cadd1b007093db50744797c7a04a34f73b35ed4447... 2020-12-24 00:00:00.000000 N N N 99 27badc983df1780b60c2b3fa9d3a19a00e46aac798451f... 2020-11-04 00:00:00.000000 N Y N <p>100 rows \u00d7 5 columns</p> <p>The internal working of the <code>InputData</code> object will more to the next slice of data when running the process, untill all data has been processed.</p> <pre><code>inputs_chunked.next()\n</code></pre> <pre>\n<code>2022-06-17 14:46:49 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:46:49 - LocalDataCollection - INFO - Getting the next chunk of size '100' for 'Symptoms.csv'\n2022-06-17 14:46:49 - LocalDataCollection - INFO - --&gt; Got 100 rows\n</code>\n</pre> <pre><code>inputs_chunked['Symptoms.csv']\n</code></pre> PersonID visit_date symptom1 symptom2 symptom3 100 43974ed74066b207c30ffd0fed5146762e6c60745ac977... 2020-03-27 00:00:00.000000 Y Y Y 101 fc56dbc6d4652b315b86b71c8d688c1ccdea9c5f1fd077... 2020-02-04 00:00:00.000000 N N N 102 f8809aff4d69bece79dabe35be0c708b890d7eafb841f1... 2020-06-24 00:00:00.000000 N N N 103 5cf4e26bd3d87da5e03f80a43a64f1220a1f4ba9e1d634... 2020-11-15 00:00:00.000000 Y Y Y 104 f8809aff4d69bece79dabe35be0c708b890d7eafb841f1... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 195 a0f8b2c4cb1ac82abdb37f0fe5203b97be556c4468c83b... 2020-12-24 00:00:00.000000 N N N 196 4c15f47afe7f817fd559e12ddbc276f4930c5822f20490... 2020-01-04 00:00:00.000000 Y Y Y 197 983bd614bb5afece5ab3b6023f71147cd7b6bc2314f9d2... 2020-11-04 00:00:00.000000 N Y N 198 c3ea99f86b2f8a74ef4145bb245155ff5f91cd856f2875... 2020-06-24 00:00:00.000000 N N N 199 f32828acecb4282c87eaa554d2e1db74e418cd68458430... 2020-03-27 00:00:00.000000 Y Y Y <p>100 rows \u00d7 5 columns</p> <pre><code>inputs_sql = carrot.io.DataCollection()\ninputs_sql\n</code></pre> <pre>\n<code>2022-06-17 14:46:49 - DataCollection - INFO - DataCollection Object Created\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.common.DataCollection at 0x111c49130&gt;</code>\n</pre> <p>Load your data, for example fron a PostgresSQL server, using <code>sqlalchemy</code> and pandas:</p> <pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://localhost:5432/coconnect_data_test')\ndf = pd.read_sql(\"my_data_table\",engine)\ndf\n</code></pre> PersonID visit_date symptom1 symptom2 symptom3 0 16dc368a89b428b2485484313ba67a3912ca03f2b2b424... 2020-11-15 00:00:00.000000 Y Y Y 1 37834f2f25762f23e1f74a531cbe445db73d6765ebe608... 2020-01-04 00:00:00.000000 Y Y Y 2 454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51... 2020-03-27 00:00:00.000000 Y Y Y 3 5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f... 2020-06-24 00:00:00.000000 N N N 4 1253e9373e781b7500266caa55150e08e210bc8cd8cc70... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 95 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-11-04 00:00:00.000000 N Y N 96 8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f6... 2020-07-27 00:00:00.000000 Y Y Y 97 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-01-04 00:00:00.000000 Y Y Y 98 5a39cadd1b007093db50744797c7a04a34f73b35ed4447... 2020-12-24 00:00:00.000000 N N N 99 27badc983df1780b60c2b3fa9d3a19a00e46aac798451f... 2020-11-04 00:00:00.000000 N Y N <p>100 rows \u00d7 5 columns</p> <p>Set the input object to this dataframe</p> <p>note: the name of the input table must be the same as the name in the <code>json</code> rules. In this example, the name in the <code>json</code> for the mapping for this table is <code>Symptoms.csv</code>, therefore the dataframe is associated with that name.</p> <pre><code>inputs_sql['Symptoms.csv'] = carrot.io.DataBrick(df)\ninputs_sql['Symptoms.csv']\n</code></pre> <pre>\n<code>2022-06-17 14:46:50 - DataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x111d44940&gt;]\n2022-06-17 14:46:50 - DataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n</code>\n</pre> PersonID visit_date symptom1 symptom2 symptom3 0 16dc368a89b428b2485484313ba67a3912ca03f2b2b424... 2020-11-15 00:00:00.000000 Y Y Y 1 37834f2f25762f23e1f74a531cbe445db73d6765ebe608... 2020-01-04 00:00:00.000000 Y Y Y 2 454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51... 2020-03-27 00:00:00.000000 Y Y Y 3 5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f... 2020-06-24 00:00:00.000000 N N N 4 1253e9373e781b7500266caa55150e08e210bc8cd8cc70... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 95 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-11-04 00:00:00.000000 N Y N 96 8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f6... 2020-07-27 00:00:00.000000 Y Y Y 97 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-01-04 00:00:00.000000 Y Y Y 98 5a39cadd1b007093db50744797c7a04a34f73b35ed4447... 2020-12-24 00:00:00.000000 N N N 99 27badc983df1780b60c2b3fa9d3a19a00e46aac798451f... 2020-11-04 00:00:00.000000 N Y N <p>100 rows \u00d7 5 columns</p> <pre><code>from pyspark.sql import SparkSession\n</code></pre> <pre><code>spark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.jars\", \"/Users/calummacdonald/Downloads/postgresql-42.3.1.jar\") \\\n    .getOrCreate()\n</code></pre> <pre><code>df = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5432/coconnect_data_test\") \\\n    .option(\"dbtable\", \"my_data_table\") \\\n    .option(\"driver\", \"org.postgresql.Driver\") \\\n    .load()\n\ndf.printSchema()\n</code></pre> <pre>\n<code>root\n |-- PersonID: string (nullable = true)\n |-- visit_date: string (nullable = true)\n |-- symptom1: string (nullable = true)\n |-- symptom2: string (nullable = true)\n |-- symptom3: string (nullable = true)\n\n</code>\n</pre> <pre><code>inputs_databricks = carrot.io.DataCollection()\ninputs_databricks['Symptons.csv'] = carrot.io.DataBrick(df.select(\"*\").toPandas())\ninputs_databricks['Symptons.csv']\n</code></pre> <pre>\n<code>2022-06-17 14:47:04 - DataCollection - INFO - DataCollection Object Created\n2022-06-17 14:47:07 - DataCollection - INFO - Registering  Symptons.csv [&lt;carrot.io.common.DataBrick object at 0x111c490d0&gt;]\n2022-06-17 14:47:07 - DataCollection - INFO - Retrieving initial dataframe for 'Symptons.csv' for the first time\n</code>\n</pre> PersonID visit_date symptom1 symptom2 symptom3 0 16dc368a89b428b2485484313ba67a3912ca03f2b2b424... 2020-11-15 00:00:00.000000 Y Y Y 1 37834f2f25762f23e1f74a531cbe445db73d6765ebe608... 2020-01-04 00:00:00.000000 Y Y Y 2 454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51... 2020-03-27 00:00:00.000000 Y Y Y 3 5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f... 2020-06-24 00:00:00.000000 N N N 4 1253e9373e781b7500266caa55150e08e210bc8cd8cc70... 2020-07-27 00:00:00.000000 Y Y Y ... ... ... ... ... ... 95 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-11-04 00:00:00.000000 N Y N 96 8bcbb4c131df56f7c79066016241cc4bdf4e58db55c4f6... 2020-07-27 00:00:00.000000 Y Y Y 97 a4e00d7e6aa82111575438c5e5d3e63269d4c475c718b2... 2020-01-04 00:00:00.000000 Y Y Y 98 5a39cadd1b007093db50744797c7a04a34f73b35ed4447... 2020-12-24 00:00:00.000000 N N N 99 27badc983df1780b60c2b3fa9d3a19a00e46aac798451f... 2020-11-04 00:00:00.000000 N Y N <p>100 rows \u00d7 5 columns</p> <pre><code>from carrot.cdm import CommonDataModel\n\noutputs = carrot.tools.create_csv_store(output_folder='output_dir/')\n\ncdm = CommonDataModel(name=rules['metadata']['dataset'],\n                      inputs=inputs,\n                      outputs=outputs)\ncdm\n</code></pre> <pre>\n<code>2022-06-17 14:47:07 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:47:07 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:47:07 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:47:07 - CommonDataModel - INFO - Turning on automatic cdm column filling\n</code>\n</pre> <pre>\n<code>&lt;carrot.cdm.model.CommonDataModel at 0x112313820&gt;</code>\n</pre> <pre><code>cdm.create_and_add_objects(rules)\n</code></pre> <pre>\n<code>2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_0 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_1 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_2 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_3 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_4 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added observation_5 of type observation\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added condition_occurrence_0 of type condition_occurrence\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added female of type person\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added male of type person\n2022-06-17 14:47:07 - CommonDataModel - INFO - Added covid_antibody of type measurement\n</code>\n</pre> <p>After the initialisation and creation of the CDM objects, we can see what objects we have been registered in the model..</p> <pre><code>cdm.objects()\n</code></pre> <pre>\n<code>{'observation': {'observation_0': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x112313460&gt;,\n  'observation_1': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x112305f70&gt;,\n  'observation_2': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x11236d790&gt;,\n  'observation_3': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x112313340&gt;,\n  'observation_4': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x112399fa0&gt;,\n  'observation_5': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x112305d60&gt;},\n 'condition_occurrence': {'condition_occurrence_0': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x11239ce50&gt;},\n 'person': {'female': &lt;carrot.cdm.objects.versions.v5_3_1.person.Person at 0x1123135e0&gt;,\n  'male': &lt;carrot.cdm.objects.versions.v5_3_1.person.Person at 0x11239deb0&gt;},\n 'measurement': {'covid_antibody': &lt;carrot.cdm.objects.versions.v5_3_1.measurement.Measurement at 0x11239d970&gt;}}</code>\n</pre> <pre><code>cdm.process()\n</code></pre> <pre>\n<code>2022-06-17 14:47:07 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'measurement']\n2022-06-17 14:47:07 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"observation\": 6,\n      \"condition_occurrence\": 1,\n      \"person\": 2,\n      \"measurement\": 1\n}\n2022-06-17 14:47:07 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:47:07 - CommonDataModel - INFO - working on person\n2022-06-17 14:47:07 - CommonDataModel - INFO - starting on female\n2022-06-17 14:47:07 - Person - INFO - Called apply_rules\n2022-06-17 14:47:07 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:47:07 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:47:07 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:47:07 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:47:07 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:47:08 - Person - INFO - Mapped person_id\n2022-06-17 14:47:08 - Person - WARNING - Requiring non-null values in gender_concept_id removed 400 rows, leaving 600 rows.\n2022-06-17 14:47:08 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:47:08 - Person - INFO - created df (0x1123bb490)[female]\n2022-06-17 14:47:08 - CommonDataModel - INFO - finished female (0x1123bb490) ... 1/2 completed, 600 rows\n2022-06-17 14:47:08 - LocalDataCollection - INFO - saving person_ids to output_dir//person_ids.csv\n2022-06-17 14:47:08 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:47:08 - CommonDataModel - INFO - starting on male\n2022-06-17 14:47:08 - Person - INFO - Called apply_rules\n2022-06-17 14:47:08 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:47:08 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:47:08 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:47:08 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:47:08 - Person - INFO - Mapped person_id\n2022-06-17 14:47:08 - Person - WARNING - Requiring non-null values in gender_concept_id removed 600 rows, leaving 400 rows.\n2022-06-17 14:47:08 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:47:08 - Person - INFO - created df (0x1123f5760)[male]\n2022-06-17 14:47:08 - CommonDataModel - INFO - finished male (0x1123f5760) ... 2/2 completed, 400 rows\n2022-06-17 14:47:08 - LocalDataCollection - INFO - updating person_ids in output_dir//person_ids.csv\n2022-06-17 14:47:08 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:47:08 - CommonDataModel - INFO - saving dataframe (0x1123bb190) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x112313ee0&gt;\n2022-06-17 14:47:08 - LocalDataCollection - INFO - saving person to output_dir//person.csv\n2022-06-17 14:47:08 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:47:08 - CommonDataModel - INFO - finalised person on iteration 0 producing 1000 rows from 2 tables\n2022-06-17 14:47:08 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:47:08 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:47:08 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:47:08 - CommonDataModel - INFO - for observation: found 6 objects\n2022-06-17 14:47:08 - CommonDataModel - INFO - working on observation\n2022-06-17 14:47:08 - CommonDataModel - INFO - starting on observation_0\n2022-06-17 14:47:08 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:08 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:47:08 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:08 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:08 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:09 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:09 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 900 rows, leaving 100 rows.\n2022-06-17 14:47:09 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:09 - Observation - INFO - created df (0x1123bb5e0)[observation_0]\n2022-06-17 14:47:09 - CommonDataModel - INFO - finished observation_0 (0x1123bb5e0) ... 1/6 completed, 100 rows\n2022-06-17 14:47:09 - CommonDataModel - INFO - starting on observation_1\n2022-06-17 14:47:09 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:09 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:09 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 900 rows, leaving 100 rows.\n2022-06-17 14:47:09 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:09 - Observation - INFO - created df (0x11241ce20)[observation_1]\n2022-06-17 14:47:09 - CommonDataModel - INFO - finished observation_1 (0x11241ce20) ... 2/6 completed, 100 rows\n2022-06-17 14:47:09 - CommonDataModel - INFO - starting on observation_2\n2022-06-17 14:47:09 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:09 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:09 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 900 rows, leaving 100 rows.\n2022-06-17 14:47:09 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:09 - Observation - INFO - created df (0x112427580)[observation_2]\n</code>\n</pre> <pre>\n<code>2022-06-17 14:47:09 - CommonDataModel - INFO - finished observation_2 (0x112427580) ... 3/6 completed, 100 rows\n2022-06-17 14:47:09 - CommonDataModel - INFO - starting on observation_3\n2022-06-17 14:47:09 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:09 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:09 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 700 rows, leaving 300 rows.\n2022-06-17 14:47:09 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:09 - Observation - INFO - created df (0x1124357f0)[observation_3]\n2022-06-17 14:47:09 - CommonDataModel - INFO - finished observation_3 (0x1124357f0) ... 4/6 completed, 300 rows\n2022-06-17 14:47:09 - CommonDataModel - INFO - starting on observation_4\n2022-06-17 14:47:09 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:09 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:09 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:09 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 800 rows, leaving 200 rows.\n2022-06-17 14:47:09 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:09 - Observation - INFO - created df (0x112440490)[observation_4]\n2022-06-17 14:47:09 - CommonDataModel - INFO - finished observation_4 (0x112440490) ... 5/6 completed, 200 rows\n2022-06-17 14:47:10 - CommonDataModel - INFO - starting on observation_5\n2022-06-17 14:47:10 - Observation - INFO - Called apply_rules\n2022-06-17 14:47:10 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:47:10 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:47:10 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:47:10 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:47:10 - Observation - INFO - Mapped person_id\n2022-06-17 14:47:10 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 900 rows, leaving 100 rows.\n2022-06-17 14:47:10 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:47:10 - Observation - INFO - created df (0x112456f70)[observation_5]\n2022-06-17 14:47:10 - CommonDataModel - INFO - finished observation_5 (0x112456f70) ... 6/6 completed, 100 rows\n2022-06-17 14:47:10 - CommonDataModel - INFO - saving dataframe (0x112427280) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x112313ee0&gt;\n2022-06-17 14:47:10 - LocalDataCollection - INFO - saving observation to output_dir//observation.csv\n2022-06-17 14:47:10 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:47:10 - CommonDataModel - INFO - finalised observation on iteration 0 producing 900 rows from 6 tables\n2022-06-17 14:47:10 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:47:10 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:47:10 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:47:10 - CommonDataModel - INFO - for condition_occurrence: found 1 object\n2022-06-17 14:47:10 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:47:10 - CommonDataModel - INFO - starting on condition_occurrence_0\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:47:10 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:47:10 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 400 rows, leaving 400 rows.\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:47:10 - ConditionOccurrence - INFO - created df (0x112456790)[condition_occurrence_0]\n2022-06-17 14:47:10 - CommonDataModel - INFO - finished condition_occurrence_0 (0x112456790) ... 1/1 completed, 400 rows\n</code>\n</pre> <pre><code>cdm.keys()\n</code></pre> <pre><code>cdm['person'].dropna(axis=1,how='all')\n</code></pre> <pre><code>cdm['observation'].dropna(axis=1,how='all')\n</code></pre> <pre><code>cdm['condition_occurrence'].dropna(axis=1,how='all')\n</code></pre>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#introduction","title":"Introduction","text":"<p>The ETL transform to CDM using the classes defined in <code>carrot.cdm</code> is documented here as python notebook, as an example of how the classes can be used. Developers can follow the following workbook example, changing the rules file and the input files.</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#installing","title":"Installing","text":"<p>The best way is to install the module via <code>pip</code>. </p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#loading-the-rules","title":"Loading the Rules","text":"<p>Given the full path to a <code>json</code> file containing the rules, the first step is to load this up into a <code>json</code> object/dict.</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#loading-the-input-data","title":"Loading the input data","text":"<p>The ETL Tool takes in as input pandas dataframes and provides a tool for loading CSV files</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#csv","title":"CSV","text":"<p>A convienience function is available to create a map between a file name and a file path for all files in a directory:</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#chunked-csv","title":"Chunked CSV","text":"<p>For large datasets, it's better to chunk the data as to not overload your computer memory, this can be achieved by supplying a <code>chunksize</code> argument:</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#sql","title":"SQL","text":"<p>Another alternative, if your input data is not in <code>csv</code> format is to load the data manually yourself from SQL / Spark / DataBricks views etc.</p> <p>Firstly, initialise an input data handler object\"</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#spark-databricks","title":"Spark Databricks","text":"<p>If you want to use something like Spark for integration with DataBricks, you can use <code>pyspark</code> to load the data:</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#creating-a-cdm","title":"Creating a CDM","text":"<p>As CO-CONNECT-Tools contains a pythonic version of the CDM, we can create an instance of the <code>CommonDataModel</code> class.</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#adding-cdm-objects-to-the-cdm","title":"Adding CDM Objects to the CDM","text":"<p>The next step is to loop over all the rules from the <code>json</code>, creating and adding a new CDM object (e.g. Person) to the CDM.</p> <p>Within the loop the CDM object define function is set a lambda function to the apply rules. This means that during the executing, in runtime, the tool (via the <code>CommonDataModel</code> class, will execute the define function and know how to apply the mapping rules.</p>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#process-the-cdm","title":"Process The CDM","text":"<p>Processing the CDM will execute all objects, pandas dataframes will be created for each object, based on the rules that have been provided.</p> <p>Importantly the CDM will also format, finalise and merge all the individual dataframes for each objects. </p> <ul> <li>Formatting makes sure the columns are in the correct format i.e. a date is YYY-MM-DD</li> <li>Finalise makes sure </li> </ul>"},{"location":"CaRROT-CDM/notebooks/Introduction%20-%20Using%20the%20ETLTool/#inspect-outputs","title":"Inspect Outputs","text":""},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/","title":"Part 1   Introduction","text":"<pre><code>from carrot.cdm.objects import Person\nPerson\n</code></pre> <pre>\n<code>carrot.cdm.objects.versions.v5_3_1.person.Person</code>\n</pre> <pre><code>person = Person()\nperson.get_field_names()\n</code></pre> <pre>\n<code>['person_id',\n 'gender_concept_id',\n 'year_of_birth',\n 'month_of_birth',\n 'day_of_birth',\n 'birth_datetime',\n 'race_concept_id',\n 'ethnicity_concept_id',\n 'location_id',\n 'provider_id',\n 'care_site_id',\n 'person_source_value',\n 'gender_source_value',\n 'gender_source_concept_id',\n 'race_source_value',\n 'race_source_concept_id',\n 'ethnicity_source_value',\n 'ethnicity_source_concept_id']</code>\n</pre> <pre><code>person.get_field_dtypes()\n</code></pre> <pre>\n<code>{'person_id': 'Integer',\n 'gender_concept_id': 'Integer',\n 'year_of_birth': 'Integer',\n 'month_of_birth': 'Integer',\n 'day_of_birth': 'Integer',\n 'birth_datetime': 'Timestamp',\n 'race_concept_id': 'Integer',\n 'ethnicity_concept_id': 'Integer',\n 'location_id': 'Integer',\n 'provider_id': 'Integer',\n 'care_site_id': 'Integer',\n 'person_source_value': 'Text50',\n 'gender_source_value': 'Text50',\n 'gender_source_concept_id': 'Integer',\n 'race_source_value': 'Text50',\n 'race_source_concept_id': 'Integer',\n 'ethnicity_source_value': 'Text50',\n 'ethnicity_source_concept_id': 'Integer'}</code>\n</pre> <pre><code>import pandas as pd\nimport numpy as np\ndef build_person(self):\n    n = 10\n    self.person_id.series = pd.Series((i for i in range (n)))\n    self.gender_concept_id.series = pd.Series(np.random.choice([8507,8532],size=n))\n    self.birth_datetime.series = pd.Series(np.random.choice(['1970-01-01','1990-01-01'],size=n))\n\nperson.define = build_person\n</code></pre> <pre><code>person.get_df(force_rebuild=True)\n</code></pre> <pre>\n<code>2022-06-17 14:47:43 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:47:43 - Person - INFO - created df (0x10e2485e0)[0x10e19ee50]\n</code>\n</pre> person_id gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime race_concept_id ethnicity_concept_id location_id provider_id care_site_id person_source_value gender_source_value gender_source_concept_id race_source_value race_source_concept_id ethnicity_source_value ethnicity_source_concept_id 0 0 8532 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 1 8507 1990 1 1 1990-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 2 8507 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 3 8507 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 4 8532 1990 1 1 1990-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 5 8532 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 6 6 8507 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 7 7 8507 1990 1 1 1990-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 8 8532 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 9 9 8507 1970 1 1 1970-01-01 00:00:00.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN <pre><code>from carrot.io import LocalDataCollection, SqlDataCollection, BCLinkDataCollection\nLocalDataCollection, SqlDataCollection, BCLinkDataCollection\n</code></pre> <pre>\n<code>(carrot.io.plugins.local.LocalDataCollection,\n carrot.io.plugins.sql.SqlDataCollection,\n carrot.io.plugins.bclink.BCLinkDataCollection)</code>\n</pre> <p>A <code>LocalDataCollection</code> can be used to load local csv files</p> <pre><code>local = LocalDataCollection({'Demographics.csv':'../data/part1/Demographics.csv'},nrows=10,chunksize=5)\nlocal\n</code></pre> <pre>\n<code>2022-06-17 14:47:43 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:47:43 - LocalDataCollection - INFO - Using a chunksize of '5' nrows\n2022-06-17 14:47:43 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x10a500370&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x10a500b80&gt;</code>\n</pre> <pre><code>local['Demographics.csv']\n</code></pre> <pre>\n<code>2022-06-17 14:47:43 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n</code>\n</pre> ID Age Sex 0 pk1 57.0 Male 1 pk2 68.0 Female 2 pk3 78.0 Female 3 pk4 51.0 Female 4 pk5 51.0 Male <pre><code>local.next()\n</code></pre> <pre>\n<code>2022-06-17 14:47:43 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:47:44 - LocalDataCollection - INFO - Getting the next chunk of size '5' for 'Demographics.csv'\n2022-06-17 14:47:44 - LocalDataCollection - INFO - --&gt; Got 5 rows\n</code>\n</pre> <pre><code>local['Demographics.csv']\n</code></pre> ID Age Sex 5 pk6 64.0 Male 6 pk7 76.0 Female 7 pk8 60.0 Male 8 pk9 92.0 Female 9 pk10 58.0 Male <pre><code>local.reset()\n</code></pre> <pre>\n<code>2022-06-17 14:47:44 - LocalDataCollection - INFO - resetting used bricks\n</code>\n</pre> <p>A <code>BCLinkDataCollection</code> is used to interact with BCLink (for either I/O)</p> <pre><code>bclink = BCLinkDataCollection({'dry_run':True,'tables':{'person':'ds1000','observation':'ds10002'}},\n                              output_folder='cache')\n</code></pre> <pre>\n<code>2022-06-17 14:47:44 - BCLinkDataCollection - INFO - setup bclink collection\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'ds1000' ) bclink\n2022-06-17 14:47:44 - BCLinkHelpers - INFO - ds1000 (person) already exists --&gt; all good\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'ds10002' ) bclink\n2022-06-17 14:47:44 - BCLinkHelpers - INFO - ds10002 (observation) already exists --&gt; all good\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds1000 bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds10002 bclink\n2022-06-17 14:47:44 - BCLinkDataCollection - INFO - DataCollection Object Created\n</code>\n</pre> <pre><code>bclink.bclink_helpers.get_table_map()\n</code></pre> <pre>\n<code>{'person': 'ds1000', 'observation': 'ds10002'}</code>\n</pre> <pre><code>bclink.bclink_helpers.check_table_exists('person')\n</code></pre> <pre>\n<code>2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person' ) bclink\n</code>\n</pre> <pre>\n<code>True</code>\n</pre> <p>Example, create an indexing map by retrieving the last index of the table currently in BCLink:</p> <pre><code>bclink.load_indexing()\n</code></pre> <pre>\n<code>2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds1000 bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'ds1000' LIMIT 1  bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM ds1000 ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds10002 bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'ds10002' LIMIT 1  bclink\n2022-06-17 14:47:44 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM ds10002 ORDER BY -person_id LIMIT 1;  bclink\n</code>\n</pre> <pre>\n<code>{}</code>\n</pre> <pre><code>from IPython.display import SVG, display\nfrom carrot.tools import load_json,make_dag\n\nrules = load_json('../data/rules.json')\n\ndef show_svg():\n    return display(SVG(make_dag(rules['cdm'])))\nshow_svg()\n</code></pre> cluster_1 Source cluster_0 Common Data Model person_birth_datetime birth_datetime Demographics.csv_Age Age person_birth_datetime-&gt;Demographics.csv_Age Demographics.csv Demographics.csv Demographics.csv_Age-&gt;Demographics.csv person_gender_concept_id gender_concept_id Demographics.csv_Sex Sex person_gender_concept_id-&gt;Demographics.csv_Sex Demographics.csv_Sex-&gt;Demographics.csv person_gender_source_concept_id gender_source_concept_id person_gender_source_concept_id-&gt;Demographics.csv_Sex person_gender_source_value gender_source_value person_gender_source_value-&gt;Demographics.csv_Sex person_person_id person_id Demographics.csv_ID ID person_person_id-&gt;Demographics.csv_ID Demographics.csv_ID-&gt;Demographics.csv observation_observation_concept_id observation_concept_id Serology.csv_IgG IgG observation_observation_concept_id-&gt;Serology.csv_IgG Hospital_Visit.csv_reason reason observation_observation_concept_id-&gt;Hospital_Visit.csv_reason Serology.csv Serology.csv Serology.csv_IgG-&gt;Serology.csv observation_observation_datetime observation_datetime Serology.csv_Date Date observation_observation_datetime-&gt;Serology.csv_Date Hospital_Visit.csv_admission_date admission_date observation_observation_datetime-&gt;Hospital_Visit.csv_admission_date Serology.csv_Date-&gt;Serology.csv observation_observation_source_concept_id observation_source_concept_id observation_observation_source_concept_id-&gt;Serology.csv_IgG observation_observation_source_concept_id-&gt;Hospital_Visit.csv_reason observation_observation_source_value observation_source_value observation_observation_source_value-&gt;Serology.csv_IgG observation_observation_source_value-&gt;Hospital_Visit.csv_reason observation_person_id person_id Serology.csv_ID ID observation_person_id-&gt;Serology.csv_ID Hospital_Visit.csv_ID ID observation_person_id-&gt;Hospital_Visit.csv_ID Serology.csv_ID-&gt;Serology.csv Hospital_Visit.csv Hospital_Visit.csv Hospital_Visit.csv_reason-&gt;Hospital_Visit.csv Hospital_Visit.csv_admission_date-&gt;Hospital_Visit.csv Hospital_Visit.csv_ID-&gt;Hospital_Visit.csv condition_occurrence_condition_concept_id condition_concept_id condition_occurrence_condition_concept_id-&gt;Hospital_Visit.csv_reason Symptoms.csv_Headache Headache condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Headache Symptoms.csv_Fatigue Fatigue condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Fatigue Symptoms.csv_Dizzy Dizzy condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Dizzy Symptoms.csv_Cough Cough condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Cough Symptoms.csv_Fever Fever condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Fever Symptoms.csv_Muscle_Pain Muscle_Pain condition_occurrence_condition_concept_id-&gt;Symptoms.csv_Muscle_Pain GP_Records.csv_comorbidity comorbidity condition_occurrence_condition_concept_id-&gt;GP_Records.csv_comorbidity Symptoms.csv Symptoms.csv Symptoms.csv_Headache-&gt;Symptoms.csv condition_occurrence_condition_end_datetime condition_end_datetime condition_occurrence_condition_end_datetime-&gt;Hospital_Visit.csv_admission_date Symptoms.csv_date_occurrence date_occurrence condition_occurrence_condition_end_datetime-&gt;Symptoms.csv_date_occurrence GP_Records.csv_date_of_visit date_of_visit condition_occurrence_condition_end_datetime-&gt;GP_Records.csv_date_of_visit Symptoms.csv_date_occurrence-&gt;Symptoms.csv condition_occurrence_condition_source_concept_id condition_source_concept_id condition_occurrence_condition_source_concept_id-&gt;Hospital_Visit.csv_reason condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Headache condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Fatigue condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Dizzy condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Cough condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Fever condition_occurrence_condition_source_concept_id-&gt;Symptoms.csv_Muscle_Pain condition_occurrence_condition_source_concept_id-&gt;GP_Records.csv_comorbidity condition_occurrence_condition_source_value condition_source_value condition_occurrence_condition_source_value-&gt;Hospital_Visit.csv_reason condition_occurrence_condition_source_value-&gt;Symptoms.csv_Headache condition_occurrence_condition_source_value-&gt;Symptoms.csv_Fatigue condition_occurrence_condition_source_value-&gt;Symptoms.csv_Dizzy condition_occurrence_condition_source_value-&gt;Symptoms.csv_Cough condition_occurrence_condition_source_value-&gt;Symptoms.csv_Fever condition_occurrence_condition_source_value-&gt;Symptoms.csv_Muscle_Pain condition_occurrence_condition_source_value-&gt;GP_Records.csv_comorbidity condition_occurrence_condition_start_datetime condition_start_datetime condition_occurrence_condition_start_datetime-&gt;Hospital_Visit.csv_admission_date condition_occurrence_condition_start_datetime-&gt;Symptoms.csv_date_occurrence condition_occurrence_condition_start_datetime-&gt;GP_Records.csv_date_of_visit condition_occurrence_person_id person_id condition_occurrence_person_id-&gt;Hospital_Visit.csv_ID Symptoms.csv_ID ID condition_occurrence_person_id-&gt;Symptoms.csv_ID GP_Records.csv_ID ID condition_occurrence_person_id-&gt;GP_Records.csv_ID Symptoms.csv_ID-&gt;Symptoms.csv Symptoms.csv_Fatigue-&gt;Symptoms.csv Symptoms.csv_Dizzy-&gt;Symptoms.csv Symptoms.csv_Cough-&gt;Symptoms.csv Symptoms.csv_Fever-&gt;Symptoms.csv Symptoms.csv_Muscle_Pain-&gt;Symptoms.csv GP_Records.csv GP_Records.csv GP_Records.csv_comorbidity-&gt;GP_Records.csv GP_Records.csv_date_of_visit-&gt;GP_Records.csv GP_Records.csv_ID-&gt;GP_Records.csv drug_exposure_drug_concept_id drug_concept_id Vaccinations.csv_type type drug_exposure_drug_concept_id-&gt;Vaccinations.csv_type Vaccinations.csv Vaccinations.csv Vaccinations.csv_type-&gt;Vaccinations.csv drug_exposure_drug_exposure_end_datetime drug_exposure_end_datetime Vaccinations.csv_date_of_vaccination date_of_vaccination drug_exposure_drug_exposure_end_datetime-&gt;Vaccinations.csv_date_of_vaccination Vaccinations.csv_date_of_vaccination-&gt;Vaccinations.csv drug_exposure_drug_exposure_start_datetime drug_exposure_start_datetime drug_exposure_drug_exposure_start_datetime-&gt;Vaccinations.csv_date_of_vaccination drug_exposure_drug_source_concept_id drug_source_concept_id drug_exposure_drug_source_concept_id-&gt;Vaccinations.csv_type drug_exposure_drug_source_value drug_source_value drug_exposure_drug_source_value-&gt;Vaccinations.csv_type drug_exposure_person_id person_id Vaccinations.csv_ID ID drug_exposure_person_id-&gt;Vaccinations.csv_ID Vaccinations.csv_ID-&gt;Vaccinations.csv person person person-&gt;person_birth_datetime person-&gt;person_gender_concept_id person-&gt;person_gender_source_concept_id person-&gt;person_gender_source_value person-&gt;person_person_id observation observation observation-&gt;observation_observation_concept_id observation-&gt;observation_observation_datetime observation-&gt;observation_observation_source_concept_id observation-&gt;observation_observation_source_value observation-&gt;observation_person_id condition_occurrence condition_occurrence condition_occurrence-&gt;condition_occurrence_condition_concept_id condition_occurrence-&gt;condition_occurrence_condition_end_datetime condition_occurrence-&gt;condition_occurrence_condition_source_concept_id condition_occurrence-&gt;condition_occurrence_condition_source_value condition_occurrence-&gt;condition_occurrence_condition_start_datetime condition_occurrence-&gt;condition_occurrence_person_id drug_exposure drug_exposure drug_exposure-&gt;drug_exposure_drug_concept_id drug_exposure-&gt;drug_exposure_drug_exposure_end_datetime drug_exposure-&gt;drug_exposure_drug_exposure_start_datetime drug_exposure-&gt;drug_exposure_drug_source_concept_id drug_exposure-&gt;drug_exposure_drug_source_value drug_exposure-&gt;drug_exposure_person_id <pre><code>from carrot.tools import remove_missing_sources_from_rules\nfiltered_rules = remove_missing_sources_from_rules(rules,local.keys())\nfiltered_rules\n</code></pre> <pre>\n<code>2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Antibody 3027 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed H/O: heart failure 3043 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed 2019-nCoV 3044 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Cancer 3045 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed cdm table 'observation' from rules\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Headache 3028 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Fatigue 3029 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Dizziness 3030 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Cough 3031 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Fever 3032 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Muscle pain 3033 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Pneumonia 3042 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Mental health problem 3046 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Mental disorder 3047 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Type 2 diabetes mellitus 3048 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Ischemic heart disease 3049 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed Hypertensive disorder 3050 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed cdm table 'condition_occurrence' from rules\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3034 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3035 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3036 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 from rules because it was not loaded\n2022-06-17 14:47:45 - remove_missing_sources_from_rules - WARNING - removed cdm table 'drug_exposure' from rules\n</code>\n</pre> <pre>\n<code>{'metadata': {'date_created': '2022-02-12T12:22:48.465257',\n  'dataset': 'FAILED: ExampleV4'},\n 'cdm': {'person': {'MALE 3025': {'birth_datetime': {'source_table': 'Demographics.csv',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_value': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics.csv', 'source_field': 'ID'}},\n   'FEMALE 3026': {'birth_datetime': {'source_table': 'Demographics.csv',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_value': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics.csv', 'source_field': 'ID'}}}}}</code>\n</pre> <pre><code>from carrot.cdm import CommonDataModel\nCommonDataModel\n</code></pre> <pre>\n<code>carrot.cdm.model.CommonDataModel</code>\n</pre> <pre><code>cdm = CommonDataModel.from_rules(filtered_rules,inputs=local,outputs=bclink)\n</code></pre> <pre>\n<code>2022-06-17 14:47:45 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:47:45 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:47:45 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:47:45 - BCLinkHelpers - WARNING - No table for getting existing person ids (person_ids) has been defined\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds1000 bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'ds1000' LIMIT 1  bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM ds1000 ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM ds10002 bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'ds10002' LIMIT 1  bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM ds10002 ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:47:45 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:47:45 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n</code>\n</pre> <pre><code>cdm.process()\n</code></pre> <pre>\n<code>2022-06-17 14:47:45 - CommonDataModel - INFO - Starting processing in order: ['person']\n2022-06-17 14:47:45 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2\n}\n2022-06-17 14:47:45 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:47:45 - CommonDataModel - INFO - working on person\n2022-06-17 14:47:45 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:47:45 - Person - INFO - Called apply_rules\n2022-06-17 14:47:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:47:45 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:47:45 - Person - INFO - Mapped person_id\n2022-06-17 14:47:45 - Person - WARNING - Requiring non-null values in gender_concept_id removed 3 rows, leaving 2 rows.\n2022-06-17 14:47:45 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:47:45 - Person - INFO - created df (0x10e65aee0)[MALE_3025]\n2022-06-17 14:47:45 - CommonDataModel - INFO - finished MALE 3025 (0x10e65aee0) ... 1/2 completed, 2 rows\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - saving person_ids to cache/person_ids.csv\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:45 - BCLinkHelpers - ERROR - table person_ids unknown in dict_keys(['person', 'observation'])\n2022-06-17 14:47:45 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:47:45 - Person - INFO - Called apply_rules\n2022-06-17 14:47:45 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:47:45 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:47:45 - Person - INFO - Mapped person_id\n2022-06-17 14:47:45 - Person - WARNING - Requiring non-null values in gender_concept_id removed 2 rows, leaving 3 rows.\n2022-06-17 14:47:45 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:47:45 - Person - INFO - created df (0x10e65ef70)[FEMALE_3026]\n2022-06-17 14:47:45 - CommonDataModel - INFO - finished FEMALE 3026 (0x10e65ef70) ... 2/2 completed, 3 rows\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - updating person_ids in cache/person_ids.csv\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:45 - BCLinkHelpers - ERROR - table person_ids unknown in dict_keys(['person', 'observation'])\n2022-06-17 14:47:45 - CommonDataModel - INFO - saving dataframe (0x10e650d30) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x10e2995e0&gt;\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - saving person to cache/person.csv\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - dataset_tool --load --table=ds1000 --user=data --data_file=cache/person.csv --support --bcqueue bclink\n2022-06-17 14:47:45 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=ds1000 --user=data --database=bclink\n2022-06-17 14:47:45 - CommonDataModel - INFO - finalised person on iteration 0 producing 5 rows from 2 tables\n2022-06-17 14:47:45 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:47:45 - LocalDataCollection - INFO - Getting the next chunk of size '5' for 'Demographics.csv'\n2022-06-17 14:47:45 - LocalDataCollection - INFO - --&gt; Got 5 rows\n2022-06-17 14:47:45 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:47:45 - CommonDataModel - INFO - working on person\n2022-06-17 14:47:45 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:47:45 - CommonDataModel - INFO - finished MALE 3025 (0x10e65aee0) ... 1/2 completed, 2 rows\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - updating person_ids in cache/person_ids.csv\n2022-06-17 14:47:45 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:45 - BCLinkHelpers - ERROR - table person_ids unknown in dict_keys(['person', 'observation'])\n2022-06-17 14:47:45 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:47:46 - CommonDataModel - INFO - finished FEMALE 3026 (0x10e65ef70) ... 2/2 completed, 3 rows\n2022-06-17 14:47:46 - BCLinkDataCollection - INFO - updating person_ids in cache/person_ids.csv\n2022-06-17 14:47:46 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:46 - BCLinkHelpers - ERROR - table person_ids unknown in dict_keys(['person', 'observation'])\n2022-06-17 14:47:46 - CommonDataModel - INFO - saving dataframe (0x10e653100) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x10e2995e0&gt;\n2022-06-17 14:47:46 - BCLinkDataCollection - INFO - updating person in cache/person.csv\n2022-06-17 14:47:46 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:47:46 - BCLinkHelpers - NOTICE - dataset_tool --load --table=ds1000 --user=data --data_file=cache/person.csv --support --bcqueue bclink\n2022-06-17 14:47:46 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=ds1000 --user=data --database=bclink\n2022-06-17 14:47:46 - CommonDataModel - INFO - finalised person on iteration 1 producing 5 rows from 2 tables\n2022-06-17 14:47:46 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:47:46 - LocalDataCollection - INFO - Getting the next chunk of size '5' for 'Demographics.csv'\n2022-06-17 14:47:46 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:47:46 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 6 8507 1963 7 16 1963-07-16 00:00:00.000000 Male 8507 7 8507 1969 7 14 1969-07-14 00:00:00.000000 Male 8507 8 8532 1952 7 18 1952-07-18 00:00:00.000000 Female 8532 9 8532 1942 7 21 1942-07-21 00:00:00.000000 Female 8532 10 8532 1969 7 14 1969-07-14 00:00:00.000000 Female 8532"},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/#what-is-co-connect-tools","title":"What is CO-CONNECT Tools?","text":"<p><code>carrot.cdm</code> is a software package. The main component is the `carrot.module that contains python classes and tools for:</p> <ul> <li>Common Data Model</li> <li>Health Data Elements (person, condition_occurence...)</li> <li>I/O </li> <li>Command Line Interface</li> <li>Various Tools</li> </ul> <p></p>"},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/#what-are-the-health-data-elements","title":"What are the Health Data Elements ?","text":"<p>Classes for controlling and handling the building of elements such as the <code>person</code> table. </p>"},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/#what-do-you-mean-by-io","title":"What do you mean by I/O ?","text":"<p>Various helper classes for data collections that control the Input/Output</p>"},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/#what-are-the-tools","title":"What are the tools?","text":"<p>Lots of different features, mostly helper functions used throughout the code</p> <p>For example, loading a json rules file:</p>"},{"location":"CaRROT-CDM/notebooks/Part%201%20-%20Introduction/#what-is-the-commondatamodel","title":"What is the <code>CommonDataModel</code> ?","text":"<p>The python class that controls everything when building a common data model</p>"},{"location":"CaRROT-CDM/notebooks/Part%202%20-%20Command%20Line%20/","title":"Part 2   Command Line","text":"<p>There are many available commands, many are decrepit or used for testing </p> <pre><code>!carrot --help\n</code></pre> <pre>\n<code>Usage: carrot [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -v, --version\n  -l, --log-level [0|1|2|3]  change the level for log messaging. 0 - ERROR, 1\n                             - WARNING, 2 - INFO (default), 3 - DEBUG\n  -cp, --cprofile            use cProfile to profile the tool\n  --help                     Show this message and exit.\n\nCommands:\n  airflow       Command group for configuring runs with airflow\n  display       Commands for displaying various types of data and files.\n  etl           Command group for running the full ETL of a dataset\n  generate      Commands to generate helpful files.\n  get           Commands to get data from the CCOM api.\n  info          Commands to find information about the package.\n  pseudonymise  Command to help pseudonymise data.\n  run           Commands for mapping data to the OMOP CommonDataModel (CDM).\n  search        Commands for search/help with mapping\n</code>\n</pre> <pre><code>!carrot run map --rules ../data/rules.json --output-folder ./temp/1/  ../data/part1/*.csv\n</code></pre> <pre>\n<code>2022-06-17 14:48:43 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x10e414d90&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x10e414c10&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x10e414fd0&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x10e414b50&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x10e414c70&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x10e45e6d0&gt;]\n2022-06-17 14:48:43 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:43 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:48:43 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:48:43 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:48:43 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 14:48:43 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:48:43 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:48:43 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:48:43 - CommonDataModel - INFO - working on person\n2022-06-17 14:48:43 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:48:43 - Person - INFO - Called apply_rules\n2022-06-17 14:48:43 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\ncould not convert string to float: 'na'\n2022-06-17 14:48:43 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:43 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:43 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:43 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:43 - Person - INFO - Mapped person_id\n2022-06-17 14:48:43 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 14:48:43 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 14:48:43 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:44 - Person - INFO - created df (0x10e504f10)[MALE_3025]\n2022-06-17 14:48:44 - CommonDataModel - INFO - finished MALE 3025 (0x10e504f10) ... 1/2 completed, 561 rows\n2022-06-17 14:48:44 - LocalDataCollection - INFO - saving person_ids to ./temp/1//person_ids.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - saving dataframe (0x10e504f10) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:44 - LocalDataCollection - INFO - saving person to ./temp/1//person.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:48:44 - Person - INFO - Called apply_rules\ncould not convert string to float: 'na'\n2022-06-17 14:48:44 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:44 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:44 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:44 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:44 - Person - INFO - Mapped person_id\n2022-06-17 14:48:44 - Person - WARNING - Requiring non-null values in gender_concept_id removed 565 rows, leaving 435 rows.\n2022-06-17 14:48:44 - Person - INFO - Automatically formatting data columns.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:44 - Person - INFO - created df (0x10e596b20)[FEMALE_3026]\n2022-06-17 14:48:44 - CommonDataModel - INFO - finished FEMALE 3026 (0x10e596b20) ... 2/2 completed, 435 rows\n2022-06-17 14:48:44 - LocalDataCollection - INFO - updating person_ids in ./temp/1//person_ids.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - saving dataframe (0x10e596b20) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:44 - LocalDataCollection - INFO - updating person in ./temp/1//person.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - finalised person on iteration 0 producing 996 rows from 2 tables\n2022-06-17 14:48:44 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:44 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:44 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:48:44 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 14:48:44 - CommonDataModel - INFO - working on observation\n2022-06-17 14:48:44 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 14:48:44 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:44 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:44 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:44 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:44 - Observation - INFO - created df (0x10e596b20)[Antibody_3027]\n2022-06-17 14:48:44 - CommonDataModel - INFO - finished Antibody 3027 (0x10e596b20) ... 1/4 completed, 413 rows\n2022-06-17 14:48:44 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:44 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:44 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:44 - CommonDataModel - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 14:48:44 - CommonDataModel - INFO - saving dataframe (0x10e596b20) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:44 - LocalDataCollection - INFO - saving observation to ./temp/1//observation.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 14:48:44 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:44 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:44 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:44 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 937 rows, leaving 263 rows.\n2022-06-17 14:48:44 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:44 - Observation - INFO - created df (0x10e5a0910)[H_O_heart_failure_3043]\n2022-06-17 14:48:44 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x10e5a0910) ... 2/4 completed, 263 rows\n2022-06-17 14:48:44 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:44 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:44 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:44 - CommonDataModel - ERROR - 262/263 were good, 1 studies are removed.\n2022-06-17 14:48:44 - CommonDataModel - INFO - saving dataframe (0x10e5a0910) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:44 - LocalDataCollection - INFO - updating observation in ./temp/1//observation.tsv\n2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 14:48:44 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:44 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:44 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 1023 rows, leaving 177 rows.\n2022-06-17 14:48:44 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:44 - Observation - INFO - created df (0x10e5a0190)[2019_nCoV_3044]\n2022-06-17 14:48:44 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x10e5a0190) ... 3/4 completed, 177 rows\n2022-06-17 14:48:44 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:44 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:44 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:44 - CommonDataModel - ERROR - 176/177 were good, 1 studies are removed.\n2022-06-17 14:48:44 - CommonDataModel - INFO - saving dataframe (0x10e5a0190) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:44 - LocalDataCollection - INFO - updating observation in ./temp/1//observation.tsv\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:44 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:44 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 14:48:44 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:44 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:44 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:44 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 851 rows, leaving 349 rows.\n2022-06-17 14:48:44 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - Observation - INFO - created df (0x10e5cc8e0)[Cancer_3045]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Cancer 3045 (0x10e5cc8e0) ... 4/4 completed, 349 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5cc8e0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating observation in ./temp/1//observation.tsv\n2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - finalised observation on iteration 0 producing 1197 rows from 4 tables\n2022-06-17 14:48:45 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:45 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:45 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:48:45 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 14:48:45 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 55 rows, leaving 275 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 274 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5a7460)[Headache_3028]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Headache 3028 (0x10e5a7460) ... 1/12 completed, 274 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5a7460) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - saving condition_occurrence to ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 95 rows, leaving 235 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 234 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5d8d00)[Fatigue_3029]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Fatigue 3029 (0x10e5d8d00) ... 2/12 completed, 234 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5d8d00) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 195 rows, leaving 135 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 134 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5a0340)[Dizziness_3030]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Dizziness 3030 (0x10e5a0340) ... 3/12 completed, 134 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5a0340) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 100 rows, leaving 230 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 229 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5c29a0)[Cough_3031]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Cough 3031 (0x10e5c29a0) ... 4/12 completed, 229 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5c29a0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 265 rows, leaving 65 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5c23a0)[Fever_3032]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Fever 3032 (0x10e5c23a0) ... 5/12 completed, 65 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5c23a0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:45 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:45 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 295 rows, leaving 35 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 34 rows.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:45 - ConditionOccurrence - INFO - created df (0x10e5c4fd0)[Muscle_pain_3033]\n2022-06-17 14:48:45 - CommonDataModel - INFO - finished Muscle pain 3033 (0x10e5c4fd0) ... 6/12 completed, 34 rows\n2022-06-17 14:48:45 - CommonDataModel - INFO - saving dataframe (0x10e5c4fd0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:45 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:46 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:46 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1029 rows, leaving 171 rows.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - created df (0x10e5d1910)[Pneumonia_3042]\n2022-06-17 14:48:46 - CommonDataModel - INFO - finished Pneumonia 3042 (0x10e5d1910) ... 7/12 completed, 171 rows\n2022-06-17 14:48:46 - CommonDataModel - INFO - saving dataframe (0x10e5d1910) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:46 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:46 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:46 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - created df (0x10e5e9d60)[Mental_health_problem_3046]\n2022-06-17 14:48:46 - CommonDataModel - INFO - finished Mental health problem 3046 (0x10e5e9d60) ... 8/12 completed, 444 rows\n2022-06-17 14:48:46 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:46 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:46 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:46 - CommonDataModel - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 14:48:46 - CommonDataModel - INFO - saving dataframe (0x10e5e9d60) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:46 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:46 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:46 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - created df (0x10e5c20d0)[Mental_disorder_3047]\n2022-06-17 14:48:46 - CommonDataModel - INFO - finished Mental disorder 3047 (0x10e5c20d0) ... 9/12 completed, 444 rows\n2022-06-17 14:48:46 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:46 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:46 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:46 - CommonDataModel - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 14:48:46 - CommonDataModel - INFO - saving dataframe (0x10e5c20d0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:46 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:46 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:46 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1688 rows, leaving 264 rows.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - created df (0x10e61df10)[Type_2_diabetes_mellitus_3048]\n2022-06-17 14:48:46 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x10e61df10) ... 10/12 completed, 264 rows\n2022-06-17 14:48:46 - CommonDataModel - INFO - saving dataframe (0x10e61df10) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:46 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:46 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:46 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 14:48:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:47 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1738 rows, leaving 214 rows.\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - created df (0x10e5d8580)[Ischemic_heart_disease_3049]\n2022-06-17 14:48:47 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x10e5d8580) ... 11/12 completed, 214 rows\n2022-06-17 14:48:47 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:47 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:47 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:47 - CommonDataModel - ERROR - 213/214 were good, 1 studies are removed.\n2022-06-17 14:48:47 - CommonDataModel - INFO - saving dataframe (0x10e5d8580) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:47 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:47 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:47 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:47 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1822 rows, leaving 130 rows.\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:47 - ConditionOccurrence - INFO - created df (0x10e5f7310)[Hypertensive_disorder_3050]\n2022-06-17 14:48:47 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x10e5f7310) ... 12/12 completed, 130 rows\n2022-06-17 14:48:47 - CommonDataModel - INFO - saving dataframe (0x10e5f7310) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:47 - LocalDataCollection - INFO - updating condition_occurrence in ./temp/1//condition_occurrence.tsv\n2022-06-17 14:48:47 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:47 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 2630 rows from 12 tables\n2022-06-17 14:48:47 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:47 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:47 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:48:47 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 14:48:47 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 14:48:47 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 14:48:47 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:48:47 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:48:47 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:48:47 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:48:47 - DrugExposure - INFO - created df (0x10e5fe520)[COVID_19_vaccine_3034]\n2022-06-17 14:48:47 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x10e5fe520) ... 1/5 completed, 245 rows\n2022-06-17 14:48:47 - CommonDataModel - INFO - saving dataframe (0x10e5fe520) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:47 - LocalDataCollection - INFO - saving drug_exposure to ./temp/1//drug_exposure.tsv\n2022-06-17 14:48:47 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:47 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 14:48:47 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:48:47 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 494 rows, leaving 226 rows.\n2022-06-17 14:48:47 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 1 rows, leaving 225 rows.\n2022-06-17 14:48:47 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:48:47 - DrugExposure - INFO - created df (0x10e5a09d0)[COVID_19_vaccine_3035]\n2022-06-17 14:48:47 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x10e5a09d0) ... 2/5 completed, 225 rows\n2022-06-17 14:48:47 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:47 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:47 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:47 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:48:47 - CommonDataModel - INFO - saving dataframe (0x10e5a09d0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:47 - LocalDataCollection - INFO - updating drug_exposure in ./temp/1//drug_exposure.tsv\n2022-06-17 14:48:47 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:47 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 14:48:47 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:48:47 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:48:47 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:48:47 - DrugExposure - INFO - Automatically formatting data columns.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:47 - DrugExposure - INFO - created df (0x10e5fbd90)[COVID_19_vaccine_3036]\n2022-06-17 14:48:48 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x10e5fbd90) ... 3/5 completed, 249 rows\n2022-06-17 14:48:48 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:48 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:48 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:48 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:48:48 - CommonDataModel - INFO - saving dataframe (0x10e5fbd90) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:48 - LocalDataCollection - INFO - updating drug_exposure in ./temp/1//drug_exposure.tsv\n2022-06-17 14:48:48 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:48 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 14:48:48 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:48:48 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:48:48 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:48:48 - DrugExposure - INFO - created df (0x10e5f7ac0)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 14:48:48 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x10e5f7ac0) ... 4/5 completed, 245 rows\n2022-06-17 14:48:48 - CommonDataModel - INFO - saving dataframe (0x10e5f7ac0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:48 - LocalDataCollection - INFO - updating drug_exposure in ./temp/1//drug_exposure.tsv\n2022-06-17 14:48:48 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:48 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 14:48:48 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:48:48 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:48:48 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:48:48 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:48:48 - DrugExposure - INFO - created df (0x10e623340)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 14:48:48 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x10e623340) ... 5/5 completed, 249 rows\n2022-06-17 14:48:48 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:48 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:48 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:48 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:48:48 - CommonDataModel - INFO - saving dataframe (0x10e623340) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10e414970&gt;\n2022-06-17 14:48:48 - LocalDataCollection - INFO - updating drug_exposure in ./temp/1//drug_exposure.tsv\n2022-06-17 14:48:49 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:49 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 1210 rows from 5 tables\n2022-06-17 14:48:49 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:49 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:49 - CommonDataModel - INFO - {\n      \"version\": \"0.0.0\",\n      \"created_by\": \"calummacdonald\",\n      \"created_at\": \"2022-06-17T134843\",\n      \"dataset\": \"CommonDataModel::FAILED: ExampleV4\",\n      \"total_data_processed\": {\n            \"person\": 996,\n            \"observation\": 1197,\n            \"condition_occurrence\": 2630,\n            \"drug_exposure\": 1210\n      }\n}\n</code>\n</pre> <pre><code>!ls ./temp/1/\n</code></pre> <pre>\n<code>condition_occurrence.tsv observation.tsv          person_ids.tsv\ndrug_exposure.tsv        person.tsv\n</code>\n</pre> <p>View other options</p> <pre><code>!carrot run map --help\n</code></pre> <pre>\n<code>Usage: carrot run map [OPTIONS] [INPUTS]...\n\n  Perform OMOP Mapping given an json file and a series of input files\n\n  INPUTS should be a space separated list of individual input files or\n  directories (which contain .csv files)\n\nOptions:\n  --rules TEXT                    input json file containing all the mapping\n                                  rules to be applied  [required]\n  --indexing-conf TEXT            configuration file to specify how to start\n                                  the indexing\n  --csv-separator [;|:| |,| ]     choose a separator to use when dumping\n                                  output csv files\n  --use-profiler                  turn on saving statistics for profiling CPU\n                                  and memory usage\n  --format-level [0|1|2]          Choose the level of formatting to apply on\n                                  the output data. 0 - no formatting. 1 -\n                                  automatic formatting. 2 (default) - check\n                                  formatting (will crash if input data is not\n                                  already formatted).\n  --output-folder TEXT            define the output folder where to dump csv\n                                  files to\n  --write-mode [w|a]              force the write-mode on existing files\n  --split-outputs                 force the output files to be split into\n                                  separate files\n  --allow-missing-data            don't crash if there is data tables in rules\n                                  file that hasnt been loaded\n  --database TEXT                 define the output database where to insert\n                                  data into\n  -nc, --number-of-rows-per-chunk INTEGER\n                                  Choose the number of rows (INTEGER) of input\n                                  data to load (chunksize). The option 'auto'\n                                  will work out the ideal chunksize. Inputing\n                                  a value &lt;=0 will turn off data chunking\n                                  (default behaviour).\n  -np, --number-of-rows-to-process INTEGER\n                                  the total number of rows to process\n  --person-id-map TEXT            pass the location of a file containing\n                                  existing masked person_ids\n  --db TEXT                       instead, pass a connection string to a db\n  --merge-output                  merge the output into one file\n  --parse-original-person-id      turn off automatic conversion (creation) of\n                                  person_id to (as) Integer\n  --no-fill-missing-columns       Turn off automatically filling missing CDM\n                                  columns\n  --log-file TEXT                 specify a path for a log file\n  --max-rules INTEGER             maximum number of rules to process\n  --object TEXT                   give a list of objects by name to process\n  --table TEXT                    give a list of tables by name to process\n  --help                          Show this message and exit.\n</code>\n</pre> <pre><code>!carrot run map --rules ../data/rules.json --output-folder ./temp/2/ -np 1000 --split-outputs  ../data/part2/*.csv\n</code></pre> <pre>\n<code>2022-06-17 14:48:56 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Using a chunksize of '1000' nrows\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x10d9f8d30&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x10d9f8f40&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x10da2e1c0&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x10da2e460&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x10da2e700&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x10da2e9a0&gt;]\n2022-06-17 14:48:56 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:56 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:48:56 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:48:56 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:48:56 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 14:48:56 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:48:56 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:48:56 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:48:56 - CommonDataModel - INFO - working on person\n2022-06-17 14:48:56 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:48:56 - Person - INFO - Called apply_rules\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:48:56 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:56 - Person - INFO - Mapped person_id\n2022-06-17 14:48:56 - Person - WARNING - Requiring non-null values in gender_concept_id removed 216 rows, leaving 284 rows.\n2022-06-17 14:48:56 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:56 - Person - INFO - created df (0x10da5c580)[MALE_3025]\n2022-06-17 14:48:56 - CommonDataModel - INFO - finished MALE 3025 (0x10da5c580) ... 1/2 completed, 284 rows\n2022-06-17 14:48:56 - LocalDataCollection - INFO - saving person_ids.0x10da7f4c0.2022-06-17T134856 to ./temp/2//person_ids.0x10da7f4c0.2022-06-17T134856.tsv\n2022-06-17 14:48:56 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:56 - CommonDataModel - INFO - saving dataframe (0x10da5c580) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10d9f8970&gt;\n2022-06-17 14:48:56 - LocalDataCollection - INFO - saving person.MALE_3025.0x10da5c580.2022-06-17T134856 to ./temp/2//person.MALE_3025.0x10da5c580.2022-06-17T134856.tsv\n2022-06-17 14:48:56 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:56 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:48:56 - Person - INFO - Called apply_rules\n2022-06-17 14:48:56 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:56 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:56 - Person - INFO - Mapped person_id\n2022-06-17 14:48:56 - Person - WARNING - Requiring non-null values in gender_concept_id removed 286 rows, leaving 214 rows.\n2022-06-17 14:48:56 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:56 - Person - INFO - created df (0x10daeadc0)[FEMALE_3026]\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:56 - CommonDataModel - INFO - finished FEMALE 3026 (0x10daeadc0) ... 2/2 completed, 214 rows\n2022-06-17 14:48:56 - LocalDataCollection - INFO - saving person_ids.0x10da7f850.2022-06-17T134856 to ./temp/2//person_ids.0x10da7f850.2022-06-17T134856.tsv\n2022-06-17 14:48:56 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:56 - CommonDataModel - INFO - saving dataframe (0x10daeadc0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10d9f8970&gt;\n2022-06-17 14:48:56 - LocalDataCollection - INFO - saving person.FEMALE_3026.0x10daeadc0.2022-06-17T134856 to ./temp/2//person.FEMALE_3026.0x10daeadc0.2022-06-17T134856.tsv\n2022-06-17 14:48:56 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:48:56 - CommonDataModel - INFO - finalised person on iteration 0 producing 498 rows from 2 tables\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:56 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Demographics.csv'\n2022-06-17 14:48:56 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:48:56 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:56 - LocalDataCollection - INFO - resetting used bricks\nTraceback (most recent call last):\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/bin/carrot\", line 33, in &lt;module&gt;\n    sys.exit(load_entry_point('carrot-cdm', 'console_scripts', 'carrot')())\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/decorators.py\", line 26, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/cli/subcommands/run.py\", line 584, in map\n    cdm.process(conserve_memory=True)\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/cdm/model.py\", line 707, in process\n    self.inputs.reset()\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/io/common.py\", line 101, in reset\n    brick.reset()\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/io/common.py\", line 130, in reset\n    f.seek(0)\nValueError: I/O operation on closed file.\n</code>\n</pre> <pre><code>!ls ./temp/2/\n</code></pre> <pre>\n<code>condition_occurrence.Cough_3031.0x11bcd0220.2022-06-17T134233.tsv\ncondition_occurrence.Dizziness_3030.0x11bcc8220.2022-06-17T134233.tsv\ncondition_occurrence.Fatigue_3029.0x11bcbaf70.2022-06-17T134233.tsv\ncondition_occurrence.Fever_3032.0x11bcba5b0.2022-06-17T134233.tsv\ncondition_occurrence.Headache_3028.0x11bcba820.2022-06-17T134233.tsv\ncondition_occurrence.Hypertensive_disorder_3050.0x11bd18910.2022-06-17T134234.tsv\ncondition_occurrence.Ischemic_heart_disease_3049.0x11bd10520.2022-06-17T134234.tsv\ncondition_occurrence.Mental_disorder_3047.0x11bd18250.2022-06-17T134234.tsv\ncondition_occurrence.Mental_health_problem_3046.0x11bd02490.2022-06-17T134234.tsv\ncondition_occurrence.Muscle_pain_3033.0x11bcddb20.2022-06-17T134233.tsv\ncondition_occurrence.Pneumonia_3042.0x11bcd0970.2022-06-17T134234.tsv\ncondition_occurrence.Type_2_diabetes_mellitus_3048.0x11bd183a0.2022-06-17T134234.tsv\ndrug_exposure.COVID_19_vaccine_3034.0x11bd18910.2022-06-17T134234.tsv\ndrug_exposure.COVID_19_vaccine_3035.0x11bd10ac0.2022-06-17T134234.tsv\ndrug_exposure.COVID_19_vaccine_3036.0x11bd272e0.2022-06-17T134234.tsv\ndrug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040.0x11bd135b0.2022-06-17T134234.tsv\ndrug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041.0x11bd02a90.2022-06-17T134235.tsv\nobservation.2019_nCoV_3044.0x11bcba430.2022-06-17T134233.tsv\nobservation.Antibody_3027.0x11bc81970.2022-06-17T134233.tsv\nobservation.Cancer_3045.0x11bcbaa60.2022-06-17T134233.tsv\nobservation.H_O_heart_failure_3043.0x11bca2400.2022-06-17T134233.tsv\nperson.FEMALE_3026.0x10abb0e80.2022-06-17T133551.tsv\nperson.FEMALE_3026.0x10daeadc0.2022-06-17T134856.tsv\nperson.FEMALE_3026.0x10e66ee80.2022-06-17T133508.tsv\nperson.FEMALE_3026.0x11bc22610.2022-06-17T134233.tsv\nperson.MALE_3025.0x10ab4dd30.2022-06-17T133551.tsv\nperson.MALE_3025.0x10da5c580.2022-06-17T134856.tsv\nperson.MALE_3025.0x10e60bd30.2022-06-17T133507.tsv\nperson.MALE_3025.0x11bc3e670.2022-06-17T134233.tsv\nperson_ids.0x10ab21520.2022-06-17T133551.tsv\nperson_ids.0x10ab7b520.2022-06-17T133551.tsv\nperson_ids.0x10da7f4c0.2022-06-17T134856.tsv\nperson_ids.0x10da7f850.2022-06-17T134856.tsv\nperson_ids.0x10e5df520.2022-06-17T133507.tsv\nperson_ids.0x10e639520.2022-06-17T133508.tsv\nperson_ids.0x11bbf2400.2022-06-17T134233.tsv\nperson_ids.0x11bbf2880.2022-06-17T134233.tsv\n</code>\n</pre> <pre><code>!rm -rf ./temp/3\n</code></pre> <pre><code>!carrot run merge --output-folder ./temp/3/  ./temp/2/*2022-03-17*\n</code></pre> <pre>\n<code>2022-06-17 14:49:00 - LocalDataCollection - INFO - DataCollection Object Created\nTraceback (most recent call last):\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/bin/carrot\", line 33, in &lt;module&gt;\n    sys.exit(load_entry_point('carrot-cdm', 'console_scripts', 'carrot')())\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/cli/subcommands/run.py\", line 186, in merge\n    inputs = tools.load_tsv(inputs)#,chunksize=number_of_rows_per_chunk)\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/tools/file_helpers.py\", line 179, in load_tsv\n    return load_csv(_map,**kwargs)\n  File \"/Users/calummacdonald/Usher/CO-CONNECT/Software/CaRROT-CDM/carrot/tools/file_helpers.py\", line 154, in load_csv\n    df = pd.read_csv(load_path+fname,\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 575, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 933, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1217, in _make_engine\n    self.handles = get_handle(  # type: ignore[call-overload]\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/pandas/io/common.py\", line 789, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: './temp/2/*2022-03-17*'\n</code>\n</pre> <pre><code>!pseudonymise --help\n</code></pre> <pre>\n<code>Usage: pseudonymise [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  csv  Command to pseudonymise csv files, given a salt, the name of the...\n</code>\n</pre> <p>(That only works for csv files right now)</p> <pre><code>!pseudonymise csv --help\n</code></pre> <pre>\n<code>Usage: pseudonymise csv [OPTIONS] INPUT\n\n  Command to pseudonymise csv files, given a salt, the name of the columns to\n  pseudonymise and the input file.\n\nOptions:\n  -s, --salt TEXT           salt hash  [required]\n  -c, --column, --id TEXT   name of the identifier columns  [required]\n  -o, --output-folder TEXT  path of the output folder  [required]\n  --chunksize INTEGER       set the chunksize when loading data, useful for\n                            large files\n  --help                    Show this message and exit.\n</code>\n</pre> <pre><code>!pseudonymise csv --salt 123456789 --id ID -o ./temp/pseudo/ ../data/part2/Demographics.csv \n</code></pre> <pre>\n<code>2022-06-17 14:49:04.549 | INFO     | cli.cli:csv:16 - Working on file ../data/part2/Demographics.csv, pseudonymising columns '['ID']' with salt '123456789'\n2022-06-17 14:49:04.549 | INFO     | cli.cli:csv:22 - Saving new file to ./temp/pseudo//Demographics.csv\n2022-06-17 14:49:04.556 | DEBUG    | cli.cli:csv:32 - 0      499a2a6f6ce4ecc47010d16e992cffe88570e3a7ca287f...\n1      978e0d392954933a08d48326a00cec476f516f504f32d2...\n2      22027010878a0b242d60bb8a6a13f865b85d209ee0b2d3...\n3      def6fdd856051b80e93d8864f96101053115be975eefe7...\n4      8bfebcde35b212c4a31ab32dc21496fd70580295626fdb...\n                             ...                        \n495    0920cf0e06108a3ecc42eec0f30f3e22c90c564843a67d...\n496    d290686a953e4334cc1b047d9980f0dee0972f834ca5c0...\n497    69c37f599ba50f9657937fdbd312535ebca4253e8223f9...\n498    ecad0f0b5749ac78f5187b83a6a95bd400ad9e0014c418...\n499    8e20b31c73fd01798c37d3e9725243903758b9a3252a45...\nName: ID, Length: 500, dtype: object\n2022-06-17 14:49:04.563 | INFO     | cli.cli:csv:52 - Done with ./temp/pseudo//Demographics.csv!\n</code>\n</pre>"},{"location":"CaRROT-CDM/notebooks/Part%202%20-%20Command%20Line%20/#t-tool","title":"T-Tool","text":"<p>aka Transform Tool aka Map Tool aka the thing that does <code>input data + rules = CDM data</code></p> <p>It is a wrapper that builds the various python instances to perform simple mapping given rules and csv files:</p> <p></p> <p>Typical way of running:</p>"},{"location":"CaRROT-CDM/notebooks/Part%202%20-%20Command%20Line%20/#merge-tool","title":"Merge Tool","text":"<p>If someone is using local files and wants to manually pick and merge the split files they produced, the merge tool can be used to do this (and will remove any duplicates)</p>"},{"location":"CaRROT-CDM/notebooks/Part%202%20-%20Command%20Line%20/#pseudonymise","title":"Pseudonymise","text":"<p>This is actually packaged as a separate tool CO-CONNECT/Pseudonymisation which is also installed with <code>co-connect-tools</code> is run. The package comes with its own pseudonymisation command line tool:</p>"},{"location":"CaRROT-CDM/notebooks/Part%203%20-%20ETL-Tool/","title":"Part 3   ETL Tool","text":"<p>The ETL-Tool is the command line tool <code>carrot.etl</code>, it just takes in a <code>yaml</code> file to configure how to run</p> <pre><code>!carrot etl --help\n</code></pre> <pre>\n<code>Usage: carrot etl [OPTIONS] COMMAND [ARGS]...\n\n  Command group for running the full ETL of a dataset\n\nOptions:\n  --config, --config-file TEXT  specify a yaml configuration file\n  -d, --daemon                  run the ETL as a daemon process\n  -l, --log-file TEXT           specify the log file to write to\n  --help                        Show this message and exit.\n\nCommands:\n  check-tables   check tables\n  clean-table    clean (delete all rows) of a given table name\n  clean-tables   clean (delete all rows) in the tables defined in the...\n  create-tables  create new bclink tables\n  delete-tables  delete some tables\n</code>\n</pre> <p>A very basic configuration for running locally (effectively just running the T-Tool <code>carrot.run map</code> on one input) </p> <pre><code>definition = \"\"\"\ntransform: \n  settings: &amp;amp;settings\n    output: output/\n    rules: ../data/rules.json                                                                                                                                        \n  data:\n    - input: ../data/part1\n      &amp;lt;&amp;lt;: *settings\n\"\"\"\nwith open('config.yml','w') as f:\n    f.write(definition)\n</code></pre> <pre><code>!carrot etl --config config.yml\n</code></pre> <pre>\n<code>2022-06-17 14:48:53 - run_etl - INFO - running etl on config.yml (last modified: 1655473730.8621333)\n2022-06-17 14:48:53 - _run_data - WARNING - output/ exists!\n2022-06-17 14:48:53 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x1273f1130&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x1273f12e0&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x1273f1550&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x1273f17c0&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x1273f19d0&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x1273f1c10&gt;]\n2022-06-17 14:48:54 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:54 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:48:54 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:48:54 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:48:54 - LocalDataCollection - WARNING - Loading existing person ids from...\n2022-06-17 14:48:54 - LocalDataCollection - WARNING - ['output/person_ids.0x124cef850.2022-06-17T133735.tsv', 'output/person_ids.0x124cef970.2022-06-17T133735.tsv']\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:48:54 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 14:48:54 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:48:54 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:48:54 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:48:54 - CommonDataModel - INFO - working on person\n2022-06-17 14:48:54 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:48:54 - Person - INFO - Called apply_rules\n2022-06-17 14:48:54 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\ncould not convert string to float: 'na'\n2022-06-17 14:48:54 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:54 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:54 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:54 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:54 - Person - INFO - Mapped person_id\n2022-06-17 14:48:54 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 14:48:54 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 14:48:54 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:54 - Person - INFO - created df (0x1274e2c40)[MALE_3025]\n2022-06-17 14:48:54 - CommonDataModel - INFO - finished MALE 3025 (0x1274e2c40) ... 1/2 completed, 561 rows\n2022-06-17 14:48:54 - CommonDataModel - ERROR - 'pk1' already found in the person_id_masker\n2022-06-17 14:48:54 - CommonDataModel - ERROR - '1' assigned to this already\n2022-06-17 14:48:54 - CommonDataModel - ERROR - was trying to set '997'\n2022-06-17 14:48:54 - CommonDataModel - ERROR - Most likely cause is this is duplicate data!\n2022-06-17 14:48:54 - _run_data - ERROR - Duplicate person found!\n2022-06-17 14:48:54 - _run_data - ERROR - failed to map ['../data/part1/Blood_Test.csv', '../data/part1/Demographics.csv', '../data/part1/GP_Records.csv', '../data/part1/Hospital_Visit.csv', '../data/part1/Serology.csv', '../data/part1/Symptoms.csv', '../data/part1/Vaccinations.csv', '../data/part1/pks.csv'] because there were people\n2022-06-17 14:48:54 - _run_data - ERROR -  already processed and present in existing data. Check the person_id map/lookup!\n</code>\n</pre> <p>Changing to have a load tab to configure the output for bclink:</p> <pre><code>definition = \"\"\"\nload: &amp;amp;load-bclink\n  cache: ./output/cache/\n  bclink:\n      dry_run: true\ntransform: \n  settings: &amp;amp;settings\n    output: *load-bclink\n    rules: ../data/rules.json                                                                                                                                        \n  data:\n    - input: ../data/part1\n      &amp;lt;&amp;lt;: *settings\n\"\"\"\nwith open('config.yml','w') as f:\n    f.write(definition)\n</code></pre> <pre><code>!carrot etl --config config.yml\n</code></pre> <pre>\n<code>2022-06-17 14:48:57 - run_etl - INFO - running etl on config.yml (last modified: 1655473734.7268007)\n2022-06-17 14:48:57 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x114023130&gt;]\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x1140232e0&gt;]\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x114023550&gt;]\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x1140237c0&gt;]\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x1140239d0&gt;]\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x114023c10&gt;]\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - setup bclink collection\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - condition_occurrence (condition_occurrence) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'death' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - death (death) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'drug_exposure' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - drug_exposure (drug_exposure) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'measurement' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - measurement (measurement) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'observation' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - observation (observation) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - person (person) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'procedure_occurrence' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - procedure_occurrence (procedure_occurrence) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'specimen' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - specimen (specimen) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'visit_occurrence' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - visit_occurrence (visit_occurrence) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person_ids' ) bclink\n2022-06-17 14:48:57 - BCLinkHelpers - INFO - person_ids (person_ids) already exists --&gt; all good\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM death bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM drug_exposure bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM measurement bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM observation bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM procedure_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM specimen bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM visit_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person_ids bclink\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:48:57 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:48:57 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:48:57 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT * FROM person_ids  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'condition_occurrence' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM condition_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM death bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'death' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM death ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM drug_exposure bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'drug_exposure' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM drug_exposure ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM measurement bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'measurement' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM measurement ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM observation bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'observation' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM observation ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'person' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM person ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM procedure_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'procedure_occurrence' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM procedure_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM specimen bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'specimen' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM specimen ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM visit_occurrence bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'visit_occurrence' LIMIT 1  bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM visit_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:48:57 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 14:48:57 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:48:57 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:48:57 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:48:57 - CommonDataModel - INFO - working on person\n2022-06-17 14:48:57 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:48:57 - Person - INFO - Called apply_rules\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\ncould not convert string to float: 'na'\n2022-06-17 14:48:57 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:57 - Person - INFO - Mapped person_id\n2022-06-17 14:48:57 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 14:48:57 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 14:48:57 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:57 - Person - INFO - created df (0x1141040a0)[MALE_3025]\n2022-06-17 14:48:57 - CommonDataModel - INFO - finished MALE 3025 (0x1141040a0) ... 1/2 completed, 561 rows\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - saving person_ids.0x1140ee7f0.2022-06-17T134857 to ./output/cache//person_ids.0x1140ee7f0.2022-06-17T134857.tsv\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person_ids --user=data --data_file=./output/cache//person_ids.0x1140ee7f0.2022-06-17T134857.tsv --support --bcqueue bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person_ids --user=data --database=bclink\n2022-06-17 14:48:57 - CommonDataModel - INFO - saving dataframe (0x1141040a0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - saving person.MALE_3025.0x1141040a0.2022-06-17T134857 to ./output/cache//person.MALE_3025.0x1141040a0.2022-06-17T134857.tsv\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person --user=data --data_file=./output/cache//person.MALE_3025.0x1141040a0.2022-06-17T134857.tsv --support --bcqueue bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person --user=data --database=bclink\n2022-06-17 14:48:57 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:48:57 - Person - INFO - Called apply_rules\ncould not convert string to float: 'na'\n2022-06-17 14:48:57 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:48:57 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:48:57 - Person - INFO - Mapped person_id\n2022-06-17 14:48:57 - Person - WARNING - Requiring non-null values in gender_concept_id removed 565 rows, leaving 435 rows.\n2022-06-17 14:48:57 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:48:57 - Person - INFO - created df (0x114104340)[FEMALE_3026]\n2022-06-17 14:48:57 - CommonDataModel - INFO - finished FEMALE 3026 (0x114104340) ... 2/2 completed, 435 rows\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - saving person_ids.0x1140ee7c0.2022-06-17T134857 to ./output/cache//person_ids.0x1140ee7c0.2022-06-17T134857.tsv\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person_ids --user=data --data_file=./output/cache//person_ids.0x1140ee7c0.2022-06-17T134857.tsv --support --bcqueue bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person_ids --user=data --database=bclink\n2022-06-17 14:48:57 - CommonDataModel - INFO - saving dataframe (0x114104340) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - saving person.FEMALE_3026.0x114104340.2022-06-17T134857 to ./output/cache//person.FEMALE_3026.0x114104340.2022-06-17T134857.tsv\n2022-06-17 14:48:57 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person --user=data --data_file=./output/cache//person.FEMALE_3026.0x114104340.2022-06-17T134857.tsv --support --bcqueue bclink\n2022-06-17 14:48:57 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person --user=data --database=bclink\n2022-06-17 14:48:57 - CommonDataModel - INFO - finalised person on iteration 0 producing 996 rows from 2 tables\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:57 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:57 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:48:57 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 14:48:57 - CommonDataModel - INFO - working on observation\n2022-06-17 14:48:57 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 14:48:57 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:57 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 14:48:57 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:57 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:58 - Observation - INFO - Mapped person_id\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:58 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:58 - Observation - INFO - created df (0x114177d00)[Antibody_3027]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished Antibody 3027 (0x114177d00) ... 1/4 completed, 413 rows\n2022-06-17 14:48:58 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:58 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:58 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:58 - CommonDataModel - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x114177d00) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving observation.Antibody_3027.0x114177d00.2022-06-17T134858 to ./output/cache//observation.Antibody_3027.0x114177d00.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=observation --user=data --data_file=./output/cache//observation.Antibody_3027.0x114177d00.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=observation --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 14:48:58 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:58 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:58 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:58 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 937 rows, leaving 263 rows.\n2022-06-17 14:48:58 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:58 - Observation - INFO - created df (0x114145a60)[H_O_heart_failure_3043]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x114145a60) ... 2/4 completed, 263 rows\n2022-06-17 14:48:58 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:58 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:58 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:58 - CommonDataModel - ERROR - 262/263 were good, 1 studies are removed.\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x114145a60) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving observation.H_O_heart_failure_3043.0x114145a60.2022-06-17T134858 to ./output/cache//observation.H_O_heart_failure_3043.0x114145a60.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=observation --user=data --data_file=./output/cache//observation.H_O_heart_failure_3043.0x114145a60.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=observation --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 14:48:58 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:58 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:58 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 1023 rows, leaving 177 rows.\n2022-06-17 14:48:58 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:48:58 - Observation - INFO - created df (0x114191d90)[2019_nCoV_3044]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x114191d90) ... 3/4 completed, 177 rows\n2022-06-17 14:48:58 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:58 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:58 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:58 - CommonDataModel - ERROR - 176/177 were good, 1 studies are removed.\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x114191d90) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving observation.2019_nCoV_3044.0x114191d90.2022-06-17T134858 to ./output/cache//observation.2019_nCoV_3044.0x114191d90.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=observation --user=data --data_file=./output/cache//observation.2019_nCoV_3044.0x114191d90.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=observation --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 14:48:58 - Observation - INFO - Called apply_rules\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:48:58 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:48:58 - Observation - INFO - Mapped person_id\n2022-06-17 14:48:58 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 851 rows, leaving 349 rows.\n2022-06-17 14:48:58 - Observation - INFO - Automatically formatting data columns.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:58 - Observation - INFO - created df (0x114191160)[Cancer_3045]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished Cancer 3045 (0x114191160) ... 4/4 completed, 349 rows\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x114191160) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving observation.Cancer_3045.0x114191160.2022-06-17T134858 to ./output/cache//observation.Cancer_3045.0x114191160.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=observation --user=data --data_file=./output/cache//observation.Cancer_3045.0x114191160.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=observation --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - finalised observation on iteration 0 producing 1197 rows from 4 tables\n2022-06-17 14:48:58 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:48:58 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:48:58 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:48:58 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 14:48:58 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:58 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 55 rows, leaving 275 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 274 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - created df (0x114191df0)[Headache_3028]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished Headache 3028 (0x114191df0) ... 1/12 completed, 274 rows\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x114191df0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving condition_occurrence.Headache_3028.0x114191df0.2022-06-17T134858 to ./output/cache//condition_occurrence.Headache_3028.0x114191df0.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Headache_3028.0x114191df0.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 95 rows, leaving 235 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 234 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - created df (0x11419ca30)[Fatigue_3029]\n2022-06-17 14:48:58 - CommonDataModel - INFO - finished Fatigue 3029 (0x11419ca30) ... 2/12 completed, 234 rows\n2022-06-17 14:48:58 - CommonDataModel - INFO - saving dataframe (0x11419ca30) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - saving condition_occurrence.Fatigue_3029.0x11419ca30.2022-06-17T134858 to ./output/cache//condition_occurrence.Fatigue_3029.0x11419ca30.2022-06-17T134858.tsv\n2022-06-17 14:48:58 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Fatigue_3029.0x11419ca30.2022-06-17T134858.tsv --support --bcqueue bclink\n2022-06-17 14:48:58 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:58 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 195 rows, leaving 135 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 134 rows.\n2022-06-17 14:48:58 - ConditionOccurrence - INFO - Automatically formatting data columns.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:58 - ConditionOccurrence - INFO - created df (0x114191b80)[Dizziness_3030]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Dizziness 3030 (0x114191b80) ... 3/12 completed, 134 rows\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x114191b80) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Dizziness_3030.0x114191b80.2022-06-17T134859 to ./output/cache//condition_occurrence.Dizziness_3030.0x114191b80.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Dizziness_3030.0x114191b80.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 100 rows, leaving 230 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 229 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x1141b5520)[Cough_3031]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Cough 3031 (0x1141b5520) ... 4/12 completed, 229 rows\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x1141b5520) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Cough_3031.0x1141b5520.2022-06-17T134859 to ./output/cache//condition_occurrence.Cough_3031.0x1141b5520.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Cough_3031.0x1141b5520.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 265 rows, leaving 65 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x1141b5a60)[Fever_3032]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Fever 3032 (0x1141b5a60) ... 5/12 completed, 65 rows\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x1141b5a60) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Fever_3032.0x1141b5a60.2022-06-17T134859 to ./output/cache//condition_occurrence.Fever_3032.0x1141b5a60.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Fever_3032.0x1141b5a60.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 295 rows, leaving 35 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 34 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x1141c3f10)[Muscle_pain_3033]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Muscle pain 3033 (0x1141c3f10) ... 6/12 completed, 34 rows\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x1141c3f10) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Muscle_pain_3033.0x1141c3f10.2022-06-17T134859 to ./output/cache//condition_occurrence.Muscle_pain_3033.0x1141c3f10.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Muscle_pain_3033.0x1141c3f10.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n</code>\n</pre> <pre>\n<code>2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1029 rows, leaving 171 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x1141a7f10)[Pneumonia_3042]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Pneumonia 3042 (0x1141a7f10) ... 7/12 completed, 171 rows\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x1141a7f10) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Pneumonia_3042.0x1141a7f10.2022-06-17T134859 to ./output/cache//condition_occurrence.Pneumonia_3042.0x1141a7f10.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Pneumonia_3042.0x1141a7f10.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x1141ce160)[Mental_health_problem_3046]\n2022-06-17 14:48:59 - CommonDataModel - INFO - finished Mental health problem 3046 (0x1141ce160) ... 8/12 completed, 444 rows\n2022-06-17 14:48:59 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:48:59 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:48:59 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:48:59 - CommonDataModel - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 14:48:59 - CommonDataModel - INFO - saving dataframe (0x1141ce160) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - saving condition_occurrence.Mental_health_problem_3046.0x1141ce160.2022-06-17T134859 to ./output/cache//condition_occurrence.Mental_health_problem_3046.0x1141ce160.2022-06-17T134859.tsv\n2022-06-17 14:48:59 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Mental_health_problem_3046.0x1141ce160.2022-06-17T134859.tsv --support --bcqueue bclink\n2022-06-17 14:48:59 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:48:59 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:48:59 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:48:59 - ConditionOccurrence - INFO - created df (0x114191e80)[Mental_disorder_3047]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished Mental disorder 3047 (0x114191e80) ... 9/12 completed, 444 rows\n2022-06-17 14:49:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:00 - CommonDataModel - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x114191e80) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving condition_occurrence.Mental_disorder_3047.0x114191e80.2022-06-17T134900 to ./output/cache//condition_occurrence.Mental_disorder_3047.0x114191e80.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Mental_disorder_3047.0x114191e80.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped person_id\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:00 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1688 rows, leaving 264 rows.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - created df (0x1141ed970)[Type_2_diabetes_mellitus_3048]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x1141ed970) ... 10/12 completed, 264 rows\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x1141ed970) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving condition_occurrence.Type_2_diabetes_mellitus_3048.0x1141ed970.2022-06-17T134900 to ./output/cache//condition_occurrence.Type_2_diabetes_mellitus_3048.0x1141ed970.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Type_2_diabetes_mellitus_3048.0x1141ed970.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:00 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1738 rows, leaving 214 rows.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - created df (0x1141e4c70)[Ischemic_heart_disease_3049]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x1141e4c70) ... 11/12 completed, 214 rows\n2022-06-17 14:49:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:00 - CommonDataModel - ERROR - 213/214 were good, 1 studies are removed.\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x1141e4c70) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving condition_occurrence.Ischemic_heart_disease_3049.0x1141e4c70.2022-06-17T134900 to ./output/cache//condition_occurrence.Ischemic_heart_disease_3049.0x1141e4c70.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Ischemic_heart_disease_3049.0x1141e4c70.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:00 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1822 rows, leaving 130 rows.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - ConditionOccurrence - INFO - created df (0x1141cea00)[Hypertensive_disorder_3050]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x1141cea00) ... 12/12 completed, 130 rows\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x1141cea00) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving condition_occurrence.Hypertensive_disorder_3050.0x1141cea00.2022-06-17T134900 to ./output/cache//condition_occurrence.Hypertensive_disorder_3050.0x1141cea00.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./output/cache//condition_occurrence.Hypertensive_disorder_3050.0x1141cea00.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 2630 rows from 12 tables\n2022-06-17 14:49:00 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:00 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:00 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:00 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 14:49:00 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 14:49:00 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:00 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:00 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:49:00 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - DrugExposure - INFO - created df (0x114215eb0)[COVID_19_vaccine_3034]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x114215eb0) ... 1/5 completed, 245 rows\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x114215eb0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving drug_exposure.COVID_19_vaccine_3034.0x114215eb0.2022-06-17T134900 to ./output/cache//drug_exposure.COVID_19_vaccine_3034.0x114215eb0.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./output/cache//drug_exposure.COVID_19_vaccine_3034.0x114215eb0.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 14:49:00 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:00 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 494 rows, leaving 226 rows.\n2022-06-17 14:49:00 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 1 rows, leaving 225 rows.\n2022-06-17 14:49:00 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - DrugExposure - INFO - created df (0x1141ed2b0)[COVID_19_vaccine_3035]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x1141ed2b0) ... 2/5 completed, 225 rows\n2022-06-17 14:49:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:00 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x1141ed2b0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving drug_exposure.COVID_19_vaccine_3035.0x1141ed2b0.2022-06-17T134900 to ./output/cache//drug_exposure.COVID_19_vaccine_3035.0x1141ed2b0.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./output/cache//drug_exposure.COVID_19_vaccine_3035.0x1141ed2b0.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 14:49:00 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:00 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:49:00 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:00 - DrugExposure - INFO - created df (0x1141ff310)[COVID_19_vaccine_3036]\n2022-06-17 14:49:00 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x1141ff310) ... 3/5 completed, 249 rows\n2022-06-17 14:49:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:00 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:49:00 - CommonDataModel - INFO - saving dataframe (0x1141ff310) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - saving drug_exposure.COVID_19_vaccine_3036.0x1141ff310.2022-06-17T134900 to ./output/cache//drug_exposure.COVID_19_vaccine_3036.0x1141ff310.2022-06-17T134900.tsv\n2022-06-17 14:49:00 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./output/cache//drug_exposure.COVID_19_vaccine_3036.0x1141ff310.2022-06-17T134900.tsv --support --bcqueue bclink\n2022-06-17 14:49:00 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:49:00 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 14:49:00 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:00 - DrugExposure - INFO - Mapped person_id\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:00 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:49:00 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:01 - DrugExposure - INFO - created df (0x1141ffa00)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 14:49:01 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x1141ffa00) ... 4/5 completed, 245 rows\n2022-06-17 14:49:01 - CommonDataModel - INFO - saving dataframe (0x1141ffa00) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - saving drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040.0x1141ffa00.2022-06-17T134901 to ./output/cache//drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040.0x1141ffa00.2022-06-17T134901.tsv\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:01 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./output/cache//drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040.0x1141ffa00.2022-06-17T134901.tsv --support --bcqueue bclink\n2022-06-17 14:49:01 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:49:01 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 14:49:01 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:01 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:01 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:49:01 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:01 - DrugExposure - INFO - created df (0x1141ff190)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 14:49:01 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x1141ff190) ... 5/5 completed, 249 rows\n2022-06-17 14:49:01 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:01 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:01 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:01 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:49:01 - CommonDataModel - INFO - saving dataframe (0x1141ff190) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x113fdee20&gt;\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - saving drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041.0x1141ff190.2022-06-17T134901 to ./output/cache//drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041.0x1141ff190.2022-06-17T134901.tsv\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:49:01 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./output/cache//drug_exposure.SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041.0x1141ff190.2022-06-17T134901.tsv --support --bcqueue bclink\n2022-06-17 14:49:01 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:49:01 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 1210 rows from 5 tables\n2022-06-17 14:49:01 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:01 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:01 - CommonDataModel - INFO - {\n      \"version\": \"0.0.0\",\n      \"created_by\": \"calummacdonald\",\n      \"created_at\": \"2022-06-17T134857\",\n      \"dataset\": \"CommonDataModel::FAILED: ExampleV4\",\n      \"total_data_processed\": {\n            \"person\": 996,\n            \"observation\": 1197,\n            \"condition_occurrence\": 2630,\n            \"drug_exposure\": 1210\n      }\n}\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - finalising, waiting for jobs to finish\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - job_ids to wait for: []\n2022-06-17 14:49:01 - BCLinkDataCollection - INFO - done!\n</code>\n</pre>"},{"location":"CaRROT-CDM/notebooks/Part%204%20-%20Local/","title":"Part 4   Local","text":"<p>Instead of using the CLI/T-Tool/ETL-Tool. The process can be run with a custom python script to give more control over the inputs/outputs and various configurations</p> <pre><code>import carrot\nimport glob\n</code></pre> <pre><code>inputs =  carrot.tools.load_csv(glob.glob('../data/part2/*'))\ninputs\n</code></pre> <pre>\n<code>2022-06-17 14:49:16 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Blood_Test.csv [&lt;carrot.io.common.DataBrick object at 0x106aba280&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x106af5ca0&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x10a7e3fd0&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x10a7e3fa0&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x106af5430&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x106af5580&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x10a8476d0&gt;]\n2022-06-17 14:49:16 - LocalDataCollection - INFO - Registering  pks.csv [&lt;carrot.io.common.DataBrick object at 0x10a8476a0&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x106af5d60&gt;</code>\n</pre> <pre><code>inputs.keys()\n</code></pre> <pre>\n<code>dict_keys(['Blood_Test.csv', 'Demographics.csv', 'GP_Records.csv', 'Hospital_Visit.csv', 'Serology.csv', 'Symptoms.csv', 'Vaccinations.csv', 'pks.csv'])</code>\n</pre> <pre><code>outputs = carrot.tools.create_csv_store(output_folder=\"./test_outputs/local/\")\noutputs\n</code></pre> <pre>\n<code>2022-06-17 14:49:16 - LocalDataCollection - INFO - DataCollection Object Created\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x10a847a00&gt;</code>\n</pre> <pre><code>rules = carrot.tools.load_json(\"../data/rules.json\")\n</code></pre> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=inputs,outputs=outputs)\ncdm.get_tables()\n</code></pre> <pre>\n<code>2022-06-17 14:49:16 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:49:16 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:49:16 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:49:16 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n</code>\n</pre> <pre>\n<code>['person', 'observation', 'condition_occurrence', 'drug_exposure']</code>\n</pre> <pre><code>cdm.get_objects()\n</code></pre> <pre>\n<code>{'person': {'MALE 3025': &lt;carrot.cdm.objects.versions.v5_3_1.person.Person at 0x106af5700&gt;,\n  'FEMALE 3026': &lt;carrot.cdm.objects.versions.v5_3_1.person.Person at 0x106af5850&gt;},\n 'observation': {'Antibody 3027': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x10a8a6ca0&gt;,\n  'H/O: heart failure 3043': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x10a8aa730&gt;,\n  '2019-nCoV 3044': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x10a8a6d30&gt;,\n  'Cancer 3045': &lt;carrot.cdm.objects.versions.v5_3_1.observation.Observation at 0x10a8aab80&gt;},\n 'condition_occurrence': {'Headache 3028': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8aab20&gt;,\n  'Fatigue 3029': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8ad880&gt;,\n  'Dizziness 3030': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8adf70&gt;,\n  'Cough 3031': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8ad280&gt;,\n  'Fever 3032': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8b3dc0&gt;,\n  'Muscle pain 3033': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x106af5190&gt;,\n  'Pneumonia 3042': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8b3d00&gt;,\n  'Mental health problem 3046': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8b8f10&gt;,\n  'Mental disorder 3047': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8b8b20&gt;,\n  'Type 2 diabetes mellitus 3048': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8bcdf0&gt;,\n  'Ischemic heart disease 3049': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8b8bb0&gt;,\n  'Hypertensive disorder 3050': &lt;carrot.cdm.objects.versions.v5_3_1.condition_occurrence.ConditionOccurrence at 0x10a8bca00&gt;},\n 'drug_exposure': {'COVID-19 vaccine 3034': &lt;carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure at 0x10a8c14c0&gt;,\n  'COVID-19 vaccine 3035': &lt;carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure at 0x10a8c1fd0&gt;,\n  'COVID-19 vaccine 3036': &lt;carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure at 0x10a8c1910&gt;,\n  'SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040': &lt;carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure at 0x10a8c1850&gt;,\n  'SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041': &lt;carrot.cdm.objects.versions.v5_3_1.drug_exposure.DrugExposure at 0x10a8c5d60&gt;}}</code>\n</pre> <pre><code>cdm.process()\n</code></pre> <pre>\n<code>2022-06-17 14:49:17 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:49:17 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:49:17 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:49:17 - CommonDataModel - INFO - working on person\n2022-06-17 14:49:17 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:49:17 - Person - INFO - Called apply_rules\n2022-06-17 14:49:17 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:49:17 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:49:17 - Person - INFO - Mapped person_id\n2022-06-17 14:49:17 - Person - WARNING - Requiring non-null values in gender_concept_id removed 216 rows, leaving 284 rows.\n2022-06-17 14:49:17 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:49:17 - Person - INFO - created df (0x10a8f3c70)[MALE_3025]\n2022-06-17 14:49:17 - CommonDataModel - INFO - finished MALE 3025 (0x10a8f3c70) ... 1/2 completed, 284 rows\n2022-06-17 14:49:17 - LocalDataCollection - INFO - making output folder ./test_outputs/local/\n2022-06-17 14:49:17 - LocalDataCollection - INFO - saving person_ids to ./test_outputs/local//person_ids.csv\n2022-06-17 14:49:17 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:17 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:49:17 - Person - INFO - Called apply_rules\n2022-06-17 14:49:17 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:49:17 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:49:17 - Person - INFO - Mapped person_id\n2022-06-17 14:49:17 - Person - WARNING - Requiring non-null values in gender_concept_id removed 286 rows, leaving 214 rows.\n2022-06-17 14:49:17 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:49:17 - Person - INFO - created df (0x10a919bb0)[FEMALE_3026]\n2022-06-17 14:49:17 - CommonDataModel - INFO - finished FEMALE 3026 (0x10a919bb0) ... 2/2 completed, 214 rows\n2022-06-17 14:49:17 - LocalDataCollection - INFO - updating person_ids in ./test_outputs/local//person_ids.csv\n2022-06-17 14:49:17 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:17 - CommonDataModel - INFO - saving dataframe (0x10a919be0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:17 - LocalDataCollection - INFO - saving person to ./test_outputs/local//person.csv\n2022-06-17 14:49:17 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:17 - CommonDataModel - INFO - finalised person on iteration 0 producing 498 rows from 2 tables\n2022-06-17 14:49:17 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:17 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:17 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:17 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 14:49:17 - CommonDataModel - INFO - working on observation\n2022-06-17 14:49:17 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 14:49:17 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:17 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:17 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:17 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:17 - Observation - INFO - created df (0x10aba8730)[Antibody_3027]\n2022-06-17 14:49:17 - CommonDataModel - INFO - finished Antibody 3027 (0x10aba8730) ... 1/4 completed, 204 rows\n2022-06-17 14:49:17 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 14:49:17 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:17 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:17 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:17 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:18 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 487 rows, leaving 113 rows.\n2022-06-17 14:49:18 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:18 - Observation - INFO - created df (0x10aba8c70)[H_O_heart_failure_3043]\n2022-06-17 14:49:18 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x10aba8c70) ... 2/4 completed, 113 rows\n2022-06-17 14:49:18 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 14:49:18 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:18 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:18 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 506 rows, leaving 94 rows.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:18 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:18 - Observation - INFO - created df (0x10abb9cd0)[2019_nCoV_3044]\n2022-06-17 14:49:18 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x10abb9cd0) ... 3/4 completed, 94 rows\n2022-06-17 14:49:18 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 14:49:18 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:18 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:18 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:18 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 426 rows, leaving 174 rows.\n2022-06-17 14:49:18 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:18 - Observation - INFO - created df (0x10abc4f10)[Cancer_3045]\n2022-06-17 14:49:18 - CommonDataModel - INFO - finished Cancer 3045 (0x10abc4f10) ... 4/4 completed, 174 rows\n2022-06-17 14:49:18 - CommonDataModel - INFO - saving dataframe (0x10abb9160) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:18 - LocalDataCollection - INFO - saving observation to ./test_outputs/local//observation.csv\n2022-06-17 14:49:18 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:18 - CommonDataModel - INFO - finalised observation on iteration 0 producing 585 rows from 4 tables\n2022-06-17 14:49:18 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:18 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:18 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:18 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 14:49:18 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:49:18 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:18 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:18 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 39 rows, leaving 126 rows.\n2022-06-17 14:49:18 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 125 rows.\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - created df (0x10abc49a0)[Headache_3028]\n2022-06-17 14:49:18 - CommonDataModel - INFO - finished Headache 3028 (0x10abc49a0) ... 1/12 completed, 125 rows\n2022-06-17 14:49:18 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:18 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:18 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:18 - CommonDataModel - ERROR - 123/125 were good, 2 studies are removed.\n2022-06-17 14:49:18 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:18 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 52 rows, leaving 113 rows.\n2022-06-17 14:49:18 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 112 rows.\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - created df (0x10abc4280)[Fatigue_3029]\n2022-06-17 14:49:18 - CommonDataModel - INFO - finished Fatigue 3029 (0x10abc4280) ... 2/12 completed, 112 rows\n2022-06-17 14:49:18 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:18 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:18 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:18 - CommonDataModel - ERROR - 110/112 were good, 2 studies are removed.\n2022-06-17 14:49:18 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:18 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:18 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 88 rows, leaving 77 rows.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10ac069d0)[Dizziness_3030]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Dizziness 3030 (0x10ac069d0) ... 3/12 completed, 77 rows\n2022-06-17 14:49:19 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:19 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 60 rows, leaving 105 rows.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10ac0bf10)[Cough_3031]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Cough 3031 (0x10ac0bf10) ... 4/12 completed, 105 rows\n2022-06-17 14:49:19 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:19 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:19 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:19 - CommonDataModel - ERROR - 103/105 were good, 2 studies are removed.\n2022-06-17 14:49:19 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:19 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 128 rows, leaving 37 rows.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10ac05970)[Fever_3032]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Fever 3032 (0x10ac05970) ... 5/12 completed, 37 rows\n2022-06-17 14:49:19 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:19 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 147 rows, leaving 18 rows.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10ac2eb20)[Muscle_pain_3033]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Muscle pain 3033 (0x10ac2eb20) ... 6/12 completed, 18 rows\n2022-06-17 14:49:19 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:19 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:19 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 509 rows, leaving 91 rows.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10ac24190)[Pneumonia_3042]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Pneumonia 3042 (0x10ac24190) ... 7/12 completed, 91 rows\n2022-06-17 14:49:19 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:19 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:19 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:19 - CommonDataModel - ERROR - 90/91 were good, 1 studies are removed.\n2022-06-17 14:49:19 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:19 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_source_value\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:19 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 742 rows, leaving 225 rows.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:19 - ConditionOccurrence - INFO - created df (0x10abc4820)[Mental_health_problem_3046]\n2022-06-17 14:49:19 - CommonDataModel - INFO - finished Mental health problem 3046 (0x10abc4820) ... 8/12 completed, 225 rows\n2022-06-17 14:49:20 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:20 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:20 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:20 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:49:20 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:20 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 742 rows, leaving 225 rows.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - created df (0x10ac3fcd0)[Mental_disorder_3047]\n2022-06-17 14:49:20 - CommonDataModel - INFO - finished Mental disorder 3047 (0x10ac3fcd0) ... 9/12 completed, 225 rows\n2022-06-17 14:49:20 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:20 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:20 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:20 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:49:20 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:20 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 851 rows, leaving 116 rows.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - created df (0x10ac5df40)[Type_2_diabetes_mellitus_3048]\n2022-06-17 14:49:20 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x10ac5df40) ... 10/12 completed, 116 rows\n2022-06-17 14:49:20 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:20 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 872 rows, leaving 95 rows.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - created df (0x10ac71e80)[Ischemic_heart_disease_3049]\n2022-06-17 14:49:20 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x10ac71e80) ... 11/12 completed, 95 rows\n2022-06-17 14:49:20 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:20 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 886 rows, leaving 81 rows.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:20 - ConditionOccurrence - INFO - created df (0x10ac88a60)[Hypertensive_disorder_3050]\n2022-06-17 14:49:20 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x10ac88a60) ... 12/12 completed, 81 rows\n2022-06-17 14:49:20 - CommonDataModel - INFO - saving dataframe (0x10abb9820) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:20 - LocalDataCollection - INFO - saving condition_occurrence to ./test_outputs/local//condition_occurrence.csv\n2022-06-17 14:49:20 - LocalDataCollection - INFO - finished save to file\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:21 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 1298 rows from 12 tables\n2022-06-17 14:49:21 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:21 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:21 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:21 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 14:49:21 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 14:49:21 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 14:49:21 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:21 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:21 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 231 rows, leaving 129 rows.\n2022-06-17 14:49:21 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:21 - DrugExposure - INFO - created df (0x10aca01f0)[COVID_19_vaccine_3034]\n2022-06-17 14:49:21 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x10aca01f0) ... 1/5 completed, 129 rows\n2022-06-17 14:49:21 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 14:49:21 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:21 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 246 rows, leaving 114 rows.\n2022-06-17 14:49:21 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:21 - DrugExposure - INFO - created df (0x10aca0970)[COVID_19_vaccine_3035]\n2022-06-17 14:49:21 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x10aca0970) ... 2/5 completed, 114 rows\n2022-06-17 14:49:21 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:21 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:21 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:21 - CommonDataModel - ERROR - 112/114 were good, 2 studies are removed.\n2022-06-17 14:49:21 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 14:49:21 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:21 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 243 rows, leaving 117 rows.\n2022-06-17 14:49:21 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:21 - DrugExposure - INFO - created df (0x10acccd30)[COVID_19_vaccine_3036]\n2022-06-17 14:49:21 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x10acccd30) ... 3/5 completed, 117 rows\n2022-06-17 14:49:21 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 14:49:21 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:21 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 231 rows, leaving 129 rows.\n2022-06-17 14:49:21 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:21 - DrugExposure - INFO - created df (0x10acd6730)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 14:49:21 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x10acd6730) ... 4/5 completed, 129 rows\n2022-06-17 14:49:21 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 14:49:21 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:21 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:22 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 243 rows, leaving 117 rows.\n2022-06-17 14:49:22 - DrugExposure - INFO - Automatically formatting data columns.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:22 - DrugExposure - INFO - created df (0x10aca0610)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 14:49:22 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x10aca0610) ... 5/5 completed, 117 rows\n2022-06-17 14:49:22 - CommonDataModel - INFO - saving dataframe (0x10ac2e6d0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:22 - LocalDataCollection - INFO - saving drug_exposure to ./test_outputs/local//drug_exposure.csv\n2022-06-17 14:49:22 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:22 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 604 rows from 5 tables\n2022-06-17 14:49:22 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:22 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 1 8507 1975 7 13 1975-07-13 00:00:00.000000 Male 8507 2 8507 1968 7 14 1968-07-14 00:00:00.000000 Male 8507 3 8507 1976 7 12 1976-07-12 00:00:00.000000 Male 8507 4 8507 1942 7 21 1942-07-21 00:00:00.000000 Male 8507 5 8507 1943 7 21 1943-07-21 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 494 8532 1966 7 15 1966-07-15 00:00:00.000000 Female 8532 495 8532 1979 7 12 1979-07-12 00:00:00.000000 Female 8532 496 8532 1994 7 8 1994-07-08 00:00:00.000000 Female 8532 497 8532 1950 7 19 1950-07-19 00:00:00.000000 Female 8532 498 8532 1945 7 20 1945-07-20 00:00:00.000000 Female 8532 <p>498 rows \u00d7 7 columns</p> <pre><code>cdm['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 1 458 4288455 2020-11-20 2020-11-20 00:00:00.000000 61.597638175407624 4288455 2 330 4288455 2022-09-12 2022-09-12 00:00:00.000000 6.413970727863393 4288455 3 136 4288455 2019-02-02 2019-02-02 00:00:00.000000 4.691778998076871 4288455 4 17 4288455 2020-08-18 2020-08-18 00:00:00.000000 14.77684965289811 4288455 5 55 4288455 2019-11-23 2019-11-23 00:00:00.000000 79.61828150487227 4288455 ... ... ... ... ... ... ... 581 279 40757663 2019-11-10 2019-11-10 00:00:00.000000 Cancer 40757663 582 494 40757663 2020-08-28 2020-08-28 00:00:00.000000 Cancer 40757663 583 282 40757663 2020-02-03 2020-02-03 00:00:00.000000 Cancer 40757663 584 284 40757663 2020-11-21 2020-11-21 00:00:00.000000 Cancer 40757663 585 284 40757663 2019-09-06 2019-09-06 00:00:00.000000 Cancer 40757663 <p>585 rows \u00d7 6 columns</p> <pre><code>cdm.logs\n</code></pre> <pre>\n<code>{'meta': {'version': '0.0.0',\n  'created_by': 'calummacdonald',\n  'created_at': '2022-06-17T134916',\n  'dataset': 'CommonDataModel',\n  'total_data_processed': {'person': 498,\n   'observation': 585,\n   'condition_occurrence': 1298,\n   'drug_exposure': 604}},\n 'person': {'0x10a8f3c70': {'required_fields': {'gender_concept_id': {'before': 500,\n     'after': 284,\n     'after_formatting': 284},\n    'birth_datetime': {'before': 284, 'after': 284, 'after_formatting': 284}},\n   'source_files': {'birth_datetime': {'table': 'Demographics.csv',\n     'field': 'Age'},\n    'gender_concept_id': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'gender_source_concept_id': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'gender_source_value': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'person_id': {'table': 'Demographics.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Demographics.csv',\n   'name': 'MALE_3025'},\n  '0x10a919bb0': {'required_fields': {'gender_concept_id': {'before': 500,\n     'after': 214,\n     'after_formatting': 214},\n    'birth_datetime': {'before': 214, 'after': 214, 'after_formatting': 214}},\n   'source_files': {'birth_datetime': {'table': 'Demographics.csv',\n     'field': 'Age'},\n    'gender_concept_id': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'gender_source_concept_id': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'gender_source_value': {'table': 'Demographics.csv', 'field': 'Sex'},\n    'person_id': {'table': 'Demographics.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Demographics.csv',\n   'name': 'FEMALE_3026'}},\n 'observation': {'0x10aba8730': {'required_fields': {'person_id': {'before': 204,\n     'after': 204},\n    'observation_concept_id': {'before': 204,\n     'after': 204,\n     'after_formatting': 204},\n    'observation_datetime': {'before': 204,\n     'after': 204,\n     'after_formatting': 204}},\n   'source_files': {'observation_concept_id': {'table': 'Serology.csv',\n     'field': 'IgG'},\n    'observation_datetime': {'table': 'Serology.csv', 'field': 'Date'},\n    'observation_source_concept_id': {'table': 'Serology.csv', 'field': 'IgG'},\n    'observation_source_value': {'table': 'Serology.csv', 'field': 'IgG'},\n    'person_id': {'table': 'Serology.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Serology.csv',\n   'name': 'Antibody_3027',\n   'valid_person_id': {'before': 204, 'after': 204}},\n  '0x10aba8c70': {'required_fields': {'person_id': {'before': 600,\n     'after': 600},\n    'observation_concept_id': {'before': 600,\n     'after': 113,\n     'after_formatting': 113},\n    'observation_datetime': {'before': 113,\n     'after': 113,\n     'after_formatting': 113}},\n   'source_files': {'observation_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_datetime': {'table': 'Hospital_Visit.csv',\n     'field': 'admission_date'},\n    'observation_source_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_source_value': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'person_id': {'table': 'Hospital_Visit.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Hospital_Visit.csv',\n   'name': 'H_O_heart_failure_3043',\n   'valid_person_id': {'before': 113, 'after': 113}},\n  '0x10abb9cd0': {'required_fields': {'person_id': {'before': 600,\n     'after': 600},\n    'observation_concept_id': {'before': 600,\n     'after': 94,\n     'after_formatting': 94},\n    'observation_datetime': {'before': 94,\n     'after': 94,\n     'after_formatting': 94}},\n   'source_files': {'observation_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_datetime': {'table': 'Hospital_Visit.csv',\n     'field': 'admission_date'},\n    'observation_source_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_source_value': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'person_id': {'table': 'Hospital_Visit.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Hospital_Visit.csv',\n   'name': '2019_nCoV_3044',\n   'valid_person_id': {'before': 94, 'after': 94}},\n  '0x10abc4f10': {'required_fields': {'person_id': {'before': 600,\n     'after': 600},\n    'observation_concept_id': {'before': 600,\n     'after': 174,\n     'after_formatting': 174},\n    'observation_datetime': {'before': 174,\n     'after': 174,\n     'after_formatting': 174}},\n   'source_files': {'observation_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_datetime': {'table': 'Hospital_Visit.csv',\n     'field': 'admission_date'},\n    'observation_source_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'observation_source_value': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'person_id': {'table': 'Hospital_Visit.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Hospital_Visit.csv',\n   'name': 'Cancer_3045',\n   'valid_person_id': {'before': 174, 'after': 174}}},\n 'condition_occurrence': {'0x10abc49a0': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 126,\n     'after_formatting': 125},\n    'condition_start_datetime': {'before': 126,\n     'after': 125,\n     'after_formatting': 125}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Headache'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Headache'},\n    'condition_source_value': {'table': 'Symptoms.csv', 'field': 'Headache'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Headache_3028',\n   'valid_person_id': {'before': 125, 'after': 123}},\n  '0x10abc4280': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 113,\n     'after_formatting': 112},\n    'condition_start_datetime': {'before': 113,\n     'after': 112,\n     'after_formatting': 112}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Fatigue'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Fatigue'},\n    'condition_source_value': {'table': 'Symptoms.csv', 'field': 'Fatigue'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Fatigue_3029',\n   'valid_person_id': {'before': 112, 'after': 110}},\n  '0x10ac069d0': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 77,\n     'after_formatting': 77},\n    'condition_start_datetime': {'before': 77,\n     'after': 77,\n     'after_formatting': 77}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Dizzy'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv', 'field': 'Dizzy'},\n    'condition_source_value': {'table': 'Symptoms.csv', 'field': 'Dizzy'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Dizziness_3030',\n   'valid_person_id': {'before': 77, 'after': 77}},\n  '0x10ac0bf10': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 105,\n     'after_formatting': 105},\n    'condition_start_datetime': {'before': 105,\n     'after': 105,\n     'after_formatting': 105}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Cough'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv', 'field': 'Cough'},\n    'condition_source_value': {'table': 'Symptoms.csv', 'field': 'Cough'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Cough_3031',\n   'valid_person_id': {'before': 105, 'after': 103}},\n  '0x10ac05970': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 37,\n     'after_formatting': 37},\n    'condition_start_datetime': {'before': 37,\n     'after': 37,\n     'after_formatting': 37}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Fever'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv', 'field': 'Fever'},\n    'condition_source_value': {'table': 'Symptoms.csv', 'field': 'Fever'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Fever_3032',\n   'valid_person_id': {'before': 37, 'after': 37}},\n  '0x10ac2eb20': {'required_fields': {'person_id': {'before': 165,\n     'after': 165},\n    'condition_concept_id': {'before': 165,\n     'after': 18,\n     'after_formatting': 18},\n    'condition_start_datetime': {'before': 18,\n     'after': 18,\n     'after_formatting': 18}},\n   'source_files': {'condition_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Muscle_Pain'},\n    'condition_end_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'condition_source_concept_id': {'table': 'Symptoms.csv',\n     'field': 'Muscle_Pain'},\n    'condition_source_value': {'table': 'Symptoms.csv',\n     'field': 'Muscle_Pain'},\n    'condition_start_datetime': {'table': 'Symptoms.csv',\n     'field': 'date_occurrence'},\n    'person_id': {'table': 'Symptoms.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Symptoms.csv',\n   'name': 'Muscle_pain_3033',\n   'valid_person_id': {'before': 18, 'after': 18}},\n  '0x10ac24190': {'required_fields': {'person_id': {'before': 600,\n     'after': 600},\n    'condition_concept_id': {'before': 600,\n     'after': 91,\n     'after_formatting': 91},\n    'condition_start_datetime': {'before': 91,\n     'after': 91,\n     'after_formatting': 91}},\n   'source_files': {'condition_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'condition_end_datetime': {'table': 'Hospital_Visit.csv',\n     'field': 'admission_date'},\n    'condition_source_concept_id': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'condition_source_value': {'table': 'Hospital_Visit.csv',\n     'field': 'reason'},\n    'condition_start_datetime': {'table': 'Hospital_Visit.csv',\n     'field': 'admission_date'},\n    'person_id': {'table': 'Hospital_Visit.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Hospital_Visit.csv',\n   'name': 'Pneumonia_3042',\n   'valid_person_id': {'before': 91, 'after': 90}},\n  '0x10abc4820': {'required_fields': {'person_id': {'before': 967,\n     'after': 967},\n    'condition_concept_id': {'before': 967,\n     'after': 225,\n     'after_formatting': 225},\n    'condition_start_datetime': {'before': 225,\n     'after': 225,\n     'after_formatting': 225}},\n   'source_files': {'condition_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_end_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'condition_source_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_source_value': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_start_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'person_id': {'table': 'GP_Records.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/GP_Records.csv',\n   'name': 'Mental_health_problem_3046',\n   'valid_person_id': {'before': 225, 'after': 224}},\n  '0x10ac3fcd0': {'required_fields': {'person_id': {'before': 967,\n     'after': 967},\n    'condition_concept_id': {'before': 967,\n     'after': 225,\n     'after_formatting': 225},\n    'condition_start_datetime': {'before': 225,\n     'after': 225,\n     'after_formatting': 225}},\n   'source_files': {'condition_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_end_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'condition_source_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_source_value': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_start_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'person_id': {'table': 'GP_Records.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/GP_Records.csv',\n   'name': 'Mental_disorder_3047',\n   'valid_person_id': {'before': 225, 'after': 224}},\n  '0x10ac5df40': {'required_fields': {'person_id': {'before': 967,\n     'after': 967},\n    'condition_concept_id': {'before': 967,\n     'after': 116,\n     'after_formatting': 116},\n    'condition_start_datetime': {'before': 116,\n     'after': 116,\n     'after_formatting': 116}},\n   'source_files': {'condition_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_end_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'condition_source_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_source_value': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_start_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'person_id': {'table': 'GP_Records.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/GP_Records.csv',\n   'name': 'Type_2_diabetes_mellitus_3048',\n   'valid_person_id': {'before': 116, 'after': 116}},\n  '0x10ac71e80': {'required_fields': {'person_id': {'before': 967,\n     'after': 967},\n    'condition_concept_id': {'before': 967,\n     'after': 95,\n     'after_formatting': 95},\n    'condition_start_datetime': {'before': 95,\n     'after': 95,\n     'after_formatting': 95}},\n   'source_files': {'condition_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_end_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'condition_source_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_source_value': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_start_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'person_id': {'table': 'GP_Records.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/GP_Records.csv',\n   'name': 'Ischemic_heart_disease_3049',\n   'valid_person_id': {'before': 95, 'after': 95}},\n  '0x10ac88a60': {'required_fields': {'person_id': {'before': 967,\n     'after': 967},\n    'condition_concept_id': {'before': 967,\n     'after': 81,\n     'after_formatting': 81},\n    'condition_start_datetime': {'before': 81,\n     'after': 81,\n     'after_formatting': 81}},\n   'source_files': {'condition_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_end_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'condition_source_concept_id': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_source_value': {'table': 'GP_Records.csv',\n     'field': 'comorbidity'},\n    'condition_start_datetime': {'table': 'GP_Records.csv',\n     'field': 'date_of_visit'},\n    'person_id': {'table': 'GP_Records.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/GP_Records.csv',\n   'name': 'Hypertensive_disorder_3050',\n   'valid_person_id': {'before': 81, 'after': 81}}},\n 'drug_exposure': {'0x10aca01f0': {'required_fields': {'person_id': {'before': 360,\n     'after': 360},\n    'drug_concept_id': {'before': 360, 'after': 129, 'after_formatting': 129},\n    'drug_exposure_start_datetime': {'before': 129,\n     'after': 129,\n     'after_formatting': 129}},\n   'source_files': {'drug_concept_id': {'table': 'Vaccinations.csv',\n     'field': 'type'},\n    'drug_exposure_end_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'drug_source_value': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'person_id': {'table': 'Vaccinations.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Vaccinations.csv',\n   'name': 'COVID_19_vaccine_3034',\n   'valid_person_id': {'before': 129, 'after': 129}},\n  '0x10aca0970': {'required_fields': {'person_id': {'before': 360,\n     'after': 360},\n    'drug_concept_id': {'before': 360, 'after': 114, 'after_formatting': 114},\n    'drug_exposure_start_datetime': {'before': 114,\n     'after': 114,\n     'after_formatting': 114}},\n   'source_files': {'drug_concept_id': {'table': 'Vaccinations.csv',\n     'field': 'type'},\n    'drug_exposure_end_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'drug_source_value': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'person_id': {'table': 'Vaccinations.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Vaccinations.csv',\n   'name': 'COVID_19_vaccine_3035',\n   'valid_person_id': {'before': 114, 'after': 112}},\n  '0x10acccd30': {'required_fields': {'person_id': {'before': 360,\n     'after': 360},\n    'drug_concept_id': {'before': 360, 'after': 117, 'after_formatting': 117},\n    'drug_exposure_start_datetime': {'before': 117,\n     'after': 117,\n     'after_formatting': 117}},\n   'source_files': {'drug_concept_id': {'table': 'Vaccinations.csv',\n     'field': 'type'},\n    'drug_exposure_end_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'drug_source_value': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'person_id': {'table': 'Vaccinations.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Vaccinations.csv',\n   'name': 'COVID_19_vaccine_3036',\n   'valid_person_id': {'before': 117, 'after': 117}},\n  '0x10acd6730': {'required_fields': {'person_id': {'before': 360,\n     'after': 360},\n    'drug_concept_id': {'before': 360, 'after': 129, 'after_formatting': 129},\n    'drug_exposure_start_datetime': {'before': 129,\n     'after': 129,\n     'after_formatting': 129}},\n   'source_files': {'drug_concept_id': {'table': 'Vaccinations.csv',\n     'field': 'type'},\n    'drug_exposure_end_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'drug_source_value': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'person_id': {'table': 'Vaccinations.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Vaccinations.csv',\n   'name': 'SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040',\n   'valid_person_id': {'before': 129, 'after': 129}},\n  '0x10aca0610': {'required_fields': {'person_id': {'before': 360,\n     'after': 360},\n    'drug_concept_id': {'before': 360, 'after': 117, 'after_formatting': 117},\n    'drug_exposure_start_datetime': {'before': 117,\n     'after': 117,\n     'after_formatting': 117}},\n   'source_files': {'drug_concept_id': {'table': 'Vaccinations.csv',\n     'field': 'type'},\n    'drug_exposure_end_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'table': 'Vaccinations.csv',\n     'field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'drug_source_value': {'table': 'Vaccinations.csv', 'field': 'type'},\n    'person_id': {'table': 'Vaccinations.csv', 'field': 'ID'}},\n   'original_file': '../data/part2/Vaccinations.csv',\n   'name': 'SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041',\n   'valid_person_id': {'before': 117, 'after': 117}}}}</code>\n</pre> <pre><code>table = list(cdm.logs.values())[1]\ndata = [ obj['required_fields'] for obj in table.values()]\ndata\n</code></pre> <pre>\n<code>[{'gender_concept_id': {'before': 500, 'after': 284, 'after_formatting': 284},\n  'birth_datetime': {'before': 284, 'after': 284, 'after_formatting': 284}},\n {'gender_concept_id': {'before': 500, 'after': 214, 'after_formatting': 214},\n  'birth_datetime': {'before': 214, 'after': 214, 'after_formatting': 214}}]</code>\n</pre> <pre><code>person_id_map = cdm.person_id_masker\nperson_id_map\n</code></pre> <pre>\n<code>{'pk1002': 1,\n 'pk1003': 2,\n 'pk1004': 3,\n 'pk1006': 4,\n 'pk1009': 5,\n 'pk1010': 6,\n 'pk1011': 7,\n 'pk1012': 8,\n 'pk1013': 9,\n 'pk1014': 10,\n 'pk1016': 11,\n 'pk1021': 12,\n 'pk1022': 13,\n 'pk1024': 14,\n 'pk1025': 15,\n 'pk1026': 16,\n 'pk1027': 17,\n 'pk1028': 18,\n 'pk1029': 19,\n 'pk1031': 20,\n 'pk1032': 21,\n 'pk1033': 22,\n 'pk1035': 23,\n 'pk1036': 24,\n 'pk1038': 25,\n 'pk1042': 26,\n 'pk1044': 27,\n 'pk1048': 28,\n 'pk1049': 29,\n 'pk1050': 30,\n 'pk1051': 31,\n 'pk1052': 32,\n 'pk1053': 33,\n 'pk1055': 34,\n 'pk1056': 35,\n 'pk1057': 36,\n 'pk1059': 37,\n 'pk1060': 38,\n 'pk1064': 39,\n 'pk1069': 40,\n 'pk1071': 41,\n 'pk1072': 42,\n 'pk1073': 43,\n 'pk1074': 44,\n 'pk1075': 45,\n 'pk1077': 46,\n 'pk1085': 47,\n 'pk1086': 48,\n 'pk1088': 49,\n 'pk1090': 50,\n 'pk1091': 51,\n 'pk1092': 52,\n 'pk1094': 53,\n 'pk1095': 54,\n 'pk1096': 55,\n 'pk1098': 56,\n 'pk1099': 57,\n 'pk1100': 58,\n 'pk1102': 59,\n 'pk1103': 60,\n 'pk1104': 61,\n 'pk1110': 62,\n 'pk1112': 63,\n 'pk1115': 64,\n 'pk1117': 65,\n 'pk1118': 66,\n 'pk1119': 67,\n 'pk1120': 68,\n 'pk1122': 69,\n 'pk1123': 70,\n 'pk1125': 71,\n 'pk1127': 72,\n 'pk1128': 73,\n 'pk1131': 74,\n 'pk1132': 75,\n 'pk1138': 76,\n 'pk1140': 77,\n 'pk1142': 78,\n 'pk1145': 79,\n 'pk1147': 80,\n 'pk1148': 81,\n 'pk1149': 82,\n 'pk1150': 83,\n 'pk1156': 84,\n 'pk1157': 85,\n 'pk1158': 86,\n 'pk1160': 87,\n 'pk1161': 88,\n 'pk1167': 89,\n 'pk1170': 90,\n 'pk1172': 91,\n 'pk1175': 92,\n 'pk1176': 93,\n 'pk1177': 94,\n 'pk1180': 95,\n 'pk1181': 96,\n 'pk1185': 97,\n 'pk1187': 98,\n 'pk1188': 99,\n 'pk1192': 100,\n 'pk1194': 101,\n 'pk1195': 102,\n 'pk1196': 103,\n 'pk1198': 104,\n 'pk1200': 105,\n 'pk1201': 106,\n 'pk1202': 107,\n 'pk1205': 108,\n 'pk1207': 109,\n 'pk1208': 110,\n 'pk1210': 111,\n 'pk1211': 112,\n 'pk1216': 113,\n 'pk1221': 114,\n 'pk1222': 115,\n 'pk1223': 116,\n 'pk1226': 117,\n 'pk1227': 118,\n 'pk1229': 119,\n 'pk1230': 120,\n 'pk1231': 121,\n 'pk1232': 122,\n 'pk1233': 123,\n 'pk1235': 124,\n 'pk1236': 125,\n 'pk1239': 126,\n 'pk1240': 127,\n 'pk1241': 128,\n 'pk1242': 129,\n 'pk1243': 130,\n 'pk1244': 131,\n 'pk1245': 132,\n 'pk1246': 133,\n 'pk1247': 134,\n 'pk1249': 135,\n 'pk1251': 136,\n 'pk1253': 137,\n 'pk1257': 138,\n 'pk1258': 139,\n 'pk1259': 140,\n 'pk1260': 141,\n 'pk1262': 142,\n 'pk1264': 143,\n 'pk1265': 144,\n 'pk1266': 145,\n 'pk1268': 146,\n 'pk1271': 147,\n 'pk1273': 148,\n 'pk1274': 149,\n 'pk1275': 150,\n 'pk1277': 151,\n 'pk1279': 152,\n 'pk1283': 153,\n 'pk1284': 154,\n 'pk1285': 155,\n 'pk1286': 156,\n 'pk1287': 157,\n 'pk1288': 158,\n 'pk1290': 159,\n 'pk1293': 160,\n 'pk1294': 161,\n 'pk1297': 162,\n 'pk1299': 163,\n 'pk1301': 164,\n 'pk1302': 165,\n 'pk1305': 166,\n 'pk1308': 167,\n 'pk1309': 168,\n 'pk1310': 169,\n 'pk1312': 170,\n 'pk1313': 171,\n 'pk1315': 172,\n 'pk1318': 173,\n 'pk1319': 174,\n 'pk1320': 175,\n 'pk1324': 176,\n 'pk1325': 177,\n 'pk1328': 178,\n 'pk1329': 179,\n 'pk1331': 180,\n 'pk1332': 181,\n 'pk1337': 182,\n 'pk1338': 183,\n 'pk1339': 184,\n 'pk1340': 185,\n 'pk1341': 186,\n 'pk1343': 187,\n 'pk1347': 188,\n 'pk1350': 189,\n 'pk1351': 190,\n 'pk1352': 191,\n 'pk1353': 192,\n 'pk1357': 193,\n 'pk1359': 194,\n 'pk1360': 195,\n 'pk1362': 196,\n 'pk1364': 197,\n 'pk1366': 198,\n 'pk1367': 199,\n 'pk1369': 200,\n 'pk1370': 201,\n 'pk1374': 202,\n 'pk1375': 203,\n 'pk1376': 204,\n 'pk1378': 205,\n 'pk1379': 206,\n 'pk1380': 207,\n 'pk1381': 208,\n 'pk1382': 209,\n 'pk1384': 210,\n 'pk1385': 211,\n 'pk1386': 212,\n 'pk1387': 213,\n 'pk1388': 214,\n 'pk1390': 215,\n 'pk1391': 216,\n 'pk1392': 217,\n 'pk1393': 218,\n 'pk1396': 219,\n 'pk1397': 220,\n 'pk1398': 221,\n 'pk1399': 222,\n 'pk1400': 223,\n 'pk1402': 224,\n 'pk1403': 225,\n 'pk1404': 226,\n 'pk1406': 227,\n 'pk1407': 228,\n 'pk1409': 229,\n 'pk1411': 230,\n 'pk1412': 231,\n 'pk1413': 232,\n 'pk1414': 233,\n 'pk1415': 234,\n 'pk1417': 235,\n 'pk1418': 236,\n 'pk1419': 237,\n 'pk1420': 238,\n 'pk1425': 239,\n 'pk1428': 240,\n 'pk1430': 241,\n 'pk1433': 242,\n 'pk1435': 243,\n 'pk1437': 244,\n 'pk1438': 245,\n 'pk1440': 246,\n 'pk1441': 247,\n 'pk1443': 248,\n 'pk1444': 249,\n 'pk1445': 250,\n 'pk1447': 251,\n 'pk1448': 252,\n 'pk1449': 253,\n 'pk1450': 254,\n 'pk1451': 255,\n 'pk1453': 256,\n 'pk1454': 257,\n 'pk1455': 258,\n 'pk1456': 259,\n 'pk1458': 260,\n 'pk1459': 261,\n 'pk1464': 262,\n 'pk1466': 263,\n 'pk1469': 264,\n 'pk1471': 265,\n 'pk1472': 266,\n 'pk1473': 267,\n 'pk1474': 268,\n 'pk1475': 269,\n 'pk1476': 270,\n 'pk1477': 271,\n 'pk1479': 272,\n 'pk1481': 273,\n 'pk1484': 274,\n 'pk1485': 275,\n 'pk1486': 276,\n 'pk1488': 277,\n 'pk1489': 278,\n 'pk1490': 279,\n 'pk1491': 280,\n 'pk1492': 281,\n 'pk1494': 282,\n 'pk1496': 283,\n 'pk1497': 284,\n 'pk1001': 285,\n 'pk1005': 286,\n 'pk1007': 287,\n 'pk1008': 288,\n 'pk1015': 289,\n 'pk1017': 290,\n 'pk1018': 291,\n 'pk1019': 292,\n 'pk1020': 293,\n 'pk1030': 294,\n 'pk1034': 295,\n 'pk1037': 296,\n 'pk1039': 297,\n 'pk1040': 298,\n 'pk1041': 299,\n 'pk1043': 300,\n 'pk1045': 301,\n 'pk1046': 302,\n 'pk1047': 303,\n 'pk1054': 304,\n 'pk1058': 305,\n 'pk1061': 306,\n 'pk1062': 307,\n 'pk1063': 308,\n 'pk1065': 309,\n 'pk1066': 310,\n 'pk1067': 311,\n 'pk1068': 312,\n 'pk1070': 313,\n 'pk1076': 314,\n 'pk1078': 315,\n 'pk1079': 316,\n 'pk1080': 317,\n 'pk1081': 318,\n 'pk1082': 319,\n 'pk1083': 320,\n 'pk1084': 321,\n 'pk1087': 322,\n 'pk1089': 323,\n 'pk1093': 324,\n 'pk1097': 325,\n 'pk1101': 326,\n 'pk1105': 327,\n 'pk1106': 328,\n 'pk1107': 329,\n 'pk1108': 330,\n 'pk1109': 331,\n 'pk1111': 332,\n 'pk1113': 333,\n 'pk1114': 334,\n 'pk1116': 335,\n 'pk1121': 336,\n 'pk1124': 337,\n 'pk1126': 338,\n 'pk1129': 339,\n 'pk1130': 340,\n 'pk1133': 341,\n 'pk1134': 342,\n 'pk1135': 343,\n 'pk1136': 344,\n 'pk1137': 345,\n 'pk1139': 346,\n 'pk1141': 347,\n 'pk1143': 348,\n 'pk1144': 349,\n 'pk1146': 350,\n 'pk1151': 351,\n 'pk1152': 352,\n 'pk1153': 353,\n 'pk1154': 354,\n 'pk1155': 355,\n 'pk1159': 356,\n 'pk1162': 357,\n 'pk1163': 358,\n 'pk1164': 359,\n 'pk1165': 360,\n 'pk1166': 361,\n 'pk1168': 362,\n 'pk1169': 363,\n 'pk1171': 364,\n 'pk1173': 365,\n 'pk1174': 366,\n 'pk1178': 367,\n 'pk1179': 368,\n 'pk1182': 369,\n 'pk1183': 370,\n 'pk1184': 371,\n 'pk1186': 372,\n 'pk1189': 373,\n 'pk1190': 374,\n 'pk1191': 375,\n 'pk1193': 376,\n 'pk1199': 377,\n 'pk1203': 378,\n 'pk1204': 379,\n 'pk1206': 380,\n 'pk1209': 381,\n 'pk1212': 382,\n 'pk1213': 383,\n 'pk1214': 384,\n 'pk1215': 385,\n 'pk1217': 386,\n 'pk1218': 387,\n 'pk1219': 388,\n 'pk1220': 389,\n 'pk1224': 390,\n 'pk1225': 391,\n 'pk1228': 392,\n 'pk1234': 393,\n 'pk1237': 394,\n 'pk1238': 395,\n 'pk1248': 396,\n 'pk1250': 397,\n 'pk1252': 398,\n 'pk1254': 399,\n 'pk1255': 400,\n 'pk1256': 401,\n 'pk1261': 402,\n 'pk1263': 403,\n 'pk1267': 404,\n 'pk1269': 405,\n 'pk1270': 406,\n 'pk1272': 407,\n 'pk1276': 408,\n 'pk1278': 409,\n 'pk1280': 410,\n 'pk1281': 411,\n 'pk1282': 412,\n 'pk1289': 413,\n 'pk1291': 414,\n 'pk1292': 415,\n 'pk1295': 416,\n 'pk1296': 417,\n 'pk1298': 418,\n 'pk1300': 419,\n 'pk1303': 420,\n 'pk1304': 421,\n 'pk1306': 422,\n 'pk1307': 423,\n 'pk1311': 424,\n 'pk1314': 425,\n 'pk1316': 426,\n 'pk1317': 427,\n 'pk1321': 428,\n 'pk1322': 429,\n 'pk1323': 430,\n 'pk1326': 431,\n 'pk1327': 432,\n 'pk1330': 433,\n 'pk1333': 434,\n 'pk1334': 435,\n 'pk1335': 436,\n 'pk1336': 437,\n 'pk1342': 438,\n 'pk1344': 439,\n 'pk1345': 440,\n 'pk1346': 441,\n 'pk1348': 442,\n 'pk1349': 443,\n 'pk1354': 444,\n 'pk1355': 445,\n 'pk1356': 446,\n 'pk1358': 447,\n 'pk1361': 448,\n 'pk1363': 449,\n 'pk1365': 450,\n 'pk1368': 451,\n 'pk1371': 452,\n 'pk1372': 453,\n 'pk1373': 454,\n 'pk1377': 455,\n 'pk1383': 456,\n 'pk1389': 457,\n 'pk1394': 458,\n 'pk1395': 459,\n 'pk1401': 460,\n 'pk1405': 461,\n 'pk1408': 462,\n 'pk1410': 463,\n 'pk1416': 464,\n 'pk1421': 465,\n 'pk1422': 466,\n 'pk1423': 467,\n 'pk1424': 468,\n 'pk1426': 469,\n 'pk1427': 470,\n 'pk1429': 471,\n 'pk1431': 472,\n 'pk1432': 473,\n 'pk1434': 474,\n 'pk1436': 475,\n 'pk1439': 476,\n 'pk1442': 477,\n 'pk1446': 478,\n 'pk1452': 479,\n 'pk1457': 480,\n 'pk1460': 481,\n 'pk1461': 482,\n 'pk1462': 483,\n 'pk1463': 484,\n 'pk1465': 485,\n 'pk1467': 486,\n 'pk1468': 487,\n 'pk1470': 488,\n 'pk1478': 489,\n 'pk1480': 490,\n 'pk1482': 491,\n 'pk1483': 492,\n 'pk1487': 493,\n 'pk1493': 494,\n 'pk1495': 495,\n 'pk1498': 496,\n 'pk1499': 497,\n 'pk1500': 498}</code>\n</pre> <pre><code>cdm.reset()\n</code></pre> <pre>\n<code>2022-06-17 14:49:22 - LocalDataCollection - INFO - resetting used bricks\n</code>\n</pre> <p>Change the default index start value for observation</p> <pre><code>cdm.set_indexing_map({'observation':10000000000})\n</code></pre> <pre><code>cdm.process()\n</code></pre> <pre>\n<code>2022-06-17 14:49:22 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:49:22 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:49:22 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:49:22 - CommonDataModel - INFO - working on person\n2022-06-17 14:49:22 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:49:22 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:22 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (person) has not be passed, so starting from 1\n2022-06-17 14:49:22 - Person - INFO - Called apply_rules\n2022-06-17 14:49:22 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:49:22 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:49:22 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:49:22 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:49:22 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:49:22 - Person - INFO - Mapped person_id\n2022-06-17 14:49:22 - Person - WARNING - Requiring non-null values in gender_concept_id removed 216 rows, leaving 284 rows.\n2022-06-17 14:49:22 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:49:22 - Person - INFO - created df (0x10ac2ed60)[MALE_3025]\n2022-06-17 14:49:22 - CommonDataModel - INFO - finished MALE 3025 (0x10ac2ed60) ... 1/2 completed, 284 rows\n2022-06-17 14:49:22 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:22 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (person) has not be passed, so starting from 1\n2022-06-17 14:49:22 - LocalDataCollection - INFO - saving person_ids to ./test_outputs/local//person_ids.csv\n2022-06-17 14:49:22 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:22 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:49:22 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:22 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (person) has not be passed, so starting from 1\n2022-06-17 14:49:22 - Person - INFO - Called apply_rules\n2022-06-17 14:49:23 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:49:23 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:49:23 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:49:23 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:49:23 - Person - INFO - Mapped person_id\n2022-06-17 14:49:23 - Person - WARNING - Requiring non-null values in gender_concept_id removed 286 rows, leaving 214 rows.\n2022-06-17 14:49:23 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:49:23 - Person - INFO - created df (0x10ac244f0)[FEMALE_3026]\n2022-06-17 14:49:23 - CommonDataModel - INFO - finished FEMALE 3026 (0x10ac244f0) ... 2/2 completed, 214 rows\n2022-06-17 14:49:23 - LocalDataCollection - INFO - updating person_ids in ./test_outputs/local//person_ids.csv\n2022-06-17 14:49:23 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:23 - CommonDataModel - INFO - saving dataframe (0x10ac24eb0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:23 - LocalDataCollection - INFO - saving person to ./test_outputs/local//person.csv\n2022-06-17 14:49:23 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:23 - CommonDataModel - INFO - finalised person on iteration 0 producing 498 rows from 2 tables\n2022-06-17 14:49:23 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:23 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:23 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:23 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 14:49:23 - CommonDataModel - INFO - working on observation\n2022-06-17 14:49:23 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 14:49:23 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:23 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:23 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:23 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:23 - Observation - INFO - created df (0x10ac71880)[Antibody_3027]\n2022-06-17 14:49:23 - CommonDataModel - INFO - finished Antibody 3027 (0x10ac71880) ... 1/4 completed, 204 rows\n2022-06-17 14:49:23 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 14:49:23 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:23 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:23 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:23 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 487 rows, leaving 113 rows.\n2022-06-17 14:49:23 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:23 - Observation - INFO - created df (0x10abb9820)[H_O_heart_failure_3043]\n2022-06-17 14:49:23 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x10abb9820) ... 2/4 completed, 113 rows\n2022-06-17 14:49:23 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:23 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:23 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:23 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:23 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 506 rows, leaving 94 rows.\n2022-06-17 14:49:23 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:23 - Observation - INFO - created df (0x10aca08e0)[2019_nCoV_3044]\n2022-06-17 14:49:23 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x10aca08e0) ... 3/4 completed, 94 rows\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 14:49:24 - Observation - INFO - Called apply_rules\n2022-06-17 14:49:24 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:49:24 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:49:24 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:49:24 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:49:24 - Observation - INFO - Mapped person_id\n2022-06-17 14:49:24 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 426 rows, leaving 174 rows.\n2022-06-17 14:49:24 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:49:24 - Observation - INFO - created df (0x10aba89a0)[Cancer_3045]\n2022-06-17 14:49:24 - CommonDataModel - INFO - finished Cancer 3045 (0x10aba89a0) ... 4/4 completed, 174 rows\n2022-06-17 14:49:24 - CommonDataModel - INFO - saving dataframe (0x10a8f3670) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:24 - LocalDataCollection - INFO - saving observation to ./test_outputs/local//observation.csv\n2022-06-17 14:49:24 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:24 - CommonDataModel - INFO - finalised observation on iteration 0 producing 585 rows from 4 tables\n2022-06-17 14:49:24 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:24 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:24 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:24 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 14:49:24 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 14:49:24 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:24 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:24 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 39 rows, leaving 126 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 125 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - created df (0x10ac2e490)[Headache_3028]\n2022-06-17 14:49:24 - CommonDataModel - INFO - finished Headache 3028 (0x10ac2e490) ... 1/12 completed, 125 rows\n2022-06-17 14:49:24 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:24 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:24 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:24 - CommonDataModel - ERROR - 123/125 were good, 2 studies are removed.\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 14:49:24 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:24 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 52 rows, leaving 113 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 112 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - created df (0x10a88fee0)[Fatigue_3029]\n2022-06-17 14:49:24 - CommonDataModel - INFO - finished Fatigue 3029 (0x10a88fee0) ... 2/12 completed, 112 rows\n2022-06-17 14:49:24 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:24 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:24 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:24 - CommonDataModel - ERROR - 110/112 were good, 2 studies are removed.\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 14:49:24 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:24 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 88 rows, leaving 77 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - created df (0x10ac246d0)[Dizziness_3030]\n2022-06-17 14:49:24 - CommonDataModel - INFO - finished Dizziness 3030 (0x10ac246d0) ... 3/12 completed, 77 rows\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 14:49:24 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:24 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 60 rows, leaving 105 rows.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - created df (0x10abaa3d0)[Cough_3031]\n2022-06-17 14:49:24 - CommonDataModel - INFO - finished Cough 3031 (0x10abaa3d0) ... 4/12 completed, 105 rows\n2022-06-17 14:49:24 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:24 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:24 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:24 - CommonDataModel - ERROR - 103/105 were good, 2 studies are removed.\n2022-06-17 14:49:24 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 14:49:24 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:24 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:24 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:24 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 128 rows, leaving 37 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10a88f700)[Fever_3032]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Fever 3032 (0x10a88f700) ... 5/12 completed, 37 rows\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 147 rows, leaving 18 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10abc4be0)[Muscle_pain_3033]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Muscle pain 3033 (0x10abc4be0) ... 6/12 completed, 18 rows\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 509 rows, leaving 91 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10ac06fd0)[Pneumonia_3042]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Pneumonia 3042 (0x10ac06fd0) ... 7/12 completed, 91 rows\n2022-06-17 14:49:25 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:25 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:25 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:25 - CommonDataModel - ERROR - 90/91 were good, 1 studies are removed.\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 742 rows, leaving 225 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10a88f9d0)[Mental_health_problem_3046]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Mental health problem 3046 (0x10a88f9d0) ... 8/12 completed, 225 rows\n2022-06-17 14:49:25 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:25 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:25 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:25 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 742 rows, leaving 225 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10abf67f0)[Mental_disorder_3047]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Mental disorder 3047 (0x10abf67f0) ... 9/12 completed, 225 rows\n2022-06-17 14:49:25 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:25 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:25 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:25 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 851 rows, leaving 116 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10aba3910)[Type_2_diabetes_mellitus_3048]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x10aba3910) ... 10/12 completed, 116 rows\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 872 rows, leaving 95 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10abc49d0)[Ischemic_heart_disease_3049]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x10abc49d0) ... 11/12 completed, 95 rows\n2022-06-17 14:49:25 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 14:49:25 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:25 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (condition_occurrence) has not be passed, so starting from 1\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:49:25 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 886 rows, leaving 81 rows.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:49:25 - ConditionOccurrence - INFO - created df (0x10a919040)[Hypertensive_disorder_3050]\n2022-06-17 14:49:25 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x10a919040) ... 12/12 completed, 81 rows\n2022-06-17 14:49:26 - CommonDataModel - INFO - saving dataframe (0x10acbb3d0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:26 - LocalDataCollection - INFO - saving condition_occurrence to ./test_outputs/local//condition_occurrence.csv\n2022-06-17 14:49:26 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:26 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 1298 rows from 12 tables\n2022-06-17 14:49:26 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:26 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:49:26 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:49:26 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 14:49:26 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 14:49:26 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 14:49:26 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:26 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (drug_exposure) has not be passed, so starting from 1\n2022-06-17 14:49:26 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:26 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:26 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 231 rows, leaving 129 rows.\n2022-06-17 14:49:26 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:26 - DrugExposure - INFO - created df (0x10ace3550)[COVID_19_vaccine_3034]\n2022-06-17 14:49:26 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x10ace3550) ... 1/5 completed, 129 rows\n2022-06-17 14:49:26 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 14:49:26 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:26 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (drug_exposure) has not be passed, so starting from 1\n2022-06-17 14:49:26 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_value\n</code>\n</pre> <pre>\n<code>2022-06-17 14:49:26 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:26 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 246 rows, leaving 114 rows.\n2022-06-17 14:49:26 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:26 - DrugExposure - INFO - created df (0x10ace3eb0)[COVID_19_vaccine_3035]\n2022-06-17 14:49:26 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x10ace3eb0) ... 2/5 completed, 114 rows\n2022-06-17 14:49:26 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:49:26 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:49:26 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:49:26 - CommonDataModel - ERROR - 112/114 were good, 2 studies are removed.\n2022-06-17 14:49:26 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 14:49:26 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:26 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (drug_exposure) has not be passed, so starting from 1\n2022-06-17 14:49:26 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:26 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 243 rows, leaving 117 rows.\n2022-06-17 14:49:26 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:26 - DrugExposure - INFO - created df (0x10accca00)[COVID_19_vaccine_3036]\n2022-06-17 14:49:26 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x10accca00) ... 3/5 completed, 117 rows\n2022-06-17 14:49:26 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 14:49:26 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:26 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (drug_exposure) has not be passed, so starting from 1\n2022-06-17 14:49:26 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:26 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 231 rows, leaving 129 rows.\n2022-06-17 14:49:26 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:26 - DrugExposure - INFO - created df (0x10acd67c0)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 14:49:26 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x10acd67c0) ... 4/5 completed, 129 rows\n2022-06-17 14:49:26 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 14:49:26 - CommonDataModel - WARNING - {'observation': 10000000000}\n2022-06-17 14:49:26 - CommonDataModel - WARNING - indexing configuration has be parsed but this table (drug_exposure) has not be passed, so starting from 1\n2022-06-17 14:49:26 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:49:26 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:49:26 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 243 rows, leaving 117 rows.\n2022-06-17 14:49:26 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:49:26 - DrugExposure - INFO - created df (0x10ad37bb0)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 14:49:26 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x10ad37bb0) ... 5/5 completed, 117 rows\n2022-06-17 14:49:27 - CommonDataModel - INFO - saving dataframe (0x10a8a6130) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x10a847a00&gt;\n2022-06-17 14:49:27 - LocalDataCollection - INFO - saving drug_exposure to ./test_outputs/local//drug_exposure.csv\n2022-06-17 14:49:27 - LocalDataCollection - INFO - finished save to file\n2022-06-17 14:49:27 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 604 rows from 5 tables\n2022-06-17 14:49:27 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:49:27 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 10000000585 458 4288455 2020-11-20 2020-11-20 00:00:00.000000 61.597638175407624 4288455 10000000586 330 4288455 2022-09-12 2022-09-12 00:00:00.000000 6.413970727863393 4288455 10000000587 136 4288455 2019-02-02 2019-02-02 00:00:00.000000 4.691778998076871 4288455 10000000588 17 4288455 2020-08-18 2020-08-18 00:00:00.000000 14.77684965289811 4288455 10000000589 55 4288455 2019-11-23 2019-11-23 00:00:00.000000 79.61828150487227 4288455 ... ... ... ... ... ... ... 10000001165 279 40757663 2019-11-10 2019-11-10 00:00:00.000000 Cancer 40757663 10000001166 494 40757663 2020-08-28 2020-08-28 00:00:00.000000 Cancer 40757663 10000001167 282 40757663 2020-02-03 2020-02-03 00:00:00.000000 Cancer 40757663 10000001168 284 40757663 2020-11-21 2020-11-21 00:00:00.000000 Cancer 40757663 10000001169 284 40757663 2019-09-06 2019-09-06 00:00:00.000000 Cancer 40757663 <p>585 rows \u00d7 6 columns</p>"},{"location":"CaRROT-CDM/notebooks/Part%204%20-%20Local/#reset-and-change","title":"Reset and change","text":"<p>Reset the CDM - remove created dataframes and removing any indexing</p>"},{"location":"CaRROT-CDM/notebooks/Part%205%20-%20SQL/","title":"Part 5   SQL","text":"<p>Alternatively it's possible to connect directly to an SQL database</p> <pre><code>import carrot\nimport glob\n</code></pre> <pre><code>inputs = carrot.tools.create_sql_store(connection_string=\"postgresql://localhost:5432/ExampleCOVID19DataSet\")\n</code></pre> <pre>\n<code>2022-06-17 15:04:25 - SqlDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:04:25 - SqlDataCollection - INFO - Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:04:26 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x10c04e790&gt;]\n2022-06-17 15:04:26 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10c098550&gt;]\n2022-06-17 15:04:26 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10c098a60&gt;]\n2022-06-17 15:04:26 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10c0b1d30&gt;]\n2022-06-17 15:04:27 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c0badf0&gt;]\n2022-06-17 15:04:27 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x10c0bad90&gt;]\n2022-06-17 15:04:27 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c0d6310&gt;]\n</code>\n</pre> <pre><code>inputs.keys()\n</code></pre> <pre>\n<code>dict_keys(['Demographics', 'GP_Records', 'Vaccinations', 'Serology', 'Symptoms', 'Hospital_Visit', 'Blood_Test'])</code>\n</pre> <pre><code>inputs['Symptoms']\n</code></pre> <pre>\n<code>2022-06-17 15:04:27 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Symptoms' for the first time\n</code>\n</pre> ID date_occurrence Headache Fatigue Dizzy Cough Fever Muscle_Pain 0 pk1 2021-01-24 Yes Yes No Yes No Yes 1 pk1 2019-05-30 Yes Yes No No Yes No 2 pk1 2021-05-16 Yes No No No Yes No 3 pk1 2022-06-11 Yes Yes Yes Yes No No 4 pk1 2020-06-18 Yes Yes Yes Yes Yes Yes ... ... ... ... ... ... ... ... ... 56668 pk9992 2021-11-22 No No No No No Yes 56669 pk9992 2019-11-07 Yes Yes No Yes Yes Yes 56670 pk9992 2018-09-01 Yes Yes No No Yes Yes 56671 pk9993 2018-12-02 No No No No Yes No 56672 pk9996 2018-08-11 Yes Yes No Yes No Yes <p>56673 rows \u00d7 8 columns</p> <pre><code>rules = carrot.tools.load_json(\"../data/rules_sql.json\")\nrules\n</code></pre> <pre>\n<code>{'metadata': {'date_created': '2022-02-11T12:22:48.465257',\n  'dataset': 'FAILED: ExampleV4'},\n 'cdm': {'person': {'MALE 3025': {'birth_datetime': {'source_table': 'Demographics',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_concept_id': {'source_table': 'Demographics',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_value': {'source_table': 'Demographics',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics', 'source_field': 'ID'}},\n   'FEMALE 3026': {'birth_datetime': {'source_table': 'Demographics',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_concept_id': {'source_table': 'Demographics',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_value': {'source_table': 'Demographics',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics', 'source_field': 'ID'}}},\n  'observation': {'Antibody 3027': {'observation_concept_id': {'source_table': 'Serology',\n     'source_field': 'IgG',\n     'term_mapping': 4288455},\n    'observation_datetime': {'source_table': 'Serology',\n     'source_field': 'Date'},\n    'observation_source_concept_id': {'source_table': 'Serology',\n     'source_field': 'IgG',\n     'term_mapping': 4288455},\n    'observation_source_value': {'source_table': 'Serology',\n     'source_field': 'IgG'},\n    'person_id': {'source_table': 'Serology', 'source_field': 'ID'}},\n   'H/O: heart failure 3043': {'observation_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Heart Attack': 4059317}},\n    'observation_datetime': {'source_table': 'Hospital_Visit',\n     'source_field': 'admission_date'},\n    'observation_source_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Heart Attack': 4059317}},\n    'observation_source_value': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason'},\n    'person_id': {'source_table': 'Hospital_Visit', 'source_field': 'ID'}},\n   '2019-nCoV 3044': {'observation_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'COVID-19': 37311065}},\n    'observation_datetime': {'source_table': 'Hospital_Visit',\n     'source_field': 'admission_date'},\n    'observation_source_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'COVID-19': 37311065}},\n    'observation_source_value': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason'},\n    'person_id': {'source_table': 'Hospital_Visit', 'source_field': 'ID'}},\n   'Cancer 3045': {'observation_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Cancer': 40757663}},\n    'observation_datetime': {'source_table': 'Hospital_Visit',\n     'source_field': 'admission_date'},\n    'observation_source_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Cancer': 40757663}},\n    'observation_source_value': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason'},\n    'person_id': {'source_table': 'Hospital_Visit', 'source_field': 'ID'}}},\n  'condition_occurrence': {'Headache 3028': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Headache',\n     'term_mapping': {'Yes': 378253}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Headache',\n     'term_mapping': {'Yes': 378253}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Headache'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Fatigue 3029': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Fatigue',\n     'term_mapping': {'Yes': 4223659}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Fatigue',\n     'term_mapping': {'Yes': 4223659}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Fatigue'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Dizziness 3030': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Dizzy',\n     'term_mapping': {'Yes': 4223938}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Dizzy',\n     'term_mapping': {'Yes': 4223938}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Dizzy'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Cough 3031': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Cough',\n     'term_mapping': {'Yes': 254761}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Cough',\n     'term_mapping': {'Yes': 254761}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Cough'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Fever 3032': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Fever',\n     'term_mapping': {'Yes': 437663}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Fever',\n     'term_mapping': {'Yes': 437663}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Fever'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Muscle pain 3033': {'condition_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Muscle_Pain',\n     'term_mapping': {'Yes': 442752}},\n    'condition_end_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'condition_source_concept_id': {'source_table': 'Symptoms',\n     'source_field': 'Muscle_Pain',\n     'term_mapping': {'Yes': 442752}},\n    'condition_source_value': {'source_table': 'Symptoms',\n     'source_field': 'Muscle_Pain'},\n    'condition_start_datetime': {'source_table': 'Symptoms',\n     'source_field': 'date_occurrence'},\n    'person_id': {'source_table': 'Symptoms', 'source_field': 'ID'}},\n   'Pneumonia 3042': {'condition_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Pneumonia': 255848}},\n    'condition_end_datetime': {'source_table': 'Hospital_Visit',\n     'source_field': 'admission_date'},\n    'condition_source_concept_id': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason',\n     'term_mapping': {'Pneumonia': 255848}},\n    'condition_source_value': {'source_table': 'Hospital_Visit',\n     'source_field': 'reason'},\n    'condition_start_datetime': {'source_table': 'Hospital_Visit',\n     'source_field': 'admission_date'},\n    'person_id': {'source_table': 'Hospital_Visit', 'source_field': 'ID'}},\n   'Mental health problem 3046': {'condition_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Mental Health': 4131548}},\n    'condition_end_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'condition_source_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Mental Health': 4131548}},\n    'condition_source_value': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity'},\n    'condition_start_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'person_id': {'source_table': 'GP_Records', 'source_field': 'ID'}},\n   'Mental disorder 3047': {'condition_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Mental Health': 432586}},\n    'condition_end_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'condition_source_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Mental Health': 432586}},\n    'condition_source_value': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity'},\n    'condition_start_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'person_id': {'source_table': 'GP_Records', 'source_field': 'ID'}},\n   'Type 2 diabetes mellitus 3048': {'condition_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Diabetes Type-II': 201826}},\n    'condition_end_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'condition_source_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Diabetes Type-II': 201826}},\n    'condition_source_value': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity'},\n    'condition_start_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'person_id': {'source_table': 'GP_Records', 'source_field': 'ID'}},\n   'Ischemic heart disease 3049': {'condition_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Heart Condition': 4185932}},\n    'condition_end_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'condition_source_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'Heart Condition': 4185932}},\n    'condition_source_value': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity'},\n    'condition_start_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'person_id': {'source_table': 'GP_Records', 'source_field': 'ID'}},\n   'Hypertensive disorder 3050': {'condition_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'High Blood Pressure': 316866}},\n    'condition_end_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'condition_source_concept_id': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity',\n     'term_mapping': {'High Blood Pressure': 316866}},\n    'condition_source_value': {'source_table': 'GP_Records',\n     'source_field': 'comorbidity'},\n    'condition_start_datetime': {'source_table': 'GP_Records',\n     'source_field': 'date_of_visit'},\n    'person_id': {'source_table': 'GP_Records', 'source_field': 'ID'}}},\n  'drug_exposure': {'COVID-19 vaccine 3034': {'drug_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Moderna': 35894915}},\n    'drug_exposure_end_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Moderna': 35894915}},\n    'drug_source_value': {'source_table': 'Vaccinations',\n     'source_field': 'type'},\n    'person_id': {'source_table': 'Vaccinations', 'source_field': 'ID'}},\n   'COVID-19 vaccine 3035': {'drug_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'AstraZenica': 35894915}},\n    'drug_exposure_end_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'AstraZenica': 35894915}},\n    'drug_source_value': {'source_table': 'Vaccinations',\n     'source_field': 'type'},\n    'person_id': {'source_table': 'Vaccinations', 'source_field': 'ID'}},\n   'COVID-19 vaccine 3036': {'drug_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Pfizer': 35894915}},\n    'drug_exposure_end_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Pfizer': 35894915}},\n    'drug_source_value': {'source_table': 'Vaccinations',\n     'source_field': 'type'},\n    'person_id': {'source_table': 'Vaccinations', 'source_field': 'ID'}},\n   'SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040': {'drug_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Moderna': 37003518}},\n    'drug_exposure_end_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Moderna': 37003518}},\n    'drug_source_value': {'source_table': 'Vaccinations',\n     'source_field': 'type'},\n    'person_id': {'source_table': 'Vaccinations', 'source_field': 'ID'}},\n   'SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041': {'drug_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Pfizer': 37003436}},\n    'drug_exposure_end_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_exposure_start_datetime': {'source_table': 'Vaccinations',\n     'source_field': 'date_of_vaccination'},\n    'drug_source_concept_id': {'source_table': 'Vaccinations',\n     'source_field': 'type',\n     'term_mapping': {'Pfizer': 37003436}},\n    'drug_source_value': {'source_table': 'Vaccinations',\n     'source_field': 'type'},\n    'person_id': {'source_table': 'Vaccinations', 'source_field': 'ID'}}}}}</code>\n</pre> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=inputs)\ncdm.process()\n</code></pre> <pre>\n<code>2022-06-17 15:04:28 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:04:28 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 15:04:28 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 15:04:28 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 15:04:28 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 15:04:28 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 15:04:28 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 15:04:28 - CommonDataModel - INFO - working on person\n2022-06-17 15:04:28 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 15:04:28 - Person - INFO - Called apply_rules\n2022-06-17 15:04:28 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Demographics' for the first time\n2022-06-17 15:04:28 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:04:28 - Person - INFO - Mapped person_id\n2022-06-17 15:04:28 - Person - WARNING - Requiring non-null values in gender_concept_id removed 210 rows, leaving 138 rows.\n2022-06-17 15:04:28 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:04:28 - Person - INFO - created df (0x10c5ac310)[MALE_3025]\n2022-06-17 15:04:28 - CommonDataModel - INFO - finished MALE 3025 (0x10c5ac310) ... 1/2 completed, 138 rows\n2022-06-17 15:04:28 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 15:04:28 - Person - INFO - Called apply_rules\n2022-06-17 15:04:28 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:04:28 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:04:28 - Person - INFO - Mapped person_id\n2022-06-17 15:04:28 - Person - WARNING - Requiring non-null values in gender_concept_id removed 215 rows, leaving 133 rows.\n2022-06-17 15:04:28 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:04:28 - Person - INFO - created df (0x10c5973a0)[FEMALE_3026]\n2022-06-17 15:04:28 - CommonDataModel - INFO - finished FEMALE 3026 (0x10c5973a0) ... 2/2 completed, 133 rows\n2022-06-17 15:04:28 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:04:28 - CommonDataModel - INFO - finalised person on iteration 0 producing 271 rows from 2 tables\n2022-06-17 15:04:28 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:04:28 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Demographics'\n2022-06-17 15:04:28 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:28 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Symptoms'\n2022-06-17 15:04:29 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:29 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:04:29 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x10c04ef10&gt;]\n2022-06-17 15:04:29 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10c04e790&gt;]\n2022-06-17 15:04:29 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10c597430&gt;]\n2022-06-17 15:04:29 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10c597e80&gt;]\n2022-06-17 15:04:30 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c098910&gt;]\n2022-06-17 15:04:30 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x10c581760&gt;]\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:30 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c0cd130&gt;]\n2022-06-17 15:04:30 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 15:04:30 - CommonDataModel - INFO - working on observation\n2022-06-17 15:04:30 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 15:04:30 - Observation - INFO - Called apply_rules\n2022-06-17 15:04:30 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Serology' for the first time\n2022-06-17 15:04:30 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:04:30 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:04:30 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:04:30 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:04:30 - Observation - INFO - Mapped person_id\n2022-06-17 15:04:30 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:04:31 - Observation - INFO - created df (0x10c4da040)[Antibody_3027]\n2022-06-17 15:04:31 - CommonDataModel - INFO - finished Antibody 3027 (0x10c4da040) ... 1/4 completed, 20591 rows\n2022-06-17 15:04:31 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:31 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:31 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:31 - CommonDataModel - ERROR - 105/20591 were good, 20486 studies are removed.\n2022-06-17 15:04:31 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 15:04:31 - Observation - INFO - Called apply_rules\n2022-06-17 15:04:31 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit' for the first time\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:04:31 - Observation - INFO - Mapped person_id\n2022-06-17 15:04:31 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7199 rows, leaving 1511 rows.\n2022-06-17 15:04:31 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:04:31 - Observation - INFO - created df (0x10c6973d0)[H_O_heart_failure_3043]\n2022-06-17 15:04:31 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x10c6973d0) ... 2/4 completed, 1511 rows\n2022-06-17 15:04:31 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:31 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:31 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:31 - CommonDataModel - ERROR - 11/1511 were good, 1500 studies are removed.\n2022-06-17 15:04:31 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 15:04:31 - Observation - INFO - Called apply_rules\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:04:31 - Observation - INFO - Mapped person_id\n2022-06-17 15:04:31 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7256 rows, leaving 1454 rows.\n2022-06-17 15:04:31 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:04:31 - Observation - INFO - created df (0x10c697700)[2019_nCoV_3044]\n2022-06-17 15:04:31 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x10c697700) ... 3/4 completed, 1454 rows\n2022-06-17 15:04:31 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:31 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:31 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:31 - CommonDataModel - ERROR - 12/1454 were good, 1442 studies are removed.\n2022-06-17 15:04:31 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 15:04:31 - Observation - INFO - Called apply_rules\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:04:31 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:04:31 - Observation - INFO - Mapped person_id\n2022-06-17 15:04:31 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7097 rows, leaving 1613 rows.\n2022-06-17 15:04:31 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:04:32 - Observation - INFO - created df (0x10c7ad430)[Cancer_3045]\n2022-06-17 15:04:32 - CommonDataModel - INFO - finished Cancer 3045 (0x10c7ad430) ... 4/4 completed, 1613 rows\n2022-06-17 15:04:32 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:32 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:32 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:32 - CommonDataModel - ERROR - 9/1613 were good, 1604 studies are removed.\n2022-06-17 15:04:32 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:04:32 - CommonDataModel - INFO - finalised observation on iteration 0 producing 137 rows from 4 tables\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:32 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:04:32 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Serology'\n2022-06-17 15:04:32 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:32 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Hospital_Visit'\n2022-06-17 15:04:32 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:32 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:04:32 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x10c45a430&gt;]\n2022-06-17 15:04:32 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10c4a4490&gt;]\n2022-06-17 15:04:32 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10c04e130&gt;]\n2022-06-17 15:04:33 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10c4a47f0&gt;]\n2022-06-17 15:04:33 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c7ad8b0&gt;]\n2022-06-17 15:04:33 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x10c16d880&gt;]\n2022-06-17 15:04:33 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c16d8e0&gt;]\n2022-06-17 15:04:33 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 15:04:33 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 15:04:33 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 15:04:33 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:33 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Symptoms' for the first time\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:34 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 20289 rows, leaving 36384 rows.\n2022-06-17 15:04:34 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 30 rows, leaving 36354 rows.\n2022-06-17 15:04:34 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - created df (0x10c16d0d0)[Headache_3028]\n2022-06-17 15:04:35 - CommonDataModel - INFO - finished Headache 3028 (0x10c16d0d0) ... 1/12 completed, 36354 rows\n2022-06-17 15:04:35 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:35 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:35 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:35 - CommonDataModel - ERROR - 224/36354 were good, 36130 studies are removed.\n2022-06-17 15:04:35 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:35 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:36 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 23601 rows, leaving 33072 rows.\n2022-06-17 15:04:36 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 28 rows, leaving 33044 rows.\n2022-06-17 15:04:36 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:37 - ConditionOccurrence - INFO - created df (0x10e31feb0)[Fatigue_3029]\n2022-06-17 15:04:38 - CommonDataModel - INFO - finished Fatigue 3029 (0x10e31feb0) ... 2/12 completed, 33044 rows\n2022-06-17 15:04:38 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:38 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:38 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:38 - CommonDataModel - ERROR - 201/33044 were good, 32843 studies are removed.\n2022-06-17 15:04:38 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 30706 rows, leaving 25967 rows.\n2022-06-17 15:04:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 25 rows, leaving 25942 rows.\n2022-06-17 15:04:38 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - created df (0x10e075c10)[Dizziness_3030]\n2022-06-17 15:04:39 - CommonDataModel - INFO - finished Dizziness 3030 (0x10e075c10) ... 3/12 completed, 25942 rows\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:39 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:39 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:39 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:39 - CommonDataModel - ERROR - 149/25942 were good, 25793 studies are removed.\n2022-06-17 15:04:39 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:39 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 23571 rows, leaving 33102 rows.\n2022-06-17 15:04:39 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 26 rows, leaving 33076 rows.\n2022-06-17 15:04:39 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - created df (0x10d06dc40)[Cough_3031]\n2022-06-17 15:04:41 - CommonDataModel - INFO - finished Cough 3031 (0x10d06dc40) ... 4/12 completed, 33076 rows\n2022-06-17 15:04:41 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:41 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:41 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:41 - CommonDataModel - ERROR - 208/33076 were good, 32868 studies are removed.\n2022-06-17 15:04:41 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:41 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:42 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 36272 rows, leaving 20401 rows.\n2022-06-17 15:04:42 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 22 rows, leaving 20379 rows.\n2022-06-17 15:04:42 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:42 - ConditionOccurrence - INFO - created df (0x10c6bc790)[Fever_3032]\n2022-06-17 15:04:43 - CommonDataModel - INFO - finished Fever 3032 (0x10c6bc790) ... 5/12 completed, 20379 rows\n2022-06-17 15:04:43 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:43 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:43 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:43 - CommonDataModel - ERROR - 118/20379 were good, 20261 studies are removed.\n2022-06-17 15:04:43 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:43 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 41813 rows, leaving 14860 rows.\n2022-06-17 15:04:43 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 21 rows, leaving 14839 rows.\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:43 - ConditionOccurrence - INFO - created df (0x10e0e9eb0)[Muscle_pain_3033]\n2022-06-17 15:04:44 - CommonDataModel - INFO - finished Muscle pain 3033 (0x10e0e9eb0) ... 6/12 completed, 14839 rows\n2022-06-17 15:04:44 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:44 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:44 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:44 - CommonDataModel - ERROR - 84/14839 were good, 14755 studies are removed.\n2022-06-17 15:04:44 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:44 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit' for the first time\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:44 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 7267 rows, leaving 1443 rows.\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - created df (0x10cfb5a90)[Pneumonia_3042]\n2022-06-17 15:04:44 - CommonDataModel - INFO - finished Pneumonia 3042 (0x10cfb5a90) ... 7/12 completed, 1443 rows\n2022-06-17 15:04:44 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:44 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:44 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:44 - CommonDataModel - ERROR - 9/1443 were good, 1434 studies are removed.\n2022-06-17 15:04:44 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:44 - SqlDataCollection - INFO - Retrieving initial dataframe for 'GP_Records' for the first time\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:44 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53424 rows, leaving 3421 rows.\n2022-06-17 15:04:44 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - created df (0x1231b9a00)[Mental_health_problem_3046]\n2022-06-17 15:04:45 - CommonDataModel - INFO - finished Mental health problem 3046 (0x1231b9a00) ... 8/12 completed, 3421 rows\n2022-06-17 15:04:45 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:45 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:45 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:45 - CommonDataModel - ERROR - 23/3421 were good, 3398 studies are removed.\n2022-06-17 15:04:45 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53424 rows, leaving 3421 rows.\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - created df (0x125c0a7c0)[Mental_disorder_3047]\n2022-06-17 15:04:45 - CommonDataModel - INFO - finished Mental disorder 3047 (0x125c0a7c0) ... 9/12 completed, 3421 rows\n2022-06-17 15:04:45 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:45 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:45 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:45 - CommonDataModel - ERROR - 23/3421 were good, 3398 studies are removed.\n2022-06-17 15:04:45 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:45 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53808 rows, leaving 3037 rows.\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - created df (0x1253ceb80)[Type_2_diabetes_mellitus_3048]\n2022-06-17 15:04:45 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x1253ceb80) ... 10/12 completed, 3037 rows\n2022-06-17 15:04:45 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:45 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:45 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:45 - CommonDataModel - ERROR - 28/3037 were good, 3009 studies are removed.\n2022-06-17 15:04:45 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_source_value\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:45 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53941 rows, leaving 2904 rows.\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - created df (0x125c0a700)[Ischemic_heart_disease_3049]\n2022-06-17 15:04:46 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x125c0a700) ... 11/12 completed, 2904 rows\n2022-06-17 15:04:46 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:46 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:46 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:46 - CommonDataModel - ERROR - 22/2904 were good, 2882 studies are removed.\n2022-06-17 15:04:46 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:04:46 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 54362 rows, leaving 2483 rows.\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:04:46 - ConditionOccurrence - INFO - created df (0x125c07ca0)[Hypertensive_disorder_3050]\n2022-06-17 15:04:46 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x125c07ca0) ... 12/12 completed, 2483 rows\n2022-06-17 15:04:46 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:46 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:46 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:46 - CommonDataModel - ERROR - 16/2483 were good, 2467 studies are removed.\n2022-06-17 15:04:46 - CommonDataModel - ERROR - Removed 2 row(s) due to duplicates found when merging condition_occurrence\n2022-06-17 15:04:46 - CommonDataModel - WARNING - Example duplicates...\n2022-06-17 15:04:46 - CommonDataModel - WARNING -                          person_id  condition_concept_id condition_start_date  \\\ncondition_occurrence_id                                                         \n7813                          44.0               4223659           2020-09-13   \n7815                          44.0               4223659           2020-09-13   \n3628                          44.0                437663           2020-09-13   \n3630                          44.0                437663           2020-09-13   \n\n                           condition_start_datetime condition_end_date  \\\ncondition_occurrence_id                                                  \n7813                     2020-09-13 00:00:00.000000         2020-09-13   \n7815                     2020-09-13 00:00:00.000000         2020-09-13   \n3628                     2020-09-13 00:00:00.000000         2020-09-13   \n3630                     2020-09-13 00:00:00.000000         2020-09-13   \n\n                             condition_end_datetime condition_source_value  \\\ncondition_occurrence_id                                                      \n7813                     2020-09-13 00:00:00.000000                    Yes   \n7815                     2020-09-13 00:00:00.000000                    Yes   \n3628                     2020-09-13 00:00:00.000000                    Yes   \n3630                     2020-09-13 00:00:00.000000                    Yes   \n\n                         condition_source_concept_id  \ncondition_occurrence_id                               \n7813                                         4223659  \n7815                                         4223659  \n3628                                          437663  \n3630                                          437663  \n2022-06-17 15:04:46 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:04:46 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 1105 rows from 12 tables\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'GP_Records'\n2022-06-17 15:04:46 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Symptoms'\n2022-06-17 15:04:46 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Hospital_Visit'\n2022-06-17 15:04:46 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:46 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x10c098760&gt;]\n2022-06-17 15:04:46 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10c49eeb0&gt;]\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10c5ac580&gt;]\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10c444640&gt;]\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c6c4040&gt;]\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x123e91670&gt;]\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c7adcd0&gt;]\n2022-06-17 15:04:47 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 15:04:47 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 15:04:47 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:47 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:04:47 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations' for the first time\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:04:48 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:04:48 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 53291 rows, leaving 27645 rows.\n2022-06-17 15:04:48 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 27643 rows.\n2022-06-17 15:04:48 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:04:49 - DrugExposure - INFO - created df (0x125c159d0)[COVID_19_vaccine_3034]\n2022-06-17 15:04:49 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x125c159d0) ... 1/5 completed, 27643 rows\n2022-06-17 15:04:49 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:49 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:49 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:49 - CommonDataModel - ERROR - 155/27643 were good, 27488 studies are removed.\n2022-06-17 15:04:49 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 15:04:49 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:04:49 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:04:49 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54293 rows, leaving 26643 rows.\n2022-06-17 15:04:49 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26641 rows.\n2022-06-17 15:04:49 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:04:50 - DrugExposure - INFO - created df (0x125c3fd00)[COVID_19_vaccine_3035]\n2022-06-17 15:04:51 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x125c3fd00) ... 2/5 completed, 26641 rows\n2022-06-17 15:04:51 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:51 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:51 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:51 - CommonDataModel - ERROR - 144/26641 were good, 26497 studies are removed.\n2022-06-17 15:04:51 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 15:04:51 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:04:51 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:04:51 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54288 rows, leaving 26648 rows.\n2022-06-17 15:04:51 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26646 rows.\n2022-06-17 15:04:51 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:04:52 - DrugExposure - INFO - created df (0x1246dcb80)[COVID_19_vaccine_3036]\n2022-06-17 15:04:52 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x1246dcb80) ... 3/5 completed, 26646 rows\n2022-06-17 15:04:52 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:52 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:52 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:52 - CommonDataModel - ERROR - 150/26646 were good, 26496 studies are removed.\n2022-06-17 15:04:52 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 15:04:52 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:04:52 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:04:52 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 53291 rows, leaving 27645 rows.\n2022-06-17 15:04:52 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 27643 rows.\n2022-06-17 15:04:52 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:04:53 - DrugExposure - INFO - created df (0x10d4298e0)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 15:04:54 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x10d4298e0) ... 4/5 completed, 27643 rows\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:54 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:54 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:54 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:54 - CommonDataModel - ERROR - 155/27643 were good, 27488 studies are removed.\n2022-06-17 15:04:54 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 15:04:54 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:04:54 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:04:54 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54288 rows, leaving 26648 rows.\n2022-06-17 15:04:54 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26646 rows.\n2022-06-17 15:04:54 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:04:55 - DrugExposure - INFO - created df (0x125276c40)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 15:04:55 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x125276c40) ... 5/5 completed, 26646 rows\n2022-06-17 15:04:55 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:04:55 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:04:55 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:04:55 - CommonDataModel - ERROR - 150/26646 were good, 26496 studies are removed.\n2022-06-17 15:04:55 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:04:55 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 754 rows from 5 tables\n2022-06-17 15:04:55 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:04:55 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Vaccinations'\n2022-06-17 15:04:55 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:55 - SqlDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>outputs = carrot.tools.create_sql_store(connection_string=\"postgresql://localhost:5432/ExampleCDMDataSet\",\n                                          drop_existing=True)\n</code></pre> <pre>\n<code>2022-06-17 15:04:55 - SqlDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:04:57 - SqlDataCollection - INFO - Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n</code>\n</pre> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=inputs,outputs=outputs)\ncdm.process()\n</code></pre> <pre>\n<code>2022-06-17 15:04:57 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:04:57 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 15:04:57 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 15:04:57 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 15:04:57 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 15:04:57 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 15:04:57 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 15:04:57 - CommonDataModel - INFO - working on person\n2022-06-17 15:04:57 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 15:04:57 - Person - INFO - Called apply_rules\n2022-06-17 15:04:57 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Demographics' for the first time\n2022-06-17 15:04:57 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:04:57 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:04:57 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:04:57 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:04:57 - Person - INFO - Mapped person_id\n2022-06-17 15:04:57 - Person - WARNING - Requiring non-null values in gender_concept_id removed 210 rows, leaving 138 rows.\n2022-06-17 15:04:57 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:04:57 - Person - INFO - created df (0x10e5962e0)[MALE_3025]\n2022-06-17 15:04:57 - CommonDataModel - INFO - finished MALE 3025 (0x10e5962e0) ... 1/2 completed, 138 rows\n2022-06-17 15:04:57 - SqlDataCollection - INFO - updating person_ids in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:04:57 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:57 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 15:04:58 - Person - INFO - Called apply_rules\n2022-06-17 15:04:58 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:04:58 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:04:58 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:04:58 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:04:58 - Person - INFO - Mapped person_id\n2022-06-17 15:04:58 - Person - WARNING - Requiring non-null values in gender_concept_id removed 215 rows, leaving 133 rows.\n2022-06-17 15:04:58 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:04:58 - Person - INFO - created df (0x123943490)[FEMALE_3026]\n2022-06-17 15:04:58 - CommonDataModel - INFO - finished FEMALE 3026 (0x123943490) ... 2/2 completed, 133 rows\n2022-06-17 15:04:58 - SqlDataCollection - INFO - updating person_ids in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:04:58 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:58 - CommonDataModel - INFO - saving dataframe (0x123943370) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x10d429760&gt;\n2022-06-17 15:04:58 - SqlDataCollection - INFO - updating person in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:04:58 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:04:58 - CommonDataModel - INFO - finalised person on iteration 0 producing 271 rows from 2 tables\n2022-06-17 15:04:58 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:04:58 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Demographics'\n2022-06-17 15:04:58 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:04:58 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:04:58 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x1239436d0&gt;]\n2022-06-17 15:04:58 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x1238615e0&gt;]\n</code>\n</pre> <pre>\n<code>2022-06-17 15:04:58 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10c0cd910&gt;]\n2022-06-17 15:04:58 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10e0e9e80&gt;]\n2022-06-17 15:04:59 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c6c4df0&gt;]\n2022-06-17 15:04:59 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x10c6c4040&gt;]\n2022-06-17 15:04:59 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c098970&gt;]\n2022-06-17 15:04:59 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 15:04:59 - CommonDataModel - INFO - working on observation\n2022-06-17 15:04:59 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 15:04:59 - Observation - INFO - Called apply_rules\n2022-06-17 15:04:59 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Serology' for the first time\n2022-06-17 15:04:59 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:04:59 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:04:59 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:04:59 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:04:59 - Observation - INFO - Mapped person_id\n2022-06-17 15:04:59 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:05:00 - Observation - INFO - created df (0x10d1669a0)[Antibody_3027]\n2022-06-17 15:05:00 - CommonDataModel - INFO - finished Antibody 3027 (0x10d1669a0) ... 1/4 completed, 20591 rows\n2022-06-17 15:05:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:00 - CommonDataModel - ERROR - 105/20591 were good, 20486 studies are removed.\n2022-06-17 15:05:00 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 15:05:00 - Observation - INFO - Called apply_rules\n2022-06-17 15:05:00 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit' for the first time\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:05:00 - Observation - INFO - Mapped person_id\n2022-06-17 15:05:00 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7199 rows, leaving 1511 rows.\n2022-06-17 15:05:00 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:05:00 - Observation - INFO - created df (0x1252761c0)[H_O_heart_failure_3043]\n2022-06-17 15:05:00 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x1252761c0) ... 2/4 completed, 1511 rows\n2022-06-17 15:05:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:00 - CommonDataModel - ERROR - 11/1511 were good, 1500 studies are removed.\n2022-06-17 15:05:00 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 15:05:00 - Observation - INFO - Called apply_rules\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:05:00 - Observation - INFO - Mapped person_id\n2022-06-17 15:05:00 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7256 rows, leaving 1454 rows.\n2022-06-17 15:05:00 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:05:00 - Observation - INFO - created df (0x10d429640)[2019_nCoV_3044]\n2022-06-17 15:05:00 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x10d429640) ... 3/4 completed, 1454 rows\n2022-06-17 15:05:00 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:00 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:00 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:00 - CommonDataModel - ERROR - 12/1454 were good, 1442 studies are removed.\n2022-06-17 15:05:00 - CommonDataModel - INFO - starting on Cancer 3045\n2022-06-17 15:05:00 - Observation - INFO - Called apply_rules\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:05:00 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:05:00 - Observation - INFO - Mapped person_id\n2022-06-17 15:05:00 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 7097 rows, leaving 1613 rows.\n2022-06-17 15:05:00 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:05:01 - Observation - INFO - created df (0x10d429dc0)[Cancer_3045]\n2022-06-17 15:05:01 - CommonDataModel - INFO - finished Cancer 3045 (0x10d429dc0) ... 4/4 completed, 1613 rows\n2022-06-17 15:05:01 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:01 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:01 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:01 - CommonDataModel - ERROR - 9/1613 were good, 1604 studies are removed.\n2022-06-17 15:05:01 - CommonDataModel - INFO - saving dataframe (0x1242a9670) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x10d429760&gt;\n2022-06-17 15:05:01 - SqlDataCollection - INFO - updating observation in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:05:01 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:05:01 - CommonDataModel - INFO - finalised observation on iteration 0 producing 137 rows from 4 tables\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Serology'\n2022-06-17 15:05:01 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Hospital_Visit'\n2022-06-17 15:05:01 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:01 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x10dc730a0&gt;]\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10c831790&gt;]\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x1253ce0a0&gt;]\n2022-06-17 15:05:01 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x10c4943d0&gt;]\n2022-06-17 15:05:02 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c494bb0&gt;]\n2022-06-17 15:05:02 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x125c07fd0&gt;]\n2022-06-17 15:05:02 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c8315e0&gt;]\n2022-06-17 15:05:02 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 15:05:02 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 15:05:02 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:02 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Symptoms' for the first time\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:02 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 20289 rows, leaving 36384 rows.\n2022-06-17 15:05:02 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 30 rows, leaving 36354 rows.\n2022-06-17 15:05:02 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:03 - ConditionOccurrence - INFO - created df (0x1253ce4f0)[Headache_3028]\n2022-06-17 15:05:04 - CommonDataModel - INFO - finished Headache 3028 (0x1253ce4f0) ... 1/12 completed, 36354 rows\n2022-06-17 15:05:04 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:04 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:04 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:04 - CommonDataModel - ERROR - 224/36354 were good, 36130 studies are removed.\n2022-06-17 15:05:04 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 15:05:04 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:05 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 23601 rows, leaving 33072 rows.\n2022-06-17 15:05:05 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 28 rows, leaving 33044 rows.\n2022-06-17 15:05:05 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:06 - ConditionOccurrence - INFO - created df (0x10e0e97c0)[Fatigue_3029]\n2022-06-17 15:05:07 - CommonDataModel - INFO - finished Fatigue 3029 (0x10e0e97c0) ... 2/12 completed, 33044 rows\n2022-06-17 15:05:07 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:07 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:07 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:07 - CommonDataModel - ERROR - 201/33044 were good, 32843 studies are removed.\n2022-06-17 15:05:07 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:07 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:07 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 30706 rows, leaving 25967 rows.\n2022-06-17 15:05:07 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 25 rows, leaving 25942 rows.\n2022-06-17 15:05:07 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:08 - ConditionOccurrence - INFO - created df (0x123e91280)[Dizziness_3030]\n2022-06-17 15:05:09 - CommonDataModel - INFO - finished Dizziness 3030 (0x123e91280) ... 3/12 completed, 25942 rows\n2022-06-17 15:05:09 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:09 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:09 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:09 - CommonDataModel - ERROR - 149/25942 were good, 25793 studies are removed.\n2022-06-17 15:05:09 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:09 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 23571 rows, leaving 33102 rows.\n2022-06-17 15:05:09 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 26 rows, leaving 33076 rows.\n2022-06-17 15:05:09 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:10 - ConditionOccurrence - INFO - created df (0x125c0a0a0)[Cough_3031]\n2022-06-17 15:05:11 - CommonDataModel - INFO - finished Cough 3031 (0x125c0a0a0) ... 4/12 completed, 33076 rows\n2022-06-17 15:05:11 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:11 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:11 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:11 - CommonDataModel - ERROR - 208/33076 were good, 32868 studies are removed.\n2022-06-17 15:05:11 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:11 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 36272 rows, leaving 20401 rows.\n2022-06-17 15:05:11 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 22 rows, leaving 20379 rows.\n2022-06-17 15:05:11 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:12 - ConditionOccurrence - INFO - created df (0x10e0e92b0)[Fever_3032]\n2022-06-17 15:05:13 - CommonDataModel - INFO - finished Fever 3032 (0x10e0e92b0) ... 5/12 completed, 20379 rows\n2022-06-17 15:05:13 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:13 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:13 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:13 - CommonDataModel - ERROR - 118/20379 were good, 20261 studies are removed.\n2022-06-17 15:05:13 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:13 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 41813 rows, leaving 14860 rows.\n2022-06-17 15:05:13 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 21 rows, leaving 14839 rows.\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:13 - ConditionOccurrence - INFO - created df (0x123e91940)[Muscle_pain_3033]\n2022-06-17 15:05:14 - CommonDataModel - INFO - finished Muscle pain 3033 (0x123e91940) ... 6/12 completed, 14839 rows\n2022-06-17 15:05:14 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:14 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:14 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:14 - CommonDataModel - ERROR - 84/14839 were good, 14755 studies are removed.\n2022-06-17 15:05:14 - CommonDataModel - INFO - starting on Pneumonia 3042\n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:14 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:14 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit' for the first time\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:14 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 7267 rows, leaving 1443 rows.\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - created df (0x10c6bc910)[Pneumonia_3042]\n2022-06-17 15:05:14 - CommonDataModel - INFO - finished Pneumonia 3042 (0x10c6bc910) ... 7/12 completed, 1443 rows\n2022-06-17 15:05:14 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:14 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:14 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:14 - CommonDataModel - ERROR - 9/1443 were good, 1434 studies are removed.\n2022-06-17 15:05:14 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 15:05:14 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:14 - SqlDataCollection - INFO - Retrieving initial dataframe for 'GP_Records' for the first time\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:15 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53424 rows, leaving 3421 rows.\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - created df (0x125bf2dc0)[Mental_health_problem_3046]\n2022-06-17 15:05:15 - CommonDataModel - INFO - finished Mental health problem 3046 (0x125bf2dc0) ... 8/12 completed, 3421 rows\n2022-06-17 15:05:15 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:15 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:15 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:15 - CommonDataModel - ERROR - 23/3421 were good, 3398 studies are removed.\n2022-06-17 15:05:15 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:15 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:16 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53424 rows, leaving 3421 rows.\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - created df (0x10c415700)[Mental_disorder_3047]\n2022-06-17 15:05:16 - CommonDataModel - INFO - finished Mental disorder 3047 (0x10c415700) ... 9/12 completed, 3421 rows\n2022-06-17 15:05:16 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:16 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:16 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:16 - CommonDataModel - ERROR - 23/3421 were good, 3398 studies are removed.\n2022-06-17 15:05:16 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:16 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53808 rows, leaving 3037 rows.\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:16 - ConditionOccurrence - INFO - created df (0x10c4dabb0)[Type_2_diabetes_mellitus_3048]\n2022-06-17 15:05:17 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x10c4dabb0) ... 10/12 completed, 3037 rows\n2022-06-17 15:05:17 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:17 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:17 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:17 - CommonDataModel - ERROR - 28/3037 were good, 3009 studies are removed.\n2022-06-17 15:05:17 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:17 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 53941 rows, leaving 2904 rows.\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - created df (0x10c0dd8e0)[Ischemic_heart_disease_3049]\n2022-06-17 15:05:17 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x10c0dd8e0) ... 11/12 completed, 2904 rows\n2022-06-17 15:05:17 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:17 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:17 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:17 - CommonDataModel - ERROR - 22/2904 were good, 2882 studies are removed.\n2022-06-17 15:05:17 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 15:05:17 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 54362 rows, leaving 2483 rows.\n2022-06-17 15:05:17 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:05:18 - ConditionOccurrence - INFO - created df (0x10c04e190)[Hypertensive_disorder_3050]\n2022-06-17 15:05:18 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x10c04e190) ... 12/12 completed, 2483 rows\n2022-06-17 15:05:18 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:18 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:18 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:18 - CommonDataModel - ERROR - 16/2483 were good, 2467 studies are removed.\n2022-06-17 15:05:18 - CommonDataModel - ERROR - Removed 2 row(s) due to duplicates found when merging condition_occurrence\n2022-06-17 15:05:18 - CommonDataModel - WARNING - Example duplicates...\n2022-06-17 15:05:18 - CommonDataModel - WARNING -                          person_id  condition_concept_id condition_start_date  \\\ncondition_occurrence_id                                                         \n7813                          44.0               4223659           2020-09-13   \n7815                          44.0               4223659           2020-09-13   \n3628                          44.0                437663           2020-09-13   \n3630                          44.0                437663           2020-09-13   \n\n                           condition_start_datetime condition_end_date  \\\ncondition_occurrence_id                                                  \n7813                     2020-09-13 00:00:00.000000         2020-09-13   \n7815                     2020-09-13 00:00:00.000000         2020-09-13   \n3628                     2020-09-13 00:00:00.000000         2020-09-13   \n3630                     2020-09-13 00:00:00.000000         2020-09-13   \n\n                             condition_end_datetime condition_source_value  \\\ncondition_occurrence_id                                                      \n7813                     2020-09-13 00:00:00.000000                    Yes   \n7815                     2020-09-13 00:00:00.000000                    Yes   \n3628                     2020-09-13 00:00:00.000000                    Yes   \n3630                     2020-09-13 00:00:00.000000                    Yes   \n\n                         condition_source_concept_id  \ncondition_occurrence_id                               \n7813                                         4223659  \n7815                                         4223659  \n3628                                          437663  \n3630                                          437663  \n2022-06-17 15:05:18 - CommonDataModel - INFO - saving dataframe (0x10c494970) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x10d429760&gt;\n2022-06-17 15:05:18 - SqlDataCollection - INFO - updating condition_occurrence in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:05:18 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:05:18 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 1105 rows from 12 tables\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'GP_Records'\n2022-06-17 15:05:18 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Symptoms'\n2022-06-17 15:05:18 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Hospital_Visit'\n2022-06-17 15:05:18 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:18 - SqlDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Registering  Demographics [&lt;carrot.io.common.DataBrick object at 0x1242a9640&gt;]\n2022-06-17 15:05:18 - SqlDataCollection - INFO - Registering  GP_Records [&lt;carrot.io.common.DataBrick object at 0x10d06dca0&gt;]\n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:19 - SqlDataCollection - INFO - Registering  Vaccinations [&lt;carrot.io.common.DataBrick object at 0x10dc73fd0&gt;]\n2022-06-17 15:05:19 - SqlDataCollection - INFO - Registering  Serology [&lt;carrot.io.common.DataBrick object at 0x1253ce3a0&gt;]\n2022-06-17 15:05:19 - SqlDataCollection - INFO - Registering  Symptoms [&lt;carrot.io.common.DataBrick object at 0x10c4943d0&gt;]\n2022-06-17 15:05:19 - SqlDataCollection - INFO - Registering  Hospital_Visit [&lt;carrot.io.common.DataBrick object at 0x10e075250&gt;]\n2022-06-17 15:05:19 - SqlDataCollection - INFO - Registering  Blood_Test [&lt;carrot.io.common.DataBrick object at 0x10c469760&gt;]\n2022-06-17 15:05:19 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 15:05:19 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 15:05:19 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 15:05:19 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:05:19 - SqlDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations' for the first time\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:05:20 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:05:20 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 53291 rows, leaving 27645 rows.\n2022-06-17 15:05:20 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 27643 rows.\n2022-06-17 15:05:20 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:05:21 - DrugExposure - INFO - created df (0x10e028e20)[COVID_19_vaccine_3034]\n2022-06-17 15:05:23 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x10e028e20) ... 1/5 completed, 27643 rows\n2022-06-17 15:05:23 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:23 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:23 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:23 - CommonDataModel - ERROR - 155/27643 were good, 27488 studies are removed.\n2022-06-17 15:05:23 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 15:05:23 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:05:23 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:05:23 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54293 rows, leaving 26643 rows.\n2022-06-17 15:05:23 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26641 rows.\n2022-06-17 15:05:23 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:05:25 - DrugExposure - INFO - created df (0x10c597a60)[COVID_19_vaccine_3035]\n2022-06-17 15:05:25 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x10c597a60) ... 2/5 completed, 26641 rows\n2022-06-17 15:05:25 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:25 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:25 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:25 - CommonDataModel - ERROR - 144/26641 were good, 26497 studies are removed.\n2022-06-17 15:05:25 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 15:05:25 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:05:25 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:05:25 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54288 rows, leaving 26648 rows.\n2022-06-17 15:05:25 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26646 rows.\n2022-06-17 15:05:25 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:05:26 - DrugExposure - INFO - created df (0x1246d0760)[COVID_19_vaccine_3036]\n2022-06-17 15:05:27 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x1246d0760) ... 3/5 completed, 26646 rows\n2022-06-17 15:05:27 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:27 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:27 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:27 - CommonDataModel - ERROR - 150/26646 were good, 26496 studies are removed.\n2022-06-17 15:05:27 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 15:05:27 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:05:27 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:05:27 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:05:27 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:05:27 - DrugExposure - INFO - Mapped drug_source_concept_id\n</code>\n</pre> <pre>\n<code>2022-06-17 15:05:27 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:05:27 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:05:27 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 53291 rows, leaving 27645 rows.\n2022-06-17 15:05:27 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 27643 rows.\n2022-06-17 15:05:27 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:05:28 - DrugExposure - INFO - created df (0x10c5971c0)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 15:05:28 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x10c5971c0) ... 4/5 completed, 27643 rows\n2022-06-17 15:05:28 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:28 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:28 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:28 - CommonDataModel - ERROR - 155/27643 were good, 27488 studies are removed.\n2022-06-17 15:05:28 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 15:05:28 - DrugExposure - INFO - Called apply_rules\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 15:05:28 - DrugExposure - INFO - Mapped person_id\n2022-06-17 15:05:29 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 54288 rows, leaving 26648 rows.\n2022-06-17 15:05:29 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 2 rows, leaving 26646 rows.\n2022-06-17 15:05:29 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:05:29 - DrugExposure - INFO - created df (0x123e896d0)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 15:05:30 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x123e896d0) ... 5/5 completed, 26646 rows\n2022-06-17 15:05:30 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:05:30 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:05:30 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:05:30 - CommonDataModel - ERROR - 150/26646 were good, 26496 studies are removed.\n2022-06-17 15:05:30 - CommonDataModel - INFO - saving dataframe (0x122e8d2b0) to &lt;carrot.io.plugins.sql.SqlDataCollection object at 0x10d429760&gt;\n2022-06-17 15:05:30 - SqlDataCollection - INFO - updating drug_exposure in Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:05:31 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:05:31 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 754 rows from 5 tables\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Getting the next chunk of size 'None' for 'Vaccinations'\nException during reset or similar\nTraceback (most recent call last):\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 739, in _finalize_fairy\n    fairy._reset(pool)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/sqlalchemy/pool/base.py\", line 988, in _reset\n    pool._dialect.do_rollback(self)\n  File \"/Users/calummacdonald/.pyenv/versions/3.8.0/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 682, in do_rollback\n    dbapi_connection.rollback()\npsycopg2.errors.AdminShutdown: terminating connection due to administrator command\nserver closed the connection unexpectedly\n    This probably means the server terminated abnormally\n    before or while processing the request.\n\n2022-06-17 15:05:31 - SqlDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 15:05:31 - SqlDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>inputs_sql = carrot.tools.create_sql_store(connection_string=\"postgresql://localhost:5432/ExampleCDMDataSet\")\n</code></pre> <pre>\n<code>2022-06-17 15:05:31 - SqlDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Engine(postgresql://localhost:5432/ExampleCDMDataSet)\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Registering  person_ids [&lt;carrot.io.common.DataBrick object at 0x10c597af0&gt;]\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Registering  person [&lt;carrot.io.common.DataBrick object at 0x10c9c2f70&gt;]\n2022-06-17 15:05:31 - SqlDataCollection - INFO - Registering  observation [&lt;carrot.io.common.DataBrick object at 0x10c49e9d0&gt;]\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Registering  condition_occurrence [&lt;carrot.io.common.DataBrick object at 0x10c7ad070&gt;]\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Registering  drug_exposure [&lt;carrot.io.common.DataBrick object at 0x1256f8d30&gt;]\n</code>\n</pre> <pre><code>cdm = carrot.cdm.CommonDataModel.load(inputs=inputs_sql,\n                                         do_mask_person_id=False,\n                                         format_level=0)\n</code></pre> <pre>\n<code>2022-06-17 15:05:32 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:05:32 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 15:05:32 - CommonDataModel - WARNING - Not loading person_ids, this is not a valid CDM Table\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Retrieving initial dataframe for 'person' for the first time\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Retrieving initial dataframe for 'observation' for the first time\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Retrieving initial dataframe for 'condition_occurrence' for the first time\n2022-06-17 15:05:32 - SqlDataCollection - INFO - Retrieving initial dataframe for 'drug_exposure' for the first time\n</code>\n</pre> <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 1 8507 1958 7 17 1958-07-17 00:00:00.000000 Male 8507 2 8507 1983 7 11 1983-07-11 00:00:00.000000 Male 8507 3 8507 1965 7 15 1965-07-15 00:00:00.000000 Male 8507 4 8507 1925 7 25 1925-07-25 00:00:00.000000 Male 8507 5 8507 1971 7 14 1971-07-14 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 267 8532 2019 7 2 2019-07-02 00:00:00.000000 Female 8532 268 8532 1883 8 4 1883-08-04 00:00:00.000000 Female 8532 269 8532 1892 8 1 1892-08-01 00:00:00.000000 Female 8532 270 8532 1899 7 31 1899-07-31 00:00:00.000000 Female 8532 271 8532 2018 7 2 2018-07-02 00:00:00.000000 Female 8532 <p>271 rows \u00d7 7 columns</p>"},{"location":"CaRROT-CDM/notebooks/Part%205%20-%20SQL/#write-outputs-to-sql","title":"Write outputs to SQL","text":""},{"location":"CaRROT-CDM/notebooks/Part%205%20-%20SQL/#load-cdm-from-sql","title":"Load CDM from SQL","text":""},{"location":"CaRROT-CDM/notebooks/Part%206%20-%20BCLink/","title":"Part 6   BCLink","text":"<pre><code>import carrot\nimport glob\n</code></pre> <pre><code>inputs =  carrot.tools.load_csv(glob.glob('../data/part1/*'),nrows=1000)\ninputs\n</code></pre> <pre>\n<code>2022-06-17 14:50:33 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Using a chunksize of '1000' nrows\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Blood_Test.csv [&lt;carrot.io.common.DataBrick object at 0x10477c760&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x1047fc2e0&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x1047fc5e0&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x1084745b0&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x1084748b0&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x1084742e0&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x108474e20&gt;]\n2022-06-17 14:50:33 - LocalDataCollection - INFO - Registering  pks.csv [&lt;carrot.io.common.DataBrick object at 0x108474d90&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x1047fc040&gt;</code>\n</pre> <pre><code>outputs = carrot.io.BCLinkDataCollection({'dry_run':True},\n                                            output_folder=\"./cache/\",\n                                            write_separate=True)\noutputs\n</code></pre> <pre>\n<code>2022-06-17 14:50:33 - BCLinkDataCollection - INFO - setup bclink collection\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'condition_occurrence' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - condition_occurrence (condition_occurrence) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'death' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - death (death) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'drug_exposure' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - drug_exposure (drug_exposure) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'measurement' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - measurement (measurement) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'observation' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - observation (observation) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - person (person) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'procedure_occurrence' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - procedure_occurrence (procedure_occurrence) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'specimen' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - specimen (specimen) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'visit_occurrence' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - visit_occurrence (visit_occurrence) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'person_ids' ) bclink\n2022-06-17 14:50:33 - BCLinkHelpers - INFO - person_ids (person_ids) already exists --&gt; all good\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM death bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM drug_exposure bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM measurement bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM observation bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM procedure_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM specimen bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM visit_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person_ids bclink\n2022-06-17 14:50:33 - BCLinkDataCollection - INFO - DataCollection Object Created\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.bclink.BCLinkDataCollection at 0x108494820&gt;</code>\n</pre> <pre><code>rules = carrot.tools.load_json(\"../data/rules.json\")\n</code></pre> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=inputs,outputs=outputs)\ncdm.process()\n</code></pre> <pre>\n<code>2022-06-17 14:50:33 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 14:50:33 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 14:50:33 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT * FROM person_ids  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM condition_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'condition_occurrence' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM condition_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM death bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'death' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM death ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM drug_exposure bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'drug_exposure' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM drug_exposure ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM measurement bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'measurement' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM measurement ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM observation bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'observation' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM observation ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM person bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'person' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM person ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM procedure_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'procedure_occurrence' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM procedure_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM specimen bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'specimen' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM specimen ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT count(*) FROM visit_occurrence bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT column_name FROM INFORMATION_SCHEMA. COLUMNS WHERE table_name = 'visit_occurrence' LIMIT 1  bclink\n2022-06-17 14:50:33 - BCLinkHelpers - NOTICE - bc_sqlselect --user=bclink --query=SELECT person_id FROM visit_occurrence ORDER BY -person_id LIMIT 1;  bclink\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added H/O: heart failure 3043 of type observation\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added 2019-nCoV 3044 of type observation\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Cancer 3045 of type observation\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Headache 3028 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Fatigue 3029 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Dizziness 3030 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Cough 3031 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Fever 3032 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Muscle pain 3033 of type condition_occurrence\n2022-06-17 14:50:33 - CommonDataModel - INFO - Added Pneumonia 3042 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added Mental health problem 3046 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added Mental disorder 3047 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added Type 2 diabetes mellitus 3048 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added Ischemic heart disease 3049 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added Hypertensive disorder 3050 of type condition_occurrence\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added COVID-19 vaccine 3034 of type drug_exposure\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added COVID-19 vaccine 3035 of type drug_exposure\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:34 - CommonDataModel - INFO - Added COVID-19 vaccine 3036 of type drug_exposure\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 of type drug_exposure\n2022-06-17 14:50:34 - CommonDataModel - INFO - Added SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 of type drug_exposure\n2022-06-17 14:50:34 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence', 'drug_exposure']\n2022-06-17 14:50:34 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 4,\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5\n}\n2022-06-17 14:50:34 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 14:50:34 - CommonDataModel - INFO - working on person\n2022-06-17 14:50:34 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 14:50:34 - Person - INFO - Called apply_rules\n2022-06-17 14:50:34 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 14:50:34 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:50:34 - Person - INFO - Mapped person_id\n</code>\n</pre> <pre>\n<code>could not convert string to float: 'na'\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:34 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 14:50:34 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 14:50:34 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:50:34 - Person - INFO - created df (0x1084e3790)[MALE_3025]\n2022-06-17 14:50:34 - CommonDataModel - INFO - finished MALE 3025 (0x1084e3790) ... 1/2 completed, 561 rows\n2022-06-17 14:50:34 - BCLinkDataCollection - INFO - saving person_ids.0x1084e3f10.2022-06-17T135034 to ./cache//person_ids.0x1084e3f10.2022-06-17T135034.csv\n2022-06-17 14:50:34 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person_ids --user=data --data_file=./cache//person_ids.0x1084e3f10.2022-06-17T135034.csv --support --bcqueue bclink\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person_ids --user=data --database=bclink\n2022-06-17 14:50:34 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 14:50:34 - Person - INFO - Called apply_rules\n2022-06-17 14:50:34 - Person - INFO - Mapped birth_datetime\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_concept_id\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 14:50:34 - Person - INFO - Mapped gender_source_value\n2022-06-17 14:50:34 - Person - INFO - Mapped person_id\n2022-06-17 14:50:34 - Person - WARNING - Requiring non-null values in gender_concept_id removed 565 rows, leaving 435 rows.\n2022-06-17 14:50:34 - Person - INFO - Automatically formatting data columns.\n2022-06-17 14:50:34 - Person - INFO - created df (0x1084e3c10)[FEMALE_3026]\n2022-06-17 14:50:34 - CommonDataModel - INFO - finished FEMALE 3026 (0x1084e3c10) ... 2/2 completed, 435 rows\n</code>\n</pre> <pre>\n<code>could not convert string to float: 'na'\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:34 - BCLinkDataCollection - INFO - saving person_ids.0x1085161f0.2022-06-17T135034 to ./cache//person_ids.0x1085161f0.2022-06-17T135034.csv\n2022-06-17 14:50:34 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person_ids --user=data --data_file=./cache//person_ids.0x1085161f0.2022-06-17T135034.csv --support --bcqueue bclink\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person_ids --user=data --database=bclink\n2022-06-17 14:50:34 - CommonDataModel - INFO - saving dataframe (0x10854c7c0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x108494820&gt;\n2022-06-17 14:50:34 - BCLinkDataCollection - INFO - saving person.0x10854c7c0.2022-06-17T135034 to ./cache//person.0x10854c7c0.2022-06-17T135034.csv\n2022-06-17 14:50:34 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - dataset_tool --load --table=person --user=data --data_file=./cache//person.0x10854c7c0.2022-06-17T135034.csv --support --bcqueue bclink\n2022-06-17 14:50:34 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=person --user=data --database=bclink\n2022-06-17 14:50:35 - CommonDataModel - INFO - finalised person on iteration 0 producing 996 rows from 2 tables\n2022-06-17 14:50:35 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:50:35 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Demographics.csv'\n2022-06-17 14:50:35 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:35 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:50:35 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:50:35 - CommonDataModel - INFO - for observation: found 4 objects\n2022-06-17 14:50:35 - CommonDataModel - INFO - working on observation\n2022-06-17 14:50:35 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 14:50:35 - Observation - INFO - Called apply_rules\n2022-06-17 14:50:35 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:50:35 - Observation - INFO - Mapped person_id\n2022-06-17 14:50:35 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:50:35 - Observation - INFO - created df (0x108592670)[Antibody_3027]\n2022-06-17 14:50:35 - CommonDataModel - INFO - finished Antibody 3027 (0x108592670) ... 1/4 completed, 413 rows\n2022-06-17 14:50:35 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:35 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:35 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:35 - CommonDataModel - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 14:50:35 - CommonDataModel - INFO - starting on H/O: heart failure 3043\n2022-06-17 14:50:35 - Observation - INFO - Called apply_rules\n2022-06-17 14:50:35 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:50:35 - Observation - INFO - Mapped person_id\n2022-06-17 14:50:35 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 781 rows, leaving 219 rows.\n2022-06-17 14:50:35 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:50:35 - Observation - INFO - created df (0x108837760)[H_O_heart_failure_3043]\n2022-06-17 14:50:35 - CommonDataModel - INFO - finished H/O: heart failure 3043 (0x108837760) ... 2/4 completed, 219 rows\n2022-06-17 14:50:35 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:35 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:35 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:35 - CommonDataModel - ERROR - 218/219 were good, 1 studies are removed.\n2022-06-17 14:50:35 - CommonDataModel - INFO - starting on 2019-nCoV 3044\n2022-06-17 14:50:35 - Observation - INFO - Called apply_rules\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:50:35 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:50:35 - Observation - INFO - Mapped person_id\n2022-06-17 14:50:35 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 847 rows, leaving 153 rows.\n2022-06-17 14:50:35 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:50:36 - Observation - INFO - created df (0x108857700)[2019_nCoV_3044]\n2022-06-17 14:50:36 - CommonDataModel - INFO - finished 2019-nCoV 3044 (0x108857700) ... 3/4 completed, 153 rows\n2022-06-17 14:50:36 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:36 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:36 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:36 - CommonDataModel - ERROR - 152/153 were good, 1 studies are removed.\n2022-06-17 14:50:36 - CommonDataModel - INFO - starting on Cancer 3045\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:36 - Observation - INFO - Called apply_rules\n2022-06-17 14:50:36 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 14:50:36 - Observation - INFO - Mapped observation_datetime\n2022-06-17 14:50:36 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 14:50:36 - Observation - INFO - Mapped observation_source_value\n2022-06-17 14:50:36 - Observation - INFO - Mapped person_id\n2022-06-17 14:50:36 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 708 rows, leaving 292 rows.\n2022-06-17 14:50:36 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 14:50:36 - Observation - INFO - created df (0x108857190)[Cancer_3045]\n2022-06-17 14:50:36 - CommonDataModel - INFO - finished Cancer 3045 (0x108857190) ... 4/4 completed, 292 rows\n2022-06-17 14:50:36 - CommonDataModel - ERROR - Removed 1 row(s) due to duplicates found when merging observation\n2022-06-17 14:50:36 - CommonDataModel - WARNING - Example duplicates...\n2022-06-17 14:50:36 - CommonDataModel - WARNING -                 person_id  observation_concept_id observation_date  \\\nobservation_id                                                       \n440                 110.0                 4059317       2019-07-07   \n441                 110.0                 4059317       2019-07-07   \n\n                      observation_datetime observation_source_value  \\\nobservation_id                                                        \n440             2019-07-07 00:00:00.000000             Heart Attack   \n441             2019-07-07 00:00:00.000000             Heart Attack   \n\n                observation_source_concept_id  \nobservation_id                                 \n440                                   4059317  \n441                                   4059317  \n2022-06-17 14:50:36 - CommonDataModel - INFO - saving dataframe (0x108887880) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x108494820&gt;\n2022-06-17 14:50:36 - BCLinkDataCollection - INFO - saving observation.0x108887880.2022-06-17T135036 to ./cache//observation.0x108887880.2022-06-17T135036.csv\n2022-06-17 14:50:36 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:36 - BCLinkHelpers - NOTICE - dataset_tool --load --table=observation --user=data --data_file=./cache//observation.0x108887880.2022-06-17T135036.csv --support --bcqueue bclink\n2022-06-17 14:50:36 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=observation --user=data --database=bclink\n2022-06-17 14:50:36 - CommonDataModel - INFO - finalised observation on iteration 0 producing 1072 rows from 4 tables\n2022-06-17 14:50:36 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:50:36 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Hospital_Visit.csv'\n2022-06-17 14:50:36 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:36 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Serology.csv'\n2022-06-17 14:50:36 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:36 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:50:36 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:50:36 - CommonDataModel - INFO - for condition_occurrence: found 12 objects\n2022-06-17 14:50:36 - CommonDataModel - INFO - working on condition_occurrence\n2022-06-17 14:50:36 - CommonDataModel - INFO - starting on Headache 3028\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:36 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:36 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 55 rows, leaving 275 rows.\n2022-06-17 14:50:36 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 274 rows.\n2022-06-17 14:50:36 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - created df (0x1088a7ee0)[Headache_3028]\n2022-06-17 14:50:37 - CommonDataModel - INFO - finished Headache 3028 (0x1088a7ee0) ... 1/12 completed, 274 rows\n2022-06-17 14:50:37 - CommonDataModel - INFO - starting on Fatigue 3029\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 95 rows, leaving 235 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 234 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - created df (0x1088e2ac0)[Fatigue_3029]\n2022-06-17 14:50:37 - CommonDataModel - INFO - finished Fatigue 3029 (0x1088e2ac0) ... 2/12 completed, 234 rows\n2022-06-17 14:50:37 - CommonDataModel - INFO - starting on Dizziness 3030\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_value\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 195 rows, leaving 135 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 134 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - created df (0x1088e2760)[Dizziness_3030]\n2022-06-17 14:50:37 - CommonDataModel - INFO - finished Dizziness 3030 (0x1088e2760) ... 3/12 completed, 134 rows\n2022-06-17 14:50:37 - CommonDataModel - INFO - starting on Cough 3031\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 100 rows, leaving 230 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 229 rows.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - created df (0x1088f2100)[Cough_3031]\n2022-06-17 14:50:37 - CommonDataModel - INFO - finished Cough 3031 (0x1088f2100) ... 4/12 completed, 229 rows\n2022-06-17 14:50:37 - CommonDataModel - INFO - starting on Fever 3032\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:37 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 265 rows, leaving 65 rows.\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - created df (0x1088facd0)[Fever_3032]\n2022-06-17 14:50:38 - CommonDataModel - INFO - finished Fever 3032 (0x1088facd0) ... 5/12 completed, 65 rows\n2022-06-17 14:50:38 - CommonDataModel - INFO - starting on Muscle pain 3033\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 295 rows, leaving 35 rows.\n2022-06-17 14:50:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 34 rows.\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - created df (0x1089076a0)[Muscle_pain_3033]\n2022-06-17 14:50:38 - CommonDataModel - INFO - finished Muscle pain 3033 (0x1089076a0) ... 6/12 completed, 34 rows\n2022-06-17 14:50:38 - CommonDataModel - INFO - starting on Pneumonia 3042\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:38 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:38 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 866 rows, leaving 134 rows.\n2022-06-17 14:50:38 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - created df (0x1088b4130)[Pneumonia_3042]\n2022-06-17 14:50:39 - CommonDataModel - INFO - finished Pneumonia 3042 (0x1088b4130) ... 7/12 completed, 134 rows\n2022-06-17 14:50:39 - CommonDataModel - INFO - starting on Mental health problem 3046\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:39 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:39 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 781 rows, leaving 219 rows.\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:39 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - created df (0x1089595e0)[Mental_health_problem_3046]\n2022-06-17 14:50:39 - CommonDataModel - INFO - finished Mental health problem 3046 (0x1089595e0) ... 8/12 completed, 219 rows\n2022-06-17 14:50:39 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:39 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:39 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:39 - CommonDataModel - ERROR - 217/219 were good, 2 studies are removed.\n2022-06-17 14:50:39 - CommonDataModel - INFO - starting on Mental disorder 3047\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:39 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 781 rows, leaving 219 rows.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - created df (0x10898e880)[Mental_disorder_3047]\n2022-06-17 14:50:39 - CommonDataModel - INFO - finished Mental disorder 3047 (0x10898e880) ... 9/12 completed, 219 rows\n2022-06-17 14:50:39 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:39 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:39 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:39 - CommonDataModel - ERROR - 217/219 were good, 2 studies are removed.\n2022-06-17 14:50:39 - CommonDataModel - INFO - starting on Type 2 diabetes mellitus 3048\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:39 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 861 rows, leaving 139 rows.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - created df (0x10841bee0)[Type_2_diabetes_mellitus_3048]\n2022-06-17 14:50:39 - CommonDataModel - INFO - finished Type 2 diabetes mellitus 3048 (0x10841bee0) ... 10/12 completed, 139 rows\n2022-06-17 14:50:39 - CommonDataModel - INFO - starting on Ischemic heart disease 3049\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:39 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:40 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 897 rows, leaving 103 rows.\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - created df (0x108990760)[Ischemic_heart_disease_3049]\n2022-06-17 14:50:40 - CommonDataModel - INFO - finished Ischemic heart disease 3049 (0x108990760) ... 11/12 completed, 103 rows\n2022-06-17 14:50:40 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:40 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:40 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:40 - CommonDataModel - ERROR - 102/103 were good, 1 studies are removed.\n2022-06-17 14:50:40 - CommonDataModel - INFO - starting on Hypertensive disorder 3050\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Called apply_rules\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped condition_concept_id\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped condition_end_datetime\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped condition_source_concept_id\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped condition_source_value\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped condition_start_datetime\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Mapped person_id\n2022-06-17 14:50:40 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 938 rows, leaving 62 rows.\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 14:50:40 - ConditionOccurrence - INFO - created df (0x1089a82b0)[Hypertensive_disorder_3050]\n2022-06-17 14:50:40 - CommonDataModel - INFO - finished Hypertensive disorder 3050 (0x1089a82b0) ... 12/12 completed, 62 rows\n2022-06-17 14:50:40 - CommonDataModel - ERROR - Removed 2 row(s) due to duplicates found when merging condition_occurrence\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:40 - CommonDataModel - WARNING - Example duplicates...\n2022-06-17 14:50:40 - CommonDataModel - WARNING -                          person_id  condition_concept_id condition_start_date  \\\ncondition_occurrence_id                                                         \n38                           125.0                378253           2020-04-11   \n40                           125.0                378253           2020-04-11   \n308                          125.0               4223659           2020-04-11   \n310                          125.0               4223659           2020-04-11   \n\n                           condition_start_datetime condition_end_date  \\\ncondition_occurrence_id                                                  \n38                       2020-04-11 00:00:00.000000         2020-04-11   \n40                       2020-04-11 00:00:00.000000         2020-04-11   \n308                      2020-04-11 00:00:00.000000         2020-04-11   \n310                      2020-04-11 00:00:00.000000         2020-04-11   \n\n                             condition_end_datetime condition_source_value  \\\ncondition_occurrence_id                                                      \n38                       2020-04-11 00:00:00.000000                    Yes   \n40                       2020-04-11 00:00:00.000000                    Yes   \n308                      2020-04-11 00:00:00.000000                    Yes   \n310                      2020-04-11 00:00:00.000000                    Yes   \n\n                         condition_source_concept_id  \ncondition_occurrence_id                               \n38                                            378253  \n40                                            378253  \n308                                          4223659  \n310                                          4223659  \n2022-06-17 14:50:40 - CommonDataModel - INFO - saving dataframe (0x108887bb0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x108494820&gt;\n2022-06-17 14:50:40 - BCLinkDataCollection - INFO - saving condition_occurrence.0x108887bb0.2022-06-17T135040 to ./cache//condition_occurrence.0x108887bb0.2022-06-17T135040.csv\n2022-06-17 14:50:40 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:40 - BCLinkHelpers - NOTICE - dataset_tool --load --table=condition_occurrence --user=data --data_file=./cache//condition_occurrence.0x108887bb0.2022-06-17T135040.csv --support --bcqueue bclink\n2022-06-17 14:50:40 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=condition_occurrence --user=data --database=bclink\n2022-06-17 14:50:40 - CommonDataModel - INFO - finalised condition_occurrence on iteration 0 producing 1841 rows from 12 tables\n2022-06-17 14:50:40 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:50:40 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'GP_Records.csv'\n2022-06-17 14:50:40 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:40 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Hospital_Visit.csv'\n2022-06-17 14:50:40 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:40 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Symptoms.csv'\n2022-06-17 14:50:40 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:40 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 14:50:40 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 14:50:40 - CommonDataModel - INFO - for drug_exposure: found 5 objects\n2022-06-17 14:50:40 - CommonDataModel - INFO - working on drug_exposure\n2022-06-17 14:50:40 - CommonDataModel - INFO - starting on COVID-19 vaccine 3034\n2022-06-17 14:50:40 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:50:40 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:50:40 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:50:40 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:50:41 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:50:41 - DrugExposure - INFO - created df (0x1089b1e50)[COVID_19_vaccine_3034]\n2022-06-17 14:50:41 - CommonDataModel - INFO - finished COVID-19 vaccine 3034 (0x1089b1e50) ... 1/5 completed, 245 rows\n2022-06-17 14:50:41 - CommonDataModel - INFO - starting on COVID-19 vaccine 3035\n2022-06-17 14:50:41 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:50:41 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 494 rows, leaving 226 rows.\n2022-06-17 14:50:41 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 1 rows, leaving 225 rows.\n2022-06-17 14:50:41 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:50:41 - DrugExposure - INFO - created df (0x1089bf070)[COVID_19_vaccine_3035]\n2022-06-17 14:50:41 - CommonDataModel - INFO - finished COVID-19 vaccine 3035 (0x1089bf070) ... 2/5 completed, 225 rows\n2022-06-17 14:50:41 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:41 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:41 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:41 - CommonDataModel - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 14:50:41 - CommonDataModel - INFO - starting on COVID-19 vaccine 3036\n2022-06-17 14:50:41 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_concept_id\n</code>\n</pre> <pre>\n<code>2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:50:41 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:50:41 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:50:41 - DrugExposure - INFO - created df (0x1089bfb80)[COVID_19_vaccine_3036]\n2022-06-17 14:50:41 - CommonDataModel - INFO - finished COVID-19 vaccine 3036 (0x1089bfb80) ... 3/5 completed, 249 rows\n2022-06-17 14:50:41 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:41 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:41 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:41 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:50:41 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040\n2022-06-17 14:50:41 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:50:41 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 14:50:41 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:50:41 - DrugExposure - INFO - created df (0x1089f46d0)[SARS_CoV_2_COVID_19_vaccine_mRNA_1273_0_2_MG_ML_Injectable_Suspension_3040]\n2022-06-17 14:50:41 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 (0x1089f46d0) ... 4/5 completed, 245 rows\n2022-06-17 14:50:41 - CommonDataModel - INFO - starting on SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041\n2022-06-17 14:50:41 - DrugExposure - INFO - Called apply_rules\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_end_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_exposure_start_datetime\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_concept_id\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped drug_source_value\n2022-06-17 14:50:41 - DrugExposure - INFO - Mapped person_id\n2022-06-17 14:50:41 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 14:50:41 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 14:50:42 - DrugExposure - INFO - created df (0x108a0aa60)[SARS_CoV_2_COVID_19_vaccine_mRNA_BNT162b2_0_1_MG_ML_Injectable_Suspension_3041]\n2022-06-17 14:50:42 - CommonDataModel - INFO - finished SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 (0x108a0aa60) ... 5/5 completed, 249 rows\n2022-06-17 14:50:42 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 14:50:42 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 14:50:42 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 14:50:42 - CommonDataModel - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 14:50:42 - CommonDataModel - INFO - saving dataframe (0x1088874c0) to &lt;carrot.io.plugins.bclink.BCLinkDataCollection object at 0x108494820&gt;\n2022-06-17 14:50:42 - BCLinkDataCollection - INFO - saving drug_exposure.0x1088874c0.2022-06-17T135042 to ./cache//drug_exposure.0x1088874c0.2022-06-17T135042.csv\n2022-06-17 14:50:42 - BCLinkDataCollection - INFO - finished save to file\n2022-06-17 14:50:42 - BCLinkHelpers - NOTICE - dataset_tool --load --table=drug_exposure --user=data --data_file=./cache//drug_exposure.0x1088874c0.2022-06-17T135042.csv --support --bcqueue bclink\n2022-06-17 14:50:42 - BCLinkHelpers - NOTICE - datasettool2 list-updates --dataset=drug_exposure --user=data --database=bclink\n2022-06-17 14:50:42 - CommonDataModel - INFO - finalised drug_exposure on iteration 0 producing 1210 rows from 5 tables\n2022-06-17 14:50:42 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 14:50:42 - LocalDataCollection - INFO - Getting the next chunk of size '1000' for 'Vaccinations.csv'\n2022-06-17 14:50:42 - LocalDataCollection - INFO - --&gt; Got 0 rows\n2022-06-17 14:50:42 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 1 8507 1963 7 16 1963-07-16 00:00:00.000000 Male 8507 2 8507 1969 7 14 1969-07-14 00:00:00.000000 Male 8507 3 8507 1956 7 17 1956-07-17 00:00:00.000000 Male 8507 4 8507 1960 7 16 1960-07-16 00:00:00.000000 Male 8507 5 8507 1962 7 16 1962-07-16 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 992 8532 1995 7 8 1995-07-08 00:00:00.000000 Female 8532 993 8532 1956 7 17 1956-07-17 00:00:00.000000 Female 8532 994 8532 1944 7 20 1944-07-20 00:00:00.000000 Female 8532 995 8532 1966 7 15 1966-07-15 00:00:00.000000 Female 8532 996 8532 1974 7 13 1974-07-13 00:00:00.000000 Female 8532 <p>996 rows \u00d7 7 columns</p> <pre><code>cdm['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 1 357 4288455 2020-10-03 2020-10-03 00:00:00.000000 17.172114692899758 4288455 2 258 4288455 2020-11-02 2020-11-02 00:00:00.000000 201.93861878809216 4288455 4 556 4288455 2021-07-26 2021-07-26 00:00:00.000000 11.506250956970998 4288455 5 380 4288455 2021-10-29 2021-10-29 00:00:00.000000 2.6594057121417487 4288455 6 415 4288455 2021-09-07 2021-09-07 00:00:00.000000 40.844873593089126 4288455 ... ... ... ... ... ... ... 1068 469 40757663 2021-03-04 2021-03-04 00:00:00.000000 Cancer 40757663 1069 936 40757663 2020-07-17 2020-07-17 00:00:00.000000 Cancer 40757663 1070 472 40757663 2019-10-25 2019-10-25 00:00:00.000000 Cancer 40757663 1071 944 40757663 2018-08-12 2018-08-12 00:00:00.000000 Cancer 40757663 1072 944 40757663 2019-11-12 2019-11-12 00:00:00.000000 Cancer 40757663 <p>1071 rows \u00d7 6 columns</p>"},{"location":"CaRROT-CDM/notebooks/Part%207%20-%20Manually%20Loading%20Dataframes/","title":"Part 7   Manually Loading Dataframes","text":"<p>This work book shows how different types of input data can be manipulated manually and loaded into <code>pandas</code> dataframes , which are subsequently used by the <code>CommonDataModel</code></p> <p>Importing packages:</p> <pre><code>import carrot\nimport glob\nimport pandas as pd\nimport os\nfrom sqlalchemy import create_engine\n</code></pre> <pre><code>df_map = {\n            os.path.basename(x):pd.read_csv(x,iterator=True) \n            for x in glob.glob('../data/part1/*.csv')\n         }\ndf_map\n</code></pre> <pre>\n<code>{'Blood_Test.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bc1c0&gt;,\n 'Demographics.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bc430&gt;,\n 'GP_Records.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bc2e0&gt;,\n 'Hospital_Visit.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bc730&gt;,\n 'Serology.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bc970&gt;,\n 'Symptoms.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bcbe0&gt;,\n 'Vaccinations.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x1111bcdf0&gt;,\n 'pks.csv': &lt;pandas.io.parsers.readers.TextFileReader at 0x10d2f7400&gt;}</code>\n</pre> <p>Create a carrot.<code>LocalDataCollection</code> object to store the dataframes</p> <pre><code>csv_inputs = carrot.io.LocalDataCollection()\ncsv_inputs.load_input_dataframe(df_map)\ncsv_inputs\n</code></pre> <pre>\n<code>2022-06-17 15:11:44 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Blood_Test.csv [&lt;carrot.io.common.DataBrick object at 0x10d2f7c70&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x1111bc190&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x1111bc280&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x10d2bb2b0&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x10d2f74f0&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x10d2f7d30&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x10d318310&gt;]\n2022-06-17 15:11:44 - LocalDataCollection - INFO - Registering  pks.csv [&lt;carrot.io.common.DataBrick object at 0x10d318040&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x10d2f7700&gt;</code>\n</pre> <p>Check to see what data has been loaded:</p> <pre><code>csv_inputs.keys()\n</code></pre> <pre>\n<code>dict_keys(['Blood_Test.csv', 'Demographics.csv', 'GP_Records.csv', 'Hospital_Visit.csv', 'Serology.csv', 'Symptoms.csv', 'Vaccinations.csv', 'pks.csv'])</code>\n</pre> <pre><code>sql_store = carrot.io.SqlDataCollection(connection_string=\"postgresql://localhost:5432/ExampleCOVID19DataSet\",\n                                          drop_existing=True)\nsql_store\n</code></pre> <pre>\n<code>2022-06-17 15:11:44 - SqlDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:11:45 - SqlDataCollection - INFO - Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.sql.SqlDataCollection at 0x1111e0520&gt;</code>\n</pre> <p>Loop over all the inputs, get a loaded dataframe from the input collections, and use the sql store to write the dataframe to the SQL database </p> <pre><code>for name in csv_inputs.keys():\n    df = csv_inputs[name]\n    name = name.split(\".\")[0]\n    sql_store.write(name,df)\n</code></pre> <pre>\n<code>2022-06-17 15:11:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Blood_Test.csv' for the first time\n2022-06-17 15:11:45 - SqlDataCollection - INFO - updating Blood_Test in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:45 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 15:11:45 - SqlDataCollection - INFO - updating Demographics in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:45 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 15:11:45 - SqlDataCollection - INFO - updating GP_Records in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:45 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:45 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 15:11:45 - SqlDataCollection - INFO - updating Hospital_Visit in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:46 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 15:11:46 - SqlDataCollection - INFO - updating Serology in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:46 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 15:11:46 - SqlDataCollection - INFO - updating Symptoms in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:46 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 15:11:46 - SqlDataCollection - INFO - updating Vaccinations in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:46 - SqlDataCollection - INFO - finished save to psql\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Retrieving initial dataframe for 'pks.csv' for the first time\n2022-06-17 15:11:46 - SqlDataCollection - INFO - updating pks in Engine(postgresql://localhost:5432/ExampleCOVID19DataSet)\n2022-06-17 15:11:46 - SqlDataCollection - INFO - finished save to psql\n</code>\n</pre> <p>Now we can used pandas to test the SQL database we created, and load in some filtered data:</p> <pre><code>connection_string=\"postgresql://localhost:5432/ExampleCOVID19DataSet\"\nengine = create_engine(connection_string)\n</code></pre> <p>Retrieve a filtered pandas dataframe from the SQL connection</p> <pre><code>df_demo = pd.read_sql('SELECT * FROM \"Demographics\" LIMIT 1000;',con=engine)\ndf_demo\n</code></pre> ID Age Sex 0 pk1 57.0 Male 1 pk2 68.0 Female 2 pk3 78.0 Female 3 pk4 51.0 Female 4 pk5 51.0 Male ... ... ... ... 995 pk996 76.0 Female 996 pk997 62.0 Male 997 pk998 54.0 Female 998 pk999 63.0 Male 999 pk1000 46.0 Female <p>1000 rows \u00d7 3 columns</p> <p>Use a more complex SQL command to filter the Serology table based on information in the demographics table, creating a pandas dataframe object.</p> <pre><code>sql_command = r'''\nSELECT \n    * \nFROM \"Serology\" \nWHERE \"ID\" in (\n    SELECT \n        \"ID\" \n    FROM \"Demographics\" \n    LIMIT 1000\n    )\n'''\ndf_serology = pd.read_sql(sql_command,con=engine)\ndf_serology\n</code></pre> ID Date IgG 0 pk654 2020-10-03 17.172114692899758 1 pk460 2020-11-02 201.93861878809216 2 pk12 20223-11-08 a10.601377479381105 3 pk987 2021-07-26 11.506250956970998 4 pk700 2021-10-29 2.6594057121417487 ... ... ... ... 410 pk190 2022-11-07 51.77573831029082 411 pk890 2022-09-07 57.11515081936336 412 pk51 2022-11-07 15.264660709568151 413 pk263 2019-11-13 26.051354325968106 414 pk373 2020-05-25 4.266438928364172 <p>415 rows \u00d7 3 columns</p> <p>Build a new LocalDataCollection from the dataframes pulled from SQL and loaded in memory:</p> <pre><code>sql_inputs = carrot.io.LocalDataCollection()\nsql_inputs.load_input_dataframe({'Serology.csv':df_serology,'Demographics.csv':df_demo})\nsql_inputs\n</code></pre> <pre>\n<code>2022-06-17 15:11:46 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x111650fa0&gt;]\n2022-06-17 15:11:46 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x1112114f0&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x111650fd0&gt;</code>\n</pre> <p>Load some rules (and remove some missing source tables, since we only are dealing with two tables, and only want to apply rules associated with them):</p> <pre><code>rules = carrot.tools.load_json(\"../data/rules.json\")\nrules = carrot.tools.remove_missing_sources_from_rules(rules,sql_inputs.keys())\nrules\n</code></pre> <pre>\n<code>2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed H/O: heart failure 3043 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed 2019-nCoV 3044 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Cancer 3045 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Headache 3028 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Fatigue 3029 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Dizziness 3030 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Cough 3031 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Fever 3032 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Muscle pain 3033 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Pneumonia 3042 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Mental health problem 3046 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Mental disorder 3047 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Type 2 diabetes mellitus 3048 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Ischemic heart disease 3049 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed Hypertensive disorder 3050 from rules because it was not loaded\n2022-06-17 15:11:46 - remove_missing_sources_from_rules - WARNING - removed cdm table 'condition_occurrence' from rules\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3034 from rules because it was not loaded\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3035 from rules because it was not loaded\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed COVID-19 vaccine 3036 from rules because it was not loaded\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed SARS-CoV-2 (COVID-19) vaccine, mRNA-1273 0.2 MG/ML Injectable Suspension 3040 from rules because it was not loaded\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed SARS-CoV-2 (COVID-19) vaccine, mRNA-BNT162b2 0.1 MG/ML Injectable Suspension 3041 from rules because it was not loaded\n2022-06-17 15:11:47 - remove_missing_sources_from_rules - WARNING - removed cdm table 'drug_exposure' from rules\n</code>\n</pre> <pre>\n<code>{'metadata': {'date_created': '2022-02-12T12:22:48.465257',\n  'dataset': 'FAILED: ExampleV4'},\n 'cdm': {'person': {'MALE 3025': {'birth_datetime': {'source_table': 'Demographics.csv',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Male': 8507}},\n    'gender_source_value': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics.csv', 'source_field': 'ID'}},\n   'FEMALE 3026': {'birth_datetime': {'source_table': 'Demographics.csv',\n     'source_field': 'Age',\n     'operations': ['get_datetime_from_age']},\n    'gender_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_concept_id': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex',\n     'term_mapping': {'Female': 8532}},\n    'gender_source_value': {'source_table': 'Demographics.csv',\n     'source_field': 'Sex'},\n    'person_id': {'source_table': 'Demographics.csv', 'source_field': 'ID'}}},\n  'observation': {'Antibody 3027': {'observation_concept_id': {'source_table': 'Serology.csv',\n     'source_field': 'IgG',\n     'term_mapping': 4288455},\n    'observation_datetime': {'source_table': 'Serology.csv',\n     'source_field': 'Date'},\n    'observation_source_concept_id': {'source_table': 'Serology.csv',\n     'source_field': 'IgG',\n     'term_mapping': 4288455},\n    'observation_source_value': {'source_table': 'Serology.csv',\n     'source_field': 'IgG'},\n    'person_id': {'source_table': 'Serology.csv', 'source_field': 'ID'}}}}}</code>\n</pre> <p>Create a common data model object and process it to create CDM tables</p> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=sql_inputs)\ncdm.process()\n</code></pre> <pre>\n<code>2022-06-17 15:11:47 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:11:47 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 15:11:47 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 15:11:47 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 15:11:47 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 15:11:47 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 15:11:47 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation']\n2022-06-17 15:11:47 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 1\n}\n2022-06-17 15:11:47 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 15:11:47 - CommonDataModel - INFO - working on person\n2022-06-17 15:11:47 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 15:11:47 - Person - INFO - Called apply_rules\n2022-06-17 15:11:47 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 15:11:47 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:11:47 - Person - INFO - Mapped person_id\n2022-06-17 15:11:47 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 15:11:47 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 15:11:47 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:11:47 - Person - INFO - created df (0x111585a30)[MALE_3025]\n2022-06-17 15:11:47 - CommonDataModel - INFO - finished MALE 3025 (0x111585a30) ... 1/2 completed, 561 rows\n2022-06-17 15:11:47 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 15:11:47 - Person - INFO - Called apply_rules\n2022-06-17 15:11:47 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:11:47 - Person - INFO - Mapped gender_source_value\n</code>\n</pre> <pre>\n<code>could not convert string to float: 'na'\ncould not convert string to float: 'na'\n</code>\n</pre> <pre>\n<code>2022-06-17 15:11:47 - Person - INFO - Mapped person_id\n2022-06-17 15:11:47 - Person - WARNING - Requiring non-null values in gender_concept_id removed 565 rows, leaving 435 rows.\n2022-06-17 15:11:47 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:11:47 - Person - INFO - created df (0x111608820)[FEMALE_3026]\n2022-06-17 15:11:47 - CommonDataModel - INFO - finished FEMALE 3026 (0x111608820) ... 2/2 completed, 435 rows\n2022-06-17 15:11:47 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:11:47 - CommonDataModel - INFO - finalised person on iteration 0 producing 996 rows from 2 tables\n2022-06-17 15:11:47 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:11:47 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:11:47 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:11:47 - CommonDataModel - INFO - for observation: found 1 object\n2022-06-17 15:11:47 - CommonDataModel - INFO - working on observation\n2022-06-17 15:11:47 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 15:11:47 - Observation - INFO - Called apply_rules\n2022-06-17 15:11:47 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 15:11:47 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:11:47 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:11:47 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:11:47 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:11:47 - Observation - INFO - Mapped person_id\n2022-06-17 15:11:47 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:11:47 - Observation - INFO - created df (0x111689d60)[Antibody_3027]\n2022-06-17 15:11:47 - CommonDataModel - INFO - finished Antibody 3027 (0x111689d60) ... 1/1 completed, 413 rows\n2022-06-17 15:11:47 - CommonDataModel - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:11:47 - CommonDataModel - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:11:47 - CommonDataModel - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:11:47 - CommonDataModel - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 15:11:47 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:11:47 - CommonDataModel - INFO - finalised observation on iteration 0 producing 410 rows from 1 tables\n2022-06-17 15:11:47 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:11:47 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 1 8507 1963 7 16 1963-07-16 00:00:00.000000 Male 8507 2 8507 1969 7 14 1969-07-14 00:00:00.000000 Male 8507 3 8507 1956 7 17 1956-07-17 00:00:00.000000 Male 8507 4 8507 1960 7 16 1960-07-16 00:00:00.000000 Male 8507 5 8507 1962 7 16 1962-07-16 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 992 8532 1995 7 8 1995-07-08 00:00:00.000000 Female 8532 993 8532 1956 7 17 1956-07-17 00:00:00.000000 Female 8532 994 8532 1944 7 20 1944-07-20 00:00:00.000000 Female 8532 995 8532 1966 7 15 1966-07-15 00:00:00.000000 Female 8532 996 8532 1974 7 13 1974-07-13 00:00:00.000000 Female 8532 <p>996 rows \u00d7 7 columns</p> <pre><code>cdm['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 1 357 4288455 2020-10-03 2020-10-03 00:00:00.000000 17.172114692899758 4288455 2 258 4288455 2020-11-02 2020-11-02 00:00:00.000000 201.93861878809216 4288455 4 556 4288455 2021-07-26 2021-07-26 00:00:00.000000 11.506250956970998 4288455 5 380 4288455 2021-10-29 2021-10-29 00:00:00.000000 2.6594057121417487 4288455 6 415 4288455 2021-09-07 2021-09-07 00:00:00.000000 40.844873593089126 4288455 ... ... ... ... ... ... ... 411 641 4288455 2022-11-07 2022-11-07 00:00:00.000000 51.77573831029082 4288455 412 492 4288455 2022-09-07 2022-09-07 00:00:00.000000 57.11515081936336 4288455 413 31 4288455 2022-11-07 2022-11-07 00:00:00.000000 15.264660709568151 4288455 414 672 4288455 2019-11-13 2019-11-13 00:00:00.000000 26.051354325968106 4288455 415 208 4288455 2020-05-25 2020-05-25 00:00:00.000000 4.266438928364172 4288455 <p>410 rows \u00d7 6 columns</p> <pre><code>from pyspark.sql import SparkSession\n</code></pre> <p>Define the session:</p> <pre><code>spark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.jars\", \"/Users/calummacdonald/Downloads/postgresql-42.3.1.jar\") \\\n    .getOrCreate()\n</code></pre> <p>Create a reader:</p> <pre><code>reader = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5432/ExampleCOVID19DataSet\") \\\n    .option(\"driver\", \"org.postgresql.Driver\") \nreader\n</code></pre> <pre>\n<code>&lt;pyspark.sql.readwriter.DataFrameReader at 0x1119ef910&gt;</code>\n</pre> <p>Create and load a spark dataframe for the Demographics table and specify to filter this on all people under the age of 20, selecting only the first 300 rows:</p> <pre><code>sdf_demo = reader.option(\"dbtable\", '\"Demographics\"')\\\n                 .load()\\\n\nsdf_demo = sdf_demo.filter(sdf_demo.Age&amp;lt;50).limit(300)\nsdf_demo.count()\n</code></pre> <pre>\n<code>300</code>\n</pre> <p>Select the first 100 rows:</p> <pre><code>sdf_demo_first = sdf_demo.limit(100)\n</code></pre> <p>Drop the first 100 rows by subtracting the first 1000:</p> <pre><code>sdf_demo = sdf_demo.subtract(sdf_demo_first).limit(100)\nsdf_demo.count()\n</code></pre> <pre>\n<code>100</code>\n</pre> <p>Load the serology table, selecting only those whos ID is in the already loaded spark dataframe for the demographics</p> <pre><code>sdf_serology = reader.option(\"dbtable\", '\"Serology\"')\\\n                     .load()\n\nsdf_serology = sdf_serology.join(sdf_demo,\n                                 ['ID'])\\\n                            .select(*sdf_serology.columns)\n\nsdf_serology.count()\n</code></pre> <pre>\n<code>52</code>\n</pre> <p>Retrieve pandas dataframes from these spark dataframes and put them in a new map note: we keep the name as '.csv' because this is what the name is in the rules file!</p> <pre><code>df_map = {\n            'Demographics.csv': sdf_demo.select('*').toPandas(),\n            'Serology.csv': sdf_serology.select('*').toPandas()\n         }\n</code></pre> <pre><code>spark_inputs = carrot.io.LocalDataCollection()\nspark_inputs.load_input_dataframe(df_map)\nspark_inputs\n</code></pre> <pre>\n<code>2022-06-17 15:12:05 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:12:05 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x11188c910&gt;]\n2022-06-17 15:12:05 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x10d214c10&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x11188c970&gt;</code>\n</pre> <pre><code>cdm = carrot.cdm.CommonDataModel.from_rules(rules,inputs=spark_inputs)\ncdm.process()\n</code></pre> <pre>\n<code>2022-06-17 15:12:05 - CommonDataModel - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:12:05 - CommonDataModel - INFO - Running with an DataCollection object\n2022-06-17 15:12:05 - CommonDataModel - INFO - Turning on automatic cdm column filling\n2022-06-17 15:12:05 - CommonDataModel - INFO - Added MALE 3025 of type person\n2022-06-17 15:12:05 - CommonDataModel - INFO - Added FEMALE 3026 of type person\n2022-06-17 15:12:05 - CommonDataModel - INFO - Added Antibody 3027 of type observation\n2022-06-17 15:12:05 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation']\n2022-06-17 15:12:05 - CommonDataModel - INFO - Number of objects to process for each table...\n{\n      \"person\": 2,\n      \"observation\": 1\n}\n2022-06-17 15:12:05 - CommonDataModel - INFO - for person: found 2 objects\n2022-06-17 15:12:05 - CommonDataModel - INFO - working on person\n2022-06-17 15:12:05 - CommonDataModel - INFO - starting on MALE 3025\n2022-06-17 15:12:05 - Person - INFO - Called apply_rules\n2022-06-17 15:12:05 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 15:12:05 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:12:05 - Person - INFO - Mapped person_id\n2022-06-17 15:12:05 - Person - WARNING - Requiring non-null values in gender_concept_id removed 50 rows, leaving 50 rows.\n2022-06-17 15:12:05 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:12:05 - Person - INFO - created df (0x115783610)[MALE_3025]\n2022-06-17 15:12:05 - CommonDataModel - INFO - finished MALE 3025 (0x115783610) ... 1/2 completed, 50 rows\n2022-06-17 15:12:05 - CommonDataModel - INFO - starting on FEMALE 3026\n2022-06-17 15:12:05 - Person - INFO - Called apply_rules\n2022-06-17 15:12:05 - Person - INFO - Mapped birth_datetime\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_concept_id\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_source_concept_id\n2022-06-17 15:12:05 - Person - INFO - Mapped gender_source_value\n2022-06-17 15:12:05 - Person - INFO - Mapped person_id\n2022-06-17 15:12:05 - Person - WARNING - Requiring non-null values in gender_concept_id removed 50 rows, leaving 50 rows.\n2022-06-17 15:12:05 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:12:05 - Person - INFO - created df (0x1158018b0)[FEMALE_3026]\n2022-06-17 15:12:05 - CommonDataModel - INFO - finished FEMALE 3026 (0x1158018b0) ... 2/2 completed, 50 rows\n2022-06-17 15:12:05 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:12:05 - CommonDataModel - INFO - finalised person on iteration 0 producing 100 rows from 2 tables\n2022-06-17 15:12:05 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:12:05 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:12:05 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:12:05 - CommonDataModel - INFO - for observation: found 1 object\n2022-06-17 15:12:05 - CommonDataModel - INFO - working on observation\n2022-06-17 15:12:05 - CommonDataModel - INFO - starting on Antibody 3027\n2022-06-17 15:12:05 - Observation - INFO - Called apply_rules\n2022-06-17 15:12:05 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 15:12:05 - Observation - INFO - Mapped observation_concept_id\n2022-06-17 15:12:05 - Observation - INFO - Mapped observation_datetime\n2022-06-17 15:12:05 - Observation - INFO - Mapped observation_source_concept_id\n2022-06-17 15:12:05 - Observation - INFO - Mapped observation_source_value\n2022-06-17 15:12:05 - Observation - INFO - Mapped person_id\n2022-06-17 15:12:06 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:12:06 - Observation - INFO - created df (0x1158015b0)[Antibody_3027]\n2022-06-17 15:12:06 - CommonDataModel - INFO - finished Antibody 3027 (0x1158015b0) ... 1/1 completed, 52 rows\n2022-06-17 15:12:06 - CommonDataModel - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:12:06 - CommonDataModel - INFO - finalised observation on iteration 0 producing 52 rows from 1 tables\n2022-06-17 15:12:06 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:12:06 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>cdm['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 1 7 4288455 2021-04-12 2021-04-12 00:00:00.000000 67.58837665287089 4288455 2 70 4288455 2020-07-26 2020-07-26 00:00:00.000000 0.6408428671070668 4288455 3 30 4288455 2020-04-08 2020-04-08 00:00:00.000000 7.11704584051039 4288455 4 47 4288455 2020-04-05 2020-04-05 00:00:00.000000 51.60608444799083 4288455 5 14 4288455 2022-11-23 2022-11-23 00:00:00.000000 33.520886653263354 4288455 6 99 4288455 2022-10-21 2022-10-21 00:00:00.000000 30.00234968904614 4288455 7 7 4288455 2020-12-09 2020-12-09 00:00:00.000000 44.98630030598384 4288455 8 45 4288455 2021-06-03 2021-06-03 00:00:00.000000 1.356998868542723 4288455 9 44 4288455 2021-07-29 2021-07-29 00:00:00.000000 4.280139762594507 4288455 10 49 4288455 2021-08-21 2021-08-21 00:00:00.000000 38.4800593377047 4288455 11 20 4288455 2021-06-21 2021-06-21 00:00:00.000000 13.047482799652261 4288455 12 99 4288455 2020-02-10 2020-02-10 00:00:00.000000 22.78468899637097 4288455 13 14 4288455 2020-03-03 2020-03-03 00:00:00.000000 39.85660795989933 4288455 14 28 4288455 2020-10-20 2020-10-20 00:00:00.000000 130.7484060184659 4288455 15 11 4288455 2021-05-21 2021-05-21 00:00:00.000000 2.3531945048819702 4288455 16 37 4288455 2020-01-09 2020-01-09 00:00:00.000000 131.16013496197073 4288455 17 99 4288455 2021-03-19 2021-03-19 00:00:00.000000 41.23866863346225 4288455 18 22 4288455 2021-07-18 2021-07-18 00:00:00.000000 114.70845380963237 4288455 19 98 4288455 2021-02-15 2021-02-15 00:00:00.000000 33.4316118811117 4288455 20 98 4288455 2021-11-02 2021-11-02 00:00:00.000000 21.00993867468348 4288455 21 70 4288455 2021-01-27 2021-01-27 00:00:00.000000 42.32137660031578 4288455 22 47 4288455 2020-12-06 2020-12-06 00:00:00.000000 5.164881046319425 4288455 23 30 4288455 2022-07-13 2022-07-13 00:00:00.000000 15.736590923162836 4288455 24 24 4288455 2020-07-21 2020-07-21 00:00:00.000000 27.071053730612373 4288455 25 45 4288455 2021-12-31 2021-12-31 00:00:00.000000 2.1417936491708627 4288455 26 43 4288455 2019-04-09 2019-04-09 00:00:00.000000 49.488028088576364 4288455 27 35 4288455 2020-11-19 2020-11-19 00:00:00.000000 29.29234378590352 4288455 28 35 4288455 2022-06-19 2022-06-19 00:00:00.000000 14.860631891926625 4288455 29 35 4288455 2021-03-22 2021-03-22 00:00:00.000000 2.992110959527764 4288455 30 8 4288455 2018-11-11 2018-11-11 00:00:00.000000 21.735630996804954 4288455 31 49 4288455 2021-02-16 2021-02-16 00:00:00.000000 61.242604429998536 4288455 32 56 4288455 2020-09-17 2020-09-17 00:00:00.000000 35.64561946641211 4288455 33 94 4288455 2021-01-14 2021-01-14 00:00:00.000000 16.85133028315674 4288455 34 99 4288455 2020-05-01 2020-05-01 00:00:00.000000 76.31436547629494 4288455 35 44 4288455 2023-05-05 2023-05-05 00:00:00.000000 13.090714707565416 4288455 36 44 4288455 2022-03-07 2022-03-07 00:00:00.000000 46.90458111274723 4288455 37 7 4288455 2021-12-06 2021-12-06 00:00:00.000000 13.391740735874107 4288455 38 43 4288455 2023-02-10 2023-02-10 00:00:00.000000 2.355228862866826 4288455 39 22 4288455 2022-12-01 2022-12-01 00:00:00.000000 45.18338119617369 4288455 40 31 4288455 2020-07-11 2020-07-11 00:00:00.000000 30.750296296633877 4288455 41 28 4288455 2021-08-30 2021-08-30 00:00:00.000000 1.6530625810897206 4288455 42 31 4288455 2022-06-11 2022-06-11 00:00:00.000000 22.760743310029042 4288455 43 43 4288455 2021-07-20 2021-07-20 00:00:00.000000 6.855118976472302 4288455 44 73 4288455 2021-01-05 2021-01-05 00:00:00.000000 44.467218585015324 4288455 45 94 4288455 2021-06-12 2021-06-12 00:00:00.000000 17.805600462375875 4288455 46 35 4288455 2022-03-01 2022-03-01 00:00:00.000000 5.2708732462830366 4288455 47 45 4288455 2020-06-07 2020-06-07 00:00:00.000000 113.23304020407052 4288455 48 47 4288455 2022-06-29 2022-06-29 00:00:00.000000 13.213677356668985 4288455 49 72 4288455 2024-01-06 2024-01-06 00:00:00.000000 8.782375393095213 4288455 50 37 4288455 2020-04-18 2020-04-18 00:00:00.000000 39.76776532306543 4288455 51 70 4288455 2021-07-17 2021-07-17 00:00:00.000000 20.622527546946543 4288455 52 19 4288455 2022-09-16 2022-09-16 00:00:00.000000 7.680385469453483 4288455 <pre><code>cdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 1 8507 1972 7 13 1972-07-13 00:00:00.000000 Male 8507 2 8507 1979 7 12 1979-07-12 00:00:00.000000 Male 8507 3 8507 1982 7 11 1982-07-11 00:00:00.000000 Male 8507 4 8507 2012 7 3 2012-07-03 00:00:00.000000 Male 8507 5 8507 1973 7 13 1973-07-13 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 96 8532 1972 7 13 1972-07-13 00:00:00.000000 Female 8532 97 8532 1977 7 12 1977-07-12 00:00:00.000000 Female 8532 98 8532 1977 7 12 1977-07-12 00:00:00.000000 Female 8532 99 8532 1996 7 7 1996-07-07 00:00:00.000000 Female 8532 100 8532 1998 7 7 1998-07-07 00:00:00.000000 Female 8532 <p>100 rows \u00d7 7 columns</p>"},{"location":"CaRROT-CDM/notebooks/Part%207%20-%20Manually%20Loading%20Dataframes/#csv-files","title":"CSV Files","text":"<p>Create a map between the csv filename and a <code>pandas</code> dataframe, loaded from the csv</p> <p>note: <code>iterator=True</code> tells pandas to not read the data into memory, but setup a <code>parsers.TextFileReader</code>           specifying <code>chunksize=&lt;value&gt;</code> will also return an iterator, allowing for easy looping over data chunks </p>"},{"location":"CaRROT-CDM/notebooks/Part%207%20-%20Manually%20Loading%20Dataframes/#sql","title":"SQL","text":"<p>The following shows how these objects can be used to write the csv files from the input collection to a SQL database.</p>"},{"location":"CaRROT-CDM/notebooks/Part%207%20-%20Manually%20Loading%20Dataframes/#pyspark","title":"PySpark","text":"<p>Using <code>PySpark</code> we can create a session and a reader to connect to the same SQL database we created above</p>"},{"location":"CaRROT-CDM/notebooks/Part%208%20-%20Create%20PyConfig/","title":"Part 8   Create PyConfig","text":"<pre><code>!carrot run py --help\n</code></pre> <pre>\n<code>Usage: carrot run py [OPTIONS] COMMAND [ARGS]...\n\n  Commands for using python configurations to run the ETL transformation.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  list      List all the python classes there are available to run\n  make      Generate a python class from the OMOP mapping json\n  map       Perform OMOP Mapping given a python configuration file.\n  register  Register a python class with the tool\n  remove    remove a registered class\n</code>\n</pre> <pre><code>!carrot run py make --name ExampleDataset ../data/rules.json\n</code></pre> <pre>\n<code>Recreating file /Users/calummacdonald/Usher/CO-CONNECT/docs/docs/CaRROT-CDM/notebooks/ExampleDataset.py\n</code>\n</pre> <p>This automatically creates a file that looks like this:</p> <pre><code># %load ExampleDataset.py\nfrom carrot.cdm import define_person, define_condition_occurrence, define_visit_occurrence, define_measurement, define_observation, define_drug_exposure\nfrom carrot.cdm import CommonDataModel\nimport json\n\nclass ExampleDataset(CommonDataModel):\n\n    def __init__(self,**kwargs):\n        \"\"\" \n        initialise the inputs and setup indexing \n        \"\"\"\n        super().__init__(**kwargs)\n\n\n    @define_person\n    def person_0(self):\n        \"\"\"\n        Create CDM object for person\n        \"\"\"\n        self.birth_datetime.series = self.inputs[\"Demographics.csv\"][\"Age\"]\n        self.gender_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_value.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.person_id.series = self.inputs[\"Demographics.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n        self.birth_datetime.series = self.tools.get_datetime_from_age(self.birth_datetime.series)\n\n        # --- insert term mapping --- \n        self.gender_concept_id.series = self.gender_concept_id.series.map(\n            {\n                \"Male\": 8507\n            }\n        )\n        self.gender_source_concept_id.series = self.gender_source_concept_id.series.map(\n            {\n                \"Male\": 8507\n            }\n        )\n\n    @define_person\n    def person_1(self):\n        \"\"\"\n        Create CDM object for person\n        \"\"\"\n        self.birth_datetime.series = self.inputs[\"Demographics.csv\"][\"Age\"]\n        self.gender_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_value.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.person_id.series = self.inputs[\"Demographics.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n        self.birth_datetime.series = self.tools.get_datetime_from_age(self.birth_datetime.series)\n\n        # --- insert term mapping --- \n        self.gender_concept_id.series = self.gender_concept_id.series.map(\n            {\n                \"Female\": 8532\n            }\n        )\n        self.gender_source_concept_id.series = self.gender_source_concept_id.series.map(\n            {\n                \"Female\": 8532\n            }\n        )\n\n    @define_observation\n    def observation_0(self):\n        \"\"\"\n        Create CDM object for observation\n        \"\"\"\n        self.observation_concept_id.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n        self.observation_datetime.series = self.inputs[\"Serology.csv\"][\"Date\"]\n        self.observation_source_concept_id.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n        self.observation_source_value.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n        self.person_id.series = self.inputs[\"Serology.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.observation_concept_id.series = self.tools.make_scalar(self.observation_concept_id.series,4288455)\n        self.observation_source_concept_id.series = self.tools.make_scalar(self.observation_source_concept_id.series,4288455)\n\n    @define_observation\n    def observation_1(self):\n        \"\"\"\n        Create CDM object for observation\n        \"\"\"\n        self.observation_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_datetime.series = self.inputs[\"Hospital_Visit.csv\"][\"admission_date\"]\n        self.observation_source_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_source_value.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.person_id.series = self.inputs[\"Hospital_Visit.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.observation_concept_id.series = self.observation_concept_id.series.map(\n            {\n                \"Heart Attack\": 4059317\n            }\n        )\n        self.observation_source_concept_id.series = self.observation_source_concept_id.series.map(\n            {\n                \"Heart Attack\": 4059317\n            }\n        )\n\n    @define_observation\n    def observation_2(self):\n        \"\"\"\n        Create CDM object for observation\n        \"\"\"\n        self.observation_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_datetime.series = self.inputs[\"Hospital_Visit.csv\"][\"admission_date\"]\n        self.observation_source_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_source_value.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.person_id.series = self.inputs[\"Hospital_Visit.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.observation_concept_id.series = self.observation_concept_id.series.map(\n            {\n                \"COVID-19\": 37311065\n            }\n        )\n        self.observation_source_concept_id.series = self.observation_source_concept_id.series.map(\n            {\n                \"COVID-19\": 37311065\n            }\n        )\n\n    @define_observation\n    def observation_3(self):\n        \"\"\"\n        Create CDM object for observation\n        \"\"\"\n        self.observation_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_datetime.series = self.inputs[\"Hospital_Visit.csv\"][\"admission_date\"]\n        self.observation_source_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.observation_source_value.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.person_id.series = self.inputs[\"Hospital_Visit.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.observation_concept_id.series = self.observation_concept_id.series.map(\n            {\n                \"Cancer\": 40757663\n            }\n        )\n        self.observation_source_concept_id.series = self.observation_source_concept_id.series.map(\n            {\n                \"Cancer\": 40757663\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_0(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Headache\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Headache\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Headache\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 378253\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 378253\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_1(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Fatigue\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Fatigue\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Fatigue\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 4223659\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 4223659\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_2(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Dizzy\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Dizzy\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Dizzy\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 4223938\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 4223938\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_3(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Cough\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Cough\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Cough\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 254761\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 254761\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_4(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Fever\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Fever\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Fever\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 437663\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 437663\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_5(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Muscle_Pain\"]\n        self.condition_end_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.condition_source_concept_id.series = self.inputs[\"Symptoms.csv\"][\"Muscle_Pain\"]\n        self.condition_source_value.series = self.inputs[\"Symptoms.csv\"][\"Muscle_Pain\"]\n        self.condition_start_datetime.series = self.inputs[\"Symptoms.csv\"][\"date_occurrence\"]\n        self.person_id.series = self.inputs[\"Symptoms.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Yes\": 442752\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Yes\": 442752\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_6(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.condition_end_datetime.series = self.inputs[\"Hospital_Visit.csv\"][\"admission_date\"]\n        self.condition_source_concept_id.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.condition_source_value.series = self.inputs[\"Hospital_Visit.csv\"][\"reason\"]\n        self.condition_start_datetime.series = self.inputs[\"Hospital_Visit.csv\"][\"admission_date\"]\n        self.person_id.series = self.inputs[\"Hospital_Visit.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Pneumonia\": 255848\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Pneumonia\": 255848\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_7(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_end_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.condition_source_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_source_value.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_start_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.person_id.series = self.inputs[\"GP_Records.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Mental Health\": 4131548\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Mental Health\": 4131548\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_8(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_end_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.condition_source_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_source_value.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_start_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.person_id.series = self.inputs[\"GP_Records.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Mental Health\": 432586\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Mental Health\": 432586\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_9(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_end_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.condition_source_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_source_value.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_start_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.person_id.series = self.inputs[\"GP_Records.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Diabetes Type-II\": 201826\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Diabetes Type-II\": 201826\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_10(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_end_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.condition_source_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_source_value.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_start_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.person_id.series = self.inputs[\"GP_Records.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"Heart Condition\": 4185932\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"Heart Condition\": 4185932\n            }\n        )\n\n    @define_condition_occurrence\n    def condition_occurrence_11(self):\n        \"\"\"\n        Create CDM object for condition_occurrence\n        \"\"\"\n        self.condition_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_end_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.condition_source_concept_id.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_source_value.series = self.inputs[\"GP_Records.csv\"][\"comorbidity\"]\n        self.condition_start_datetime.series = self.inputs[\"GP_Records.csv\"][\"date_of_visit\"]\n        self.person_id.series = self.inputs[\"GP_Records.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.condition_concept_id.series = self.condition_concept_id.series.map(\n            {\n                \"High Blood Pressure\": 316866\n            }\n        )\n        self.condition_source_concept_id.series = self.condition_source_concept_id.series.map(\n            {\n                \"High Blood Pressure\": 316866\n            }\n        )\n\n    @define_drug_exposure\n    def drug_exposure_0(self):\n        \"\"\"\n        Create CDM object for drug_exposure\n        \"\"\"\n        self.drug_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_exposure_end_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_exposure_start_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_source_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_source_value.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.person_id.series = self.inputs[\"Vaccinations.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.drug_concept_id.series = self.drug_concept_id.series.map(\n            {\n                \"Moderna\": 35894915\n            }\n        )\n        self.drug_source_concept_id.series = self.drug_source_concept_id.series.map(\n            {\n                \"Moderna\": 35894915\n            }\n        )\n\n    @define_drug_exposure\n    def drug_exposure_1(self):\n        \"\"\"\n        Create CDM object for drug_exposure\n        \"\"\"\n        self.drug_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_exposure_end_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_exposure_start_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_source_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_source_value.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.person_id.series = self.inputs[\"Vaccinations.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.drug_concept_id.series = self.drug_concept_id.series.map(\n            {\n                \"AstraZenica\": 35894915\n            }\n        )\n        self.drug_source_concept_id.series = self.drug_source_concept_id.series.map(\n            {\n                \"AstraZenica\": 35894915\n            }\n        )\n\n    @define_drug_exposure\n    def drug_exposure_2(self):\n        \"\"\"\n        Create CDM object for drug_exposure\n        \"\"\"\n        self.drug_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_exposure_end_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_exposure_start_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_source_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_source_value.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.person_id.series = self.inputs[\"Vaccinations.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.drug_concept_id.series = self.drug_concept_id.series.map(\n            {\n                \"Pfizer\": 35894915\n            }\n        )\n        self.drug_source_concept_id.series = self.drug_source_concept_id.series.map(\n            {\n                \"Pfizer\": 35894915\n            }\n        )\n\n    @define_drug_exposure\n    def drug_exposure_3(self):\n        \"\"\"\n        Create CDM object for drug_exposure\n        \"\"\"\n        self.drug_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_exposure_end_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_exposure_start_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_source_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_source_value.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.person_id.series = self.inputs[\"Vaccinations.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.drug_concept_id.series = self.drug_concept_id.series.map(\n            {\n                \"Moderna\": 37003518\n            }\n        )\n        self.drug_source_concept_id.series = self.drug_source_concept_id.series.map(\n            {\n                \"Moderna\": 37003518\n            }\n        )\n\n    @define_drug_exposure\n    def drug_exposure_4(self):\n        \"\"\"\n        Create CDM object for drug_exposure\n        \"\"\"\n        self.drug_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_exposure_end_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_exposure_start_datetime.series = self.inputs[\"Vaccinations.csv\"][\"date_of_vaccination\"]\n        self.drug_source_concept_id.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.drug_source_value.series = self.inputs[\"Vaccinations.csv\"][\"type\"]\n        self.person_id.series = self.inputs[\"Vaccinations.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n\n        # --- insert term mapping --- \n        self.drug_concept_id.series = self.drug_concept_id.series.map(\n            {\n                \"Pfizer\": 37003436\n            }\n        )\n        self.drug_source_concept_id.series = self.drug_source_concept_id.series.map(\n            {\n                \"Pfizer\": 37003436\n            }\n        )\n</code></pre> <p>Loading some inputs..</p> <pre><code>import carrot\nimport glob\ninputs = carrot.tools.load_csv(glob.glob('../data/part1/*'))\ninputs\n</code></pre> <pre>\n<code>2022-06-17 15:17:54 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Blood_Test.csv [&lt;carrot.io.common.DataBrick object at 0x111f1be50&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x111fd15b0&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x111fd12e0&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x111f56b80&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x111f56730&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x11207f070&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x11207f340&gt;]\n2022-06-17 15:17:54 - LocalDataCollection - INFO - Registering  pks.csv [&lt;carrot.io.common.DataBrick object at 0x115f20340&gt;]\n</code>\n</pre> <pre>\n<code>&lt;carrot.io.plugins.local.LocalDataCollection at 0x111fd1f40&gt;</code>\n</pre> <p>A new instances can be created from the created python class </p> <pre><code>instance = ExampleDataset(inputs=inputs)\ninstance\n</code></pre> <pre>\n<code>2022-06-17 15:17:54 - ExampleDataset - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:17:54 - ExampleDataset - INFO - Running with an DataCollection object\n2022-06-17 15:17:54 - ExampleDataset - INFO - Turning on automatic cdm column filling\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_0 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_1 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_10 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_11 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_2 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_3 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_4 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_5 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_6 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_7 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_8 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added condition_occurrence_9 of type condition_occurrence\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added drug_exposure_0 of type drug_exposure\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added drug_exposure_1 of type drug_exposure\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added drug_exposure_2 of type drug_exposure\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added drug_exposure_3 of type drug_exposure\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added drug_exposure_4 of type drug_exposure\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added observation_0 of type observation\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added observation_1 of type observation\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added observation_2 of type observation\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added observation_3 of type observation\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added person_0 of type person\n2022-06-17 15:17:54 - ExampleDataset - INFO - Added person_1 of type person\n</code>\n</pre> <pre>\n<code>&lt;__main__.ExampleDataset at 0x115f20f40&gt;</code>\n</pre> <pre><code>instance.process()\n</code></pre> <pre>\n<code>2022-06-17 15:17:54 - ExampleDataset - INFO - Starting processing in order: ['person', 'condition_occurrence', 'drug_exposure', 'observation']\n2022-06-17 15:17:54 - ExampleDataset - INFO - Number of objects to process for each table...\n{\n      \"condition_occurrence\": 12,\n      \"drug_exposure\": 5,\n      \"observation\": 4,\n      \"person\": 2\n}\n2022-06-17 15:17:54 - ExampleDataset - INFO - for person: found 2 objects\n2022-06-17 15:17:54 - ExampleDataset - INFO - working on person\n2022-06-17 15:17:54 - ExampleDataset - INFO - starting on person_0\n2022-06-17 15:17:55 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 15:17:55 - Person - WARNING - Requiring non-null values in gender_concept_id removed 438 rows, leaving 562 rows.\n2022-06-17 15:17:55 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 561 rows.\n2022-06-17 15:17:55 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - Person - INFO - created df (0x115fa5310)[person_0]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished person_0 (0x115fa5310) ... 1/2 completed, 561 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on person_1\n2022-06-17 15:17:55 - Person - WARNING - Requiring non-null values in gender_concept_id removed 565 rows, leaving 435 rows.\n2022-06-17 15:17:55 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - Person - INFO - created df (0x115fec4f0)[person_1]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished person_1 (0x115fec4f0) ... 2/2 completed, 435 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - called save_dateframe but outputs are not defined. save_files: True\n</code>\n</pre> <pre>\n<code>could not convert string to float: 'na'\ncould not convert string to float: 'na'\n</code>\n</pre> <pre>\n<code>2022-06-17 15:17:55 - ExampleDataset - INFO - finalised person on iteration 0 producing 996 rows from 2 tables\n2022-06-17 15:17:55 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:55 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:17:55 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:17:55 - ExampleDataset - INFO - for condition_occurrence: found 12 objects\n2022-06-17 15:17:55 - ExampleDataset - INFO - working on condition_occurrence\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_0\n2022-06-17 15:17:55 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Symptoms.csv' for the first time\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 55 rows, leaving 275 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 274 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x115fec340)[condition_occurrence_0]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_0 (0x115fec340) ... 1/12 completed, 274 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_1\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 95 rows, leaving 235 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 234 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x11603e5b0)[condition_occurrence_1]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_1 (0x11603e5b0) ... 2/12 completed, 234 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_10\n2022-06-17 15:17:55 - LocalDataCollection - INFO - Retrieving initial dataframe for 'GP_Records.csv' for the first time\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1738 rows, leaving 214 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x11628fd30)[condition_occurrence_10]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_10 (0x11628fd30) ... 3/12 completed, 214 rows\n2022-06-17 15:17:55 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:55 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:55 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:55 - ExampleDataset - ERROR - 213/214 were good, 1 studies are removed.\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_11\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1822 rows, leaving 130 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x1162a1c70)[condition_occurrence_11]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_11 (0x1162a1c70) ... 4/12 completed, 130 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_2\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 195 rows, leaving 135 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 134 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x11601b5b0)[condition_occurrence_2]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_2 (0x11601b5b0) ... 5/12 completed, 134 rows\n2022-06-17 15:17:55 - ExampleDataset - INFO - starting on condition_occurrence_3\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 100 rows, leaving 230 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 229 rows.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:55 - ConditionOccurrence - INFO - created df (0x1162b97f0)[condition_occurrence_3]\n2022-06-17 15:17:55 - ExampleDataset - INFO - finished condition_occurrence_3 (0x1162b97f0) ... 6/12 completed, 229 rows\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_4\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 265 rows, leaving 65 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x1162a11f0)[condition_occurrence_4]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_4 (0x1162a11f0) ... 7/12 completed, 65 rows\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_5\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 295 rows, leaving 35 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_start_datetime removed 1 rows, leaving 34 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x1162f20a0)[condition_occurrence_5]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_5 (0x1162f20a0) ... 8/12 completed, 34 rows\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_6\n2022-06-17 15:17:56 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n</code>\n</pre> <pre>\n<code>2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1029 rows, leaving 171 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x1162f2850)[condition_occurrence_6]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_6 (0x1162f2850) ... 9/12 completed, 171 rows\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_7\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x1162f2c70)[condition_occurrence_7]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_7 (0x1162f2c70) ... 10/12 completed, 444 rows\n2022-06-17 15:17:56 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:56 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:56 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:56 - ExampleDataset - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_8\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1508 rows, leaving 444 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x1162d3400)[condition_occurrence_8]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_8 (0x1162d3400) ... 11/12 completed, 444 rows\n2022-06-17 15:17:56 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:56 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:56 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:56 - ExampleDataset - ERROR - 441/444 were good, 3 studies are removed.\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on condition_occurrence_9\n2022-06-17 15:17:56 - ConditionOccurrence - WARNING - Requiring non-null values in condition_concept_id removed 1688 rows, leaving 264 rows.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - ConditionOccurrence - INFO - created df (0x116322ca0)[condition_occurrence_9]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished condition_occurrence_9 (0x116322ca0) ... 12/12 completed, 264 rows\n2022-06-17 15:17:56 - ExampleDataset - ERROR - Removed 2 row(s) due to duplicates found when merging condition_occurrence\n2022-06-17 15:17:56 - ExampleDataset - WARNING - Example duplicates...\n2022-06-17 15:17:56 - ExampleDataset - WARNING -                          person_id  condition_concept_id condition_start_date  \\\ncondition_occurrence_id                                                         \n38                           125.0                378253           2020-04-11   \n40                           125.0                378253           2020-04-11   \n308                          125.0               4223659           2020-04-11   \n310                          125.0               4223659           2020-04-11   \n\n                           condition_start_datetime condition_end_date  \\\ncondition_occurrence_id                                                  \n38                       2020-04-11 00:00:00.000000         2020-04-11   \n40                       2020-04-11 00:00:00.000000         2020-04-11   \n308                      2020-04-11 00:00:00.000000         2020-04-11   \n310                      2020-04-11 00:00:00.000000         2020-04-11   \n\n                             condition_end_datetime condition_source_value  \\\ncondition_occurrence_id                                                      \n38                       2020-04-11 00:00:00.000000                    Yes   \n40                       2020-04-11 00:00:00.000000                    Yes   \n308                      2020-04-11 00:00:00.000000                    Yes   \n310                      2020-04-11 00:00:00.000000                    Yes   \n\n                         condition_source_concept_id  \ncondition_occurrence_id                               \n38                                            378253  \n40                                            378253  \n308                                          4223659  \n310                                          4223659  \n2022-06-17 15:17:56 - ExampleDataset - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:17:56 - ExampleDataset - INFO - finalised condition_occurrence on iteration 0 producing 2630 rows from 12 tables\n2022-06-17 15:17:56 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:56 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:17:56 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:17:56 - ExampleDataset - INFO - for drug_exposure: found 5 objects\n2022-06-17 15:17:56 - ExampleDataset - INFO - working on drug_exposure\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on drug_exposure_0\n2022-06-17 15:17:56 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Vaccinations.csv' for the first time\n2022-06-17 15:17:56 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 15:17:56 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - DrugExposure - INFO - created df (0x1163222b0)[drug_exposure_0]\n2022-06-17 15:17:56 - ExampleDataset - INFO - finished drug_exposure_0 (0x1163222b0) ... 1/5 completed, 245 rows\n2022-06-17 15:17:56 - ExampleDataset - INFO - starting on drug_exposure_1\n2022-06-17 15:17:56 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 494 rows, leaving 226 rows.\n2022-06-17 15:17:56 - DrugExposure - WARNING - Requiring non-null values in drug_exposure_start_datetime removed 1 rows, leaving 225 rows.\n2022-06-17 15:17:56 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:17:56 - DrugExposure - INFO - created df (0x11634ab20)[drug_exposure_1]\n</code>\n</pre> <pre>\n<code>2022-06-17 15:17:56 - ExampleDataset - INFO - finished drug_exposure_1 (0x11634ab20) ... 2/5 completed, 225 rows\n2022-06-17 15:17:56 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:56 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:56 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:56 - ExampleDataset - ERROR - 224/225 were good, 1 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on drug_exposure_2\n2022-06-17 15:17:57 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 15:17:57 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - DrugExposure - INFO - created df (0x1163752e0)[drug_exposure_2]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished drug_exposure_2 (0x1163752e0) ... 3/5 completed, 249 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:57 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:57 - ExampleDataset - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on drug_exposure_3\n2022-06-17 15:17:57 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 475 rows, leaving 245 rows.\n2022-06-17 15:17:57 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - DrugExposure - INFO - created df (0x116375700)[drug_exposure_3]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished drug_exposure_3 (0x116375700) ... 4/5 completed, 245 rows\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on drug_exposure_4\n2022-06-17 15:17:57 - DrugExposure - WARNING - Requiring non-null values in drug_concept_id removed 471 rows, leaving 249 rows.\n2022-06-17 15:17:57 - DrugExposure - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - DrugExposure - INFO - created df (0x1163b6610)[drug_exposure_4]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished drug_exposure_4 (0x1163b6610) ... 5/5 completed, 249 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:57 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:57 - ExampleDataset - ERROR - 248/249 were good, 1 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:17:57 - ExampleDataset - INFO - finalised drug_exposure on iteration 0 producing 1210 rows from 5 tables\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:57 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:17:57 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:17:57 - ExampleDataset - INFO - for observation: found 4 objects\n2022-06-17 15:17:57 - ExampleDataset - INFO - working on observation\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on observation_0\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 15:17:57 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - Observation - INFO - created df (0x1163e8ee0)[observation_0]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished observation_0 (0x1163e8ee0) ... 1/4 completed, 413 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:57 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:57 - ExampleDataset - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on observation_1\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Hospital_Visit.csv' for the first time\n2022-06-17 15:17:57 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 937 rows, leaving 263 rows.\n2022-06-17 15:17:57 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - Observation - INFO - created df (0x1163e8e20)[observation_1]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished observation_1 (0x1163e8e20) ... 2/4 completed, 263 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:57 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:57 - ExampleDataset - ERROR - 262/263 were good, 1 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on observation_2\n2022-06-17 15:17:57 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 1023 rows, leaving 177 rows.\n2022-06-17 15:17:57 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - Observation - INFO - created df (0x116429dc0)[observation_2]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished observation_2 (0x116429dc0) ... 3/4 completed, 177 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Either they are not in the original data, or while creating the person table, \n</code>\n</pre> <pre>\n<code>2022-06-17 15:17:57 - ExampleDataset - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:57 - ExampleDataset - ERROR - 176/177 were good, 1 studies are removed.\n2022-06-17 15:17:57 - ExampleDataset - INFO - starting on observation_3\n2022-06-17 15:17:57 - Observation - WARNING - Requiring non-null values in observation_concept_id removed 851 rows, leaving 349 rows.\n2022-06-17 15:17:57 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:17:57 - Observation - INFO - created df (0x111fd1d00)[observation_3]\n2022-06-17 15:17:57 - ExampleDataset - INFO - finished observation_3 (0x111fd1d00) ... 4/4 completed, 349 rows\n2022-06-17 15:17:57 - ExampleDataset - ERROR - Removed 1 row(s) due to duplicates found when merging observation\n2022-06-17 15:17:57 - ExampleDataset - WARNING - Example duplicates...\n2022-06-17 15:17:57 - ExampleDataset - WARNING -                 person_id  observation_concept_id observation_date  \\\nobservation_id                                                       \n440                 110.0                 4059317       2019-07-07   \n441                 110.0                 4059317       2019-07-07   \n\n                      observation_datetime observation_source_value  \\\nobservation_id                                                        \n440             2019-07-07 00:00:00.000000             Heart Attack   \n441             2019-07-07 00:00:00.000000             Heart Attack   \n\n                observation_source_concept_id  \nobservation_id                                 \n440                                   4059317  \n441                                   4059317  \n2022-06-17 15:17:57 - ExampleDataset - INFO - called save_dateframe but outputs are not defined. save_files: True\n2022-06-17 15:17:57 - ExampleDataset - INFO - finalised observation on iteration 0 producing 1197 rows from 4 tables\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:57 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre><code>instance.keys()\n</code></pre> <pre>\n<code>dict_keys(['person', 'condition_occurrence', 'drug_exposure', 'observation'])</code>\n</pre> <pre><code>instance['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id observation_id 1 357 4288455 2020-10-03 2020-10-03 00:00:00.000000 17.172114692899758 4288455 2 258 4288455 2020-11-02 2020-11-02 00:00:00.000000 201.93861878809216 4288455 4 556 4288455 2021-07-26 2021-07-26 00:00:00.000000 11.506250956970998 4288455 5 380 4288455 2021-10-29 2021-10-29 00:00:00.000000 2.6594057121417487 4288455 6 415 4288455 2021-09-07 2021-09-07 00:00:00.000000 40.844873593089126 4288455 ... ... ... ... ... ... ... 1193 988 40757663 2020-07-21 2020-07-21 00:00:00.000000 Cancer 40757663 1194 555 40757663 2020-10-03 2020-10-03 00:00:00.000000 Cancer 40757663 1195 992 40757663 2021-06-20 2021-06-20 00:00:00.000000 Cancer 40757663 1196 992 40757663 2019-05-13 2019-05-13 00:00:00.000000 Cancer 40757663 1197 992 40757663 2019-08-25 2019-08-25 00:00:00.000000 Cancer 40757663 <p>1196 rows \u00d7 6 columns</p> <pre><code># %load ExampleDatasetModified.py\nfrom carrot.cdm import define_person, define_condition_occurrence, define_visit_occurrence, define_measurement, define_observation, define_drug_exposure\nfrom carrot.cdm import CommonDataModel\nfrom carrot.tools import load_csv,create_csv_store\nimport json\nimport glob\nimport pandas as pd\n\nclass ExampleDatasetModified(CommonDataModel):\n\n    def __init__(self,**kwargs):\n        \"\"\" \n        initialise the inputs and setup indexing \n        \"\"\"\n        inputs = load_csv(glob.glob('../data/part1/*'))\n        outputs = create_csv_store(output_folder=\"./data_tests/\",\n                                                   sep=\"\\t\",\n                                                   write_separate=True,\n                                                   write_mode='w')\n\n        super().__init__(inputs=inputs,outputs=outputs,**kwargs)\n        self.process()\n\n    @define_person\n    def person_0(self):\n        \"\"\"\n        Create CDM object for person\n        \"\"\"\n        self.birth_datetime.series = self.inputs[\"Demographics.csv\"][\"Age\"]\n        self.gender_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_concept_id.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.gender_source_value.series = self.inputs[\"Demographics.csv\"][\"Sex\"]\n        self.person_id.series = self.inputs[\"Demographics.csv\"][\"ID\"]\n\n        # --- insert field operations --- \n        self.birth_datetime.series = self.tools.get_datetime_from_age(self.birth_datetime.series)\n\n        # --- insert term mapping --- \n        self.gender_concept_id.series = self.gender_concept_id.series.map(\n            {\n                \"Male\": 8507,\n                \"Female\": 8532\n            }\n        )\n\n    @define_observation\n    def observation_0(self):\n        \"\"\"\n        Create CDM object for observation\n        \"\"\"\n\n        def convert_igg(x):\n            \"\"\"\n            A custom function to convert the IgG into g/L\n            \"\"\"\n            try:\n                igg = float(x['IgG'])\n            except:\n                return None\n            #example of a dataset where the assay has been recalibrated after a certain date\n            #therefore you might need to do some conversion based upon the date\n            factor = 1.2 if x['Date'].year &amp;lt; 2021 else 1\n\n            #apply a factor to convert to g/L\n            factor = factor * 10\n\n            #return the modified IgG value\n            return igg*factor\n\n        #save the source value of the IgG\n        self.observation_source_value.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n\n        #convert the date into a datetime object\n        self.inputs[\"Serology.csv\"][\"Date\"] =  pd.to_datetime(self.inputs[\"Serology.csv\"][\"Date\"],\n                                                             errors='coerce')\n\n        #recalculate the IgG based upon a custom function\n        self.inputs[\"Serology.csv\"][\"IgG\"] = self.inputs[\"Serology.csv\"].apply(\n                                                            lambda x: convert_igg(x),axis=1)\n        #set the output units\n        self.inputs[\"Serology.csv\"][\"Units\"] = 'g/L'\n\n        #set additional columns we did not have before...\n        self.unit_source_value.series = self.inputs[\"Serology.csv\"][\"Units\"]\n        self.value_as_number.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n\n\n        self.observation_concept_id.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n        self.observation_datetime.series = self.inputs[\"Serology.csv\"][\"Date\"]\n        self.observation_source_concept_id.series = self.inputs[\"Serology.csv\"][\"IgG\"]\n        self.person_id.series = self.inputs[\"Serology.csv\"][\"ID\"]\n\n\n        # --- insert term mapping --- \n        self.observation_concept_id.series = self.tools.make_scalar(self.observation_concept_id.series,4288455)\n        self.observation_source_concept_id.series = self.tools.make_scalar(self.observation_source_concept_id.series,4288455)\n</code></pre> <pre><code>instance = ExampleDatasetModified()\ninstance\n</code></pre> <pre>\n<code>2022-06-17 15:17:57 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Blood_Test.csv [&lt;carrot.io.common.DataBrick object at 0x116322730&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Demographics.csv [&lt;carrot.io.common.DataBrick object at 0x111f1ba90&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  GP_Records.csv [&lt;carrot.io.common.DataBrick object at 0x111f56d60&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Hospital_Visit.csv [&lt;carrot.io.common.DataBrick object at 0x111f56c10&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Serology.csv [&lt;carrot.io.common.DataBrick object at 0x116458a00&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Symptoms.csv [&lt;carrot.io.common.DataBrick object at 0x116458f40&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  Vaccinations.csv [&lt;carrot.io.common.DataBrick object at 0x1163e86a0&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Registering  pks.csv [&lt;carrot.io.common.DataBrick object at 0x1163e8b80&gt;]\n2022-06-17 15:17:57 - LocalDataCollection - INFO - DataCollection Object Created\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - CommonDataModel (5.3.1) created with co-connect-tools version 0.0.0\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Running with an DataCollection object\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Turning on automatic cdm column filling\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Added observation_0 of type observation\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Added person_0 of type person\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Starting processing in order: ['person', 'observation']\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - Number of objects to process for each table...\n{\n      \"observation\": 1,\n      \"person\": 1\n}\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - for person: found 1 object\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - working on person\n2022-06-17 15:17:57 - ExampleDatasetModified - INFO - starting on person_0\n2022-06-17 15:17:57 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Demographics.csv' for the first time\n2022-06-17 15:17:57 - Person - WARNING - Requiring non-null values in gender_concept_id removed 3 rows, leaving 997 rows.\n2022-06-17 15:17:57 - Person - WARNING - Requiring non-null values in birth_datetime removed 1 rows, leaving 996 rows.\n2022-06-17 15:17:57 - Person - INFO - Automatically formatting data columns.\n2022-06-17 15:17:58 - Person - INFO - created df (0x11654bcd0)[person_0]\n</code>\n</pre> <pre>\n<code>could not convert string to float: 'na'\n</code>\n</pre> <pre>\n<code>2022-06-17 15:17:58 - ExampleDatasetModified - INFO - finished person_0 (0x11654bcd0) ... 1/1 completed, 996 rows\n2022-06-17 15:17:58 - LocalDataCollection - INFO - saving person_ids.0x1164a5d30.2022-06-17T141758 to ./data_tests//person_ids.0x1164a5d30.2022-06-17T141758.tsv\n2022-06-17 15:17:58 - LocalDataCollection - INFO - finished save to file\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - saving dataframe (0x1164852e0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x1164585e0&gt;\n2022-06-17 15:17:58 - LocalDataCollection - INFO - saving person.person_0.0x1164852e0.2022-06-17T141758 to ./data_tests//person.person_0.0x1164852e0.2022-06-17T141758.tsv\n2022-06-17 15:17:58 - LocalDataCollection - INFO - finished save to file\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - finalised person on iteration 0 producing 996 rows from 1 tables\n2022-06-17 15:17:58 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:58 - LocalDataCollection - INFO - All input files for this object have now been used.\n2022-06-17 15:17:58 - LocalDataCollection - INFO - resetting used bricks\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - for observation: found 1 object\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - working on observation\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - starting on observation_0\n2022-06-17 15:17:58 - LocalDataCollection - INFO - Retrieving initial dataframe for 'Serology.csv' for the first time\n2022-06-17 15:17:58 - Observation - WARNING - Requiring non-null values in observation_datetime removed 2 rows, leaving 413 rows.\n2022-06-17 15:17:58 - Observation - INFO - Automatically formatting data columns.\n2022-06-17 15:17:58 - Observation - INFO - created df (0x1164a5af0)[observation_0]\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - finished observation_0 (0x1164a5af0) ... 1/1 completed, 413 rows\n2022-06-17 15:17:58 - ExampleDatasetModified - ERROR - There are person_ids in this table that are not in the output person table!\n2022-06-17 15:17:58 - ExampleDatasetModified - ERROR - Either they are not in the original data, or while creating the person table, \n2022-06-17 15:17:58 - ExampleDatasetModified - ERROR - studies have been removed due to lack of required fields, such as birthdate.\n2022-06-17 15:17:58 - ExampleDatasetModified - ERROR - 410/413 were good, 3 studies are removed.\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - saving dataframe (0x11659e9d0) to &lt;carrot.io.plugins.local.LocalDataCollection object at 0x1164585e0&gt;\n2022-06-17 15:17:58 - LocalDataCollection - INFO - saving observation.observation_0.0x11659e9d0.2022-06-17T141758 to ./data_tests//observation.observation_0.0x11659e9d0.2022-06-17T141758.tsv\n2022-06-17 15:17:58 - LocalDataCollection - INFO - finished save to file\n2022-06-17 15:17:58 - ExampleDatasetModified - INFO - finalised observation on iteration 0 producing 410 rows from 1 tables\n2022-06-17 15:17:58 - LocalDataCollection - INFO - Getting next chunk of data\n2022-06-17 15:17:58 - LocalDataCollection - INFO - All input files for this object have now been used.\n</code>\n</pre> <pre>\n<code>&lt;__main__.ExampleDatasetModified at 0x116458b20&gt;</code>\n</pre> <pre><code>instance.keys()\n</code></pre> <pre>\n<code>dict_keys(['person', 'observation'])</code>\n</pre> <pre><code>instance['observation'].dropna(axis=1)\n</code></pre> person_id observation_concept_id observation_date observation_datetime observation_source_value observation_source_concept_id unit_source_value observation_id 1 650 4288455 2020-10-03 2020-10-03 00:00:00.000000 17.172114692899758 4288455 g/L 2 457 4288455 2020-11-02 2020-11-02 00:00:00.000000 201.93861878809216 4288455 g/L 3 983 4288455 2021-07-26 2021-07-26 00:00:00.000000 11.506250956970998 4288455 g/L 4 696 4288455 2021-10-29 2021-10-29 00:00:00.000000 2.6594057121417487 4288455 g/L 5 751 4288455 2021-09-07 2021-09-07 00:00:00.000000 40.844873593089126 4288455 g/L ... ... ... ... ... ... ... ... 409 187 4288455 2022-11-07 2022-11-07 00:00:00.000000 51.77573831029082 4288455 g/L 410 886 4288455 2022-09-07 2022-09-07 00:00:00.000000 57.11515081936336 4288455 g/L 411 50 4288455 2022-11-07 2022-11-07 00:00:00.000000 15.264660709568151 4288455 g/L 412 260 4288455 2019-11-13 2019-11-13 00:00:00.000000 26.051354325968106 4288455 g/L 413 370 4288455 2020-05-25 2020-05-25 00:00:00.000000 4.266438928364172 4288455 g/L <p>410 rows \u00d7 7 columns</p>"},{"location":"CaRROT-CDM/notebooks/Part%208%20-%20Create%20PyConfig/#manually-edited","title":"Manually edited","text":"<p>By generating a python class from the rules files, you can manually edit the python file setting up i/o as well as making some edits to the various tables. Once done, it could simple be run as a python file: <pre><code>python  ExampleDatasetModified.py\n</code></pre></p>"},{"location":"CaRROT-CDM/notebooks/Part%209%20-%20Analysis/","title":"Part 9   Analysis","text":"<p>The CommonDataModel has features to join and filter the CDM tables it contains, which can be used for analysis</p> <pre><code>import matplotlib.pyplot as plt\nimport carrot\ncarrot.params['debug_level'] = 0\nimport glob\nimport p\n</code></pre> <pre><code>inputs = carrot.tools.load_tsv(glob.glob(\"temp/1/*.tsv\"),dtype=None)\ncdm = carrot.cdm.CommonDataModel.load(inputs=inputs,\n                                         do_mask_person_id=False,\n                                         format_level=1)\ncdm['person'].dropna(axis=1)\n</code></pre> gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime gender_source_value gender_source_concept_id person_id 996 8507 1963 7 16 1963-07-16 00:00:00.000000 Male 8507 997 8507 1969 7 14 1969-07-14 00:00:00.000000 Male 8507 998 8507 1956 7 17 1956-07-17 00:00:00.000000 Male 8507 999 8507 1960 7 16 1960-07-16 00:00:00.000000 Male 8507 1000 8507 1962 7 16 1962-07-16 00:00:00.000000 Male 8507 ... ... ... ... ... ... ... ... 1987 8532 1995 7 8 1995-07-08 00:00:00.000000 Female 8532 1988 8532 1956 7 17 1956-07-17 00:00:00.000000 Female 8532 1989 8532 1944 7 20 1944-07-20 00:00:00.000000 Female 8532 1990 8532 1966 7 15 1966-07-15 00:00:00.000000 Female 8532 1991 8532 1974 7 13 1974-07-13 00:00:00.000000 Female 8532 <p>996 rows \u00d7 7 columns</p> <pre><code>model = cdm.filter({\n    'person':{\n                'gender_concept_id': lambda x: x == 8507,\n                'year_of_birth': lambda x: x &amp;lt; 1970\n            },\n    'observation':{\n                'observation_concept_id':lambda x: x == 4288455,\n                  }\n     }\n    )\n\ndf = model['person'].merge(model['observation'],on='person_id')\ndf\n</code></pre> person_id gender_concept_id year_of_birth month_of_birth day_of_birth birth_datetime race_concept_id ethnicity_concept_id location_id provider_id_x ... value_as_string value_as_concept_id qualifier_concept_id unit_concept_id provider_id_y visit_occurrence_id observation_source_value observation_source_concept_id unit_source_value qualifier_source_value 0 999 8507 1960 7 16 1960-07-16 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 11.357798214040226 4288455 NaN NaN 1 1012 8507 1935 7 23 1935-07-23 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 33.04527479891782 4288455 NaN NaN 2 1021 8507 1956 7 17 1956-07-17 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 14.100366588058625 4288455 NaN NaN 3 1021 8507 1956 7 17 1956-07-17 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN i14.100366588058625 4288455 NaN NaN 4 1021 8507 1956 7 17 1956-07-17 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 69.98379377754527 4288455 NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 131 1540 8507 1968 7 14 1968-07-14 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 2.255564942453296 4288455 NaN NaN 132 1540 8507 1968 7 14 1968-07-14 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 3.797943588710728 4288455 NaN NaN 133 1540 8507 1968 7 14 1968-07-14 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 32.809025248054155 4288455 NaN NaN 134 1545 8507 1955 7 18 1955-07-18 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 17.863750269708095 4288455 NaN NaN 135 1551 8507 1941 7 21 1941-07-21 00:00:00.000000 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN 11.506250956970998 4288455 NaN NaN <p>136 rows \u00d7 33 columns</p> <pre><code>pd.to_numeric(df['observation_source_value'], errors='coerce')#.plot.hist()\n#plt.xlabel('IgG Level')\n#plt.yscale('log')\n#plt.show();\n</code></pre> <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-22-84e83dc1ae13&gt; in &lt;module&gt;\n----&gt; 1 pd.to_numeric(df['observation_source_value'], errors='coerce')#.plot.hist()\n      2 #plt.xlabel('IgG Level')\n      3 #plt.yscale('log')\n      4 #plt.show();\n\nNameError: name 'pd' is not defined</pre> <pre><code>import scipy.stats\n</code></pre> <pre><code>def get_serology_analysis(_filter):\n    def serology_analysis(self):\n        df = self.filter(_filter)\n        age = 2022 - df['year_of_birth']\n        igg = df['observation_source_value'].astype(float)\n        res = scipy.stats.linregress(x=age,y=igg)\n        return res\n    return serology_analysis\n\nana = get_serology_analysis({'person':{\n                'gender_concept_id': lambda x: x == 8507,\n                'year_of_birth': lambda x: x &amp;lt; 1970\n            },\n            'observation':{\n                'observation_concept_id':lambda x: x == 4288455,\n             }\n             })\nana\n</code></pre> <pre><code>cdm.run_analysis(ana)\n</code></pre> <pre><code>year_of_birth = [(x,x+20) for x in range(1930,2020,20)]\nyear_of_birth\n</code></pre> <pre><code>genders = [8507,8532]\n</code></pre> <pre><code>carrot.params['debug_level'] = 2\n</code></pre> <pre><code>from itertools import product\n\nresults = {}\nfor ((year_low,year_high),gender) in product(year_of_birth,genders):\n    config = {'person':{\n                'gender_concept_id': lambda x,gender=gender: x == gender,\n                'year_of_birth': lambda x,year_high=year_high,year_low=year_low: ((x &amp;lt; year_high) &amp;amp; (x &amp;gt; year_low))\n            },\n            'observation':{\n                'observation_concept_id':lambda x : x == 4288455,\n            }}\n\n    ana = get_serology_analysis(config)\n    name = f'age_{year_low}_gender_{gender}'\n    cdm.add_analysis(ana,name)\n</code></pre> <pre><code>results = cdm.run_analyses()\nresults\n</code></pre> <pre><code>import numpy as np\n</code></pre> <pre><code>x = np.array([i for i in range(0,500)])\nfor label,res in sorted(results.items()):\n    plt.plot(x, res.intercept + res.slope*x, label=label)\nplt.legend(bbox_to_anchor=(1.05, 1))\nplt.show()\n</code></pre> <pre><code>config = {'person':{\n                'year_of_birth': lambda x: ((x &amp;lt; 2030) &amp;amp; (x &amp;gt; 2010))\n            },\n            'observation':{\n                'observation_concept_id':lambda x : x == 4288455,\n            }}\ndf = cdm.filter(config)\ndf['observation_source_value'].astype(float).plot.hist()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"CaRROT-CDM/source_code/","title":"Index","text":"<p>All documentation relating to the CaRROT-CDM builder can be found here:</p>"},{"location":"CaRROT-CDM/source_code/#httpshdrukgithubiocarrot-docscarrot-cdmabout","title":"https://hdruk.github.io/CaRROT-Docs/CaRROT-CDM/About/","text":""},{"location":"CaRROT-CDM/source_code/QuickStart/","title":"QuickStart","text":""},{"location":"CaRROT-CDM/source_code/QuickStart/#transforming-data-to-cdm","title":"Transforming data to CDM","text":""},{"location":"CaRROT-CDM/source_code/QuickStart/#1-install-the-co-connect-tools-python-package","title":"1. Install the co-connect-tools python package","text":"<p>This package runs with <code>python</code> versions <code>&gt;=3.6</code>, the easiest way to install is via pip: <pre><code>$ python3 -m pip install co-connect-tools\n</code></pre></p> <p>Warning</p> <p>The tool is stable on (latest) Unix releases such as MacOS, Ubuntu, Centos. If you are using Windows, you may encounter problems.</p> <p>If you are struggling to install from <code>pip</code> due to permissions, you can install as a user via: <pre><code>$  python3 -m pip install co-connect-tools --user\n</code></pre> Or, you can download the source code from https://github.com/CO-CONNECT/co-connect-tools/tags, unpack and install as a local package: <pre><code>$ cd &lt; downloaded source code folder &gt;\n$ python3 -m pip install -e . \n</code></pre></p> <p>Note</p> <p>If you have trouble with <code>pip</code> hanging on installing dependencies, try to install also using the argument <code>--no-cache-dir</code>. Also make sure that you have updated <code>pip</code> via <code>pip3 install --upgrade pip</code>.</p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#2-checking-the-package","title":"2. Checking the package","text":"<p>To verify the package is installed you can test the following information commands: <pre><code>$ coconnect info version\n&lt;tool version in format X.Y.Z&gt;\n\n$ coconnect info install_folder\n&lt;path to install folder&gt;\n</code></pre></p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#3-gather-inputs","title":"3. Gather inputs","text":"<p>To run the transformation to CDM you will need: 1. Input Data 2. <code>json</code> file containing the so-called mapping rules</p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#file-checks","title":"File checks","text":"<p>Input data is expected in <code>csv</code> format.</p> <p>It is possible to do a quick check to display the first 10 rows of an input <code>csv</code>. Run: <pre><code>$ coconnect display dataframe --head 10 data/file_0.csv\n&lt;displays dataframe&gt;\n</code></pre></p> <p>With your <code>json</code> file for the rules, you can quickly check the tool is able to read and display them via: <pre><code>$ coconnect display json rules.json\n</code></pre></p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#4-run-the-tool","title":"4. Run the tool","text":"<p><pre><code>$ coconnect map run --name &lt;Name&gt; --rules &lt;.json file for rules&gt; &lt;csv files&gt;\n</code></pre> E.g.: <pre><code>$ coconnect map run --name TestData --rules rules.json data/*.csv\n</code></pre></p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#5-check-the-output","title":"5. Check the output","text":"<p>By default, mapped <code>csv</code> files are created in the folder <code>output_data</code> within your current working directory.</p> <p>Note</p> <p>To specify a different output folder, use the command line argument <code>--output-folder</code> when running <code>coconnect map run</code></p> <p>Additionally, log files are created in a subdirectory of the output folder, for example: <pre><code>output_data/\n\u251c\u2500\u2500 condition_occurrence.csv\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2021-07-19T100054.json\n\u2514\u2500\u2500 observation.csv\n</code></pre></p> <p>Other than opening up the output csv in your favourite viewer, you can also use the command line tools to display a simple dataframe <pre><code>$ coconnect display dataframe --drop-na output_data/condition_occurrence.csv \n       condition_occurrence_id  person_id  condition_concept_id  ... condition_end_datetime condition_source_value  condition_source_concept_id\n0                            1          9                312437  ...    2020-04-10 00:00:00                      1                       312437\n1                            2         18                312437  ...    2020-04-11 00:00:00                      1                       312437\n2                            3         28                312437  ...    2020-04-10 00:00:00                      1                       312437\n3                            4         38                312437  ...    2020-04-10 00:00:00                      1                       312437\n4                            5         44                312437  ...    2020-04-10 00:00:00                      1                       312437\n</code></pre></p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#common-problems","title":"Common Problems","text":"<p>The following documents common error messages and FAQs.</p>"},{"location":"CaRROT-CDM/source_code/QuickStart/#why-do-i-get-a-keyerror-message-about-my-input-data","title":"Why do I get a <code>KeyError</code> message about my input data?","text":"<p>You may see a look up error like this, when running the tool: <pre><code>    self.observation_concept_id.series = self.inputs[\"FILE0.csv\"][\"column_name_0\"]\nKeyError: 'FILE0.csv'\n</code></pre></p> <p>This tells you that when running the tool you have not supplied the input file <code>FILE0.csv</code>.</p> <p>It is crucial that the names of the files encocded in the <code>json</code> file match those that are supplied to the tool, otherwise the tool cannot run.</p> <p>A common problem is the capitalisation of the file names, or the file names in the <code>json</code> file missing a <code>.csv</code> extension.</p> <p>In other words, whatever file names are used in the <code>json</code> file under the section <code>\"source_table\":</code> must be supplied as inputs.</p>"},{"location":"CaRROT-CDM/source_code/carrot/data/cdm/BCLINK_EXPORT/5.3.1/notes/","title":"Notes","text":"<pre><code>datasettool2 export-form PROCEDURE_OCCURRENCE --database bclink\n</code></pre>"},{"location":"CaRROT-CDM/source_code/scripts/","title":"Helper Scripts","text":"<p>This folder contains scripts showing examples of how you can implement your own script to use the python CommonDataModel to build a model and execute a transformation of a dataset into the OHDSI CDM.</p>"},{"location":"CaRROT-CDM/source_code/scripts/#etlcdmpy","title":"etlcdm.py","text":"<p>The help message for this script displays: <pre><code>usage: etlcdm.py [-h] --rules RULES --out-dir OUT_DIR --inputs INPUTS [INPUTS ...] [-nc NUMBER_OF_ROWS_PER_CHUNK]\n                 [-np NUMBER_OF_ROWS_TO_PROCESS] [--use-profiler]\n\nETL-CDM: transform a dataset into a CommonDataModel.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --rules RULES         input .json file\n  --out-dir OUT_DIR, -o OUT_DIR\n                        name of the output folder\n  --inputs INPUTS [INPUTS ...], -i INPUTS [INPUTS ...]\n                        input csv files\n  -nc NUMBER_OF_ROWS_PER_CHUNK, --number-of-rows-per-chunk NUMBER_OF_ROWS_PER_CHUNK\n                        choose to chunk running the data into nrows\n  -np NUMBER_OF_ROWS_TO_PROCESS, --number-of-rows-to-process NUMBER_OF_ROWS_TO_PROCESS\n                        the total number of rows to process\n  --use-profiler        turn on saving statistics for profiling CPU and memory usage\n</code></pre></p> <p>For example, to execute this script run: <pre><code>etlcdm.py -i &lt;input file 1&gt; &lt;input file 2&gt; .... &lt;input file N&gt;  --rules &lt;json rules&gt;  -o &lt;location of output folder&gt;\n</code></pre></p>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/","title":"Pseudonymisation","text":"<p>A separate package, to help with pseudonymisation of columns in data files by passing a salt. co-connect-pseudonymise is available to be installed via pypi, or via the source code on GitHub.</p> <p>This is provided as a separate package for those who no not wish (e.g. for security reasons) or need to install the full package (co-connect-tools).</p> <p>Info</p> <p>co-connect-pseudonymise will be installed if the packge co-connect-tools is installed, as it is a dependancy of the latter. </p> <p>co-connect-pseudonymise contains:   </p> <ul> <li>a simple CLI (command line interface) to pseudonymise <code>csv</code> files   </li> <li>a python workbook showing how pseudonymisation can be performed manually   </li> <li>additional example code for C# and Java    </li> </ul>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#installation-via-pip","title":"Installation (via pip)","text":"<p>To install the package, it's recommended to use a python virtual environment, using <code>python &gt;= 3.6.8</code>. It's best to install via <code>pip</code> with a connection to the internet.</p>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#setup-a-virtual-environment","title":"Setup a virtual environment","text":"<p>Navigate to a clean or working directory and setup a virtual environment:</p> <pre><code>python3 -m venv  .\nsource bin/activate\n</code></pre>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#upgrade-pip","title":"Upgrade Pip","text":"<p>You'll first need to upgrade pip to make sure the latest packages can be found: <pre><code>pip install pip --upgrade\n</code></pre></p>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#install-the-package","title":"Install the package","text":"<pre><code>pip install co-connect-pseudonymise\n</code></pre>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#check-the-package","title":"Check the package","text":"<p><pre><code>pseudonymise --help\n</code></pre> Print the message: <pre><code>Usage: pseudonymise [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  csv  Command to pseudonymise csv files, given a salt, the name of the...\n</code></pre></p>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#run-for-pseudonymisation-of-a-csv-file","title":"Run for pseudonymisation of a CSV file","text":""},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#display-help","title":"Display help","text":"<p>To display the help message to see what options are available, run: <pre><code>pseudonymise csv --help \n</code></pre> Which shows: <pre><code>Usage: pseudonymise csv [OPTIONS] INPUT\n\n  Command to pseudonymise csv files, given a salt, the name of the columns to\n  pseudonymise and the input file.\n\nOptions:\n  -s, --salt TEXT           salt hash  [required]\n  -c, --column, --id TEXT   name of the identifier columns  [required]\n  -o, --output-folder TEXT  path of the output folder  [required]\n  --chunksize INTEGER       set the chunksize when loading data, useful for\n                            large files\n  --help                    Show this message and exit.\n</code></pre></p>"},{"location":"CaRROT-Pseudonymise/Pseudonymisation/#execute","title":"Execute","text":"<p>Executing a command to pseudonymise the data in the column <code>PersonID</code> of the file <code>data/Demographics.csv</code> using the salt <code>beb2839bd78</code> and outputing the file to the folder <code>pseudonymised_data/</code>: <pre><code>pseudonymise csv --salt beb2839bd78 --column PersonID  data/Demographics.csv -o pseudonymised_data/\n</code></pre></p> <p>Outputs the following: <pre><code>2022-01-10 09:56:11.155 | INFO     | cli.cli:csv:16 - Working on file Demographics.csv, pseudonymising columns '['PersonID']' with salt 'beb2839bd78'\n2022-01-10 09:56:11.157 | INFO     | cli.cli:csv:22 - Saving new file to pseudonymised_data/Demographics.csv\n2022-01-10 09:56:11.173 | DEBUG    | cli.cli:csv:32 - 0      16dc368a89b428b2485484313ba67a3912ca03f2b2b424...\n1      37834f2f25762f23e1f74a531cbe445db73d6765ebe608...\n2      454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51...\n3      5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f...\n4      1253e9373e781b7500266caa55150e08e210bc8cd8cc70...\n                             ...                        \n995    8352dd9eb8b64669e0a8347fd37ae6e5cd67c817f2b4b1...\n996    235aa062e6372588dbae00552abf36b8ff9c315e3da56c...\n997    4aec429ac0bfafdbb8dab14f41d1b7a98dacf1ce3478b7...\n998    330e14d4ae80612334d94c488d29eb469626b476864abd...\n999    ab9828ca390581b72629069049793ba3c99bb8e5e9e7b9...\nName: PersonID, Length: 1000, dtype: object\n2022-01-10 09:56:11.183 | INFO     | cli.cli:csv:52 - Done with pseudonymised_data/Demographics.csv!\n</code></pre></p>"},{"location":"Carrot-Mapper/","title":"Carrot-Mapper","text":"<p>Carrot-Mapper is a webapp which allows the user  to use the metadata (as output by WhiteRabbit) from a dataset to produce mapping rules to the OMOP standard, in the JSON format. These can be ingested by CaRROT-CDM to perform the mapping of the contents of the dataset to OMOP.</p> <p>Carrot-Mapper provides automated mapping from a selection of vocabularies, reuse of mapping rules across datasets, and manual mapping rule generation.</p> <p>Carrot-Mapper also provides a number of helpful features to support and encourage  standardised mappings across datasets.</p> <p>Carrot-Mapper is built using Python, Django and React, with a PostgreSQL database, and is hosted in Azure using Azure App Services, Azure Database for PostgreSQL,  and Azure Functions.</p> <p></p>"},{"location":"Carrot-Mapper/API/","title":"The Carrot-Mapper API","text":"<p>The Carrot-Mapper API allows programmatic interaction with the Carrot-Mapper database. This API is developed using the Django REST framework.  Access to API endpoints is protected by token based authentication. The REST API is also the means of communication between the Django webapp and its supporting Azure Functions.  </p> <p>This page documents most of the endpoints defined. The only exceptions are some page-view URLs that are not designed for REST access, but rather are for internal use by the webapp.</p> <p>Our development team typically tests API endpoints using the Postman software. </p>"},{"location":"Carrot-Mapper/API/#api-root","title":"API Root","text":"<p>An API root can be accessed using: http://localhost:8080/api and this endpoint lists all the available endpoints in the API (see Figure 1).  Figure 1 also demonstrates that for testing API endpoints, a token is required, which can be requested from the system administrator.   </p> <p> Figure 1 A sample API endpoint testing through Postman</p> <p>Prepend the string <code>http://localhost:8080/api/</code> to each of the defined URLs to access a valid endpoint.  Access to the dev/test/prod systems is similarly via <code>https://ccom-dev.azurewebsites.net/api/</code>, <code>https://ccom-test.azurewebsites.net/api/</code> and <code>https://ccom.azurewebsites.net/api/</code> respectively, with different auth tokens for each.</p>"},{"location":"Carrot-Mapper/API/#filter-fields","title":"Filter fields","text":"<p>Many endpoints support selected filter fields. Here we define the syntax used to express the available filter options for a given endpoint. <pre><code>filter_fields:\nname: exact\n</code></pre> indicates that the filter field <code>?name=ABC</code> can be optionally appended to the URL to filter by the <code>name</code> property.</p> <p>Supplying no filter field will return all results. <pre><code>filter_fields:\n    id: exact, in\n</code></pre> indicates that the filter field <code>?id=ABC</code> can be appended to the URL to filter by the <code>id</code> property, or the filter field <code>?id__in=1,2,3</code> can be appended to return only results with <code>id</code> of either  <code>1</code>, <code>2</code>, or <code>3</code>. Similarly, supplying no filter field will return all results.</p> <p>Multiple filter fields can be defined for a given URL, leading to more complicated examples. Multiple filters are  separated by <code>&amp;</code>, with no restriction on the ordering of the filters, thus <pre><code>filter_fields:\n    id: exact, in\n    name: exact\n</code></pre> allows all of the following combinations: <pre><code>?id=1\n?id__in=1,2,3\n?name=ABC\n?id=1&amp;name=ABC\n?name=ABC&amp;id__in=1,2,3\n</code></pre></p>"},{"location":"Carrot-Mapper/API/#specifying-returning-fields","title":"Specifying returning fields","text":"<p>For some endpoints (those defined using DynamicFieldsMixin), users have an option of specifying return fields. On this page, these endpoints are marked with . If no return fields parameter is supplied, all fields will be returned.</p> <p>Return fields are specified by the syntax <code>?fields=id,name</code>. When combined with filters, the syntax is thus for example <code>?dataset_name=test&amp;fields=id</code>.</p>"},{"location":"Carrot-Mapper/API/#omop-db","title":"OMOP DB","text":"<p>Read-only endpoints exist for 8 tables of the OMOP CDM DB. </p> <ol> <li> <p>Concept table: </p> <ul> <li>omop/concepts/ Returns all records in the <code>concept</code> table.</li> <li>omop/concepts/1/ Returns concept details from the <code>concept</code> table for <code>concept_id=1</code>.</li> <li>omop/conceptsfilter/ <pre><code>filter_fields:\n  concept_id: in, exact\n  concept_code: in, exact\n  vocabulary_id: in, exact\n</code></pre></li> </ul> </li> <li> <p>Concept_ancestor table: </p> <ul> <li>omop/conceptancestors/ Returns all records from the <code>concept_ancestor</code> table</li> <li>omop/conceptancestors/262/ Returns all records from the <code>concept_ancestor</code> table with <code>concept_ancestor_id=262</code></li> <li>omop/conceptancestors/ <pre><code>filter_fields:\n  ancestor_concept_id: exact\n  descendant_concept_id: exact\n</code></pre></li> </ul> </li> <li> <p>Concept_class table: </p> <ul> <li>omop/conceptclasses   Returns all records from the <code>concept_class</code> table</li> <li>omop/conceptclasses/10<sup>th</sup>%20level/ Returns all records from <code>concept_class</code> table with <code>concept_class_id='10th level'</code></li> </ul> </li> <li> <p>Concept_relationship table: </p> <ul> <li>omop/conceptrelationships/ Returns all records from the <code>conceptrelationship</code> table   <pre><code>filter_fields:\n  concept_id_1: exact\n  concept_id_2: exact\n  relationship_id: exact\n</code></pre></li> <li>omop/conceptrelationshipfilter/ Returns all records from the <code>conceptrelationship</code> table   <pre><code>filter_fields:\n  concept_id_1: in, exact\n  concept_id_2: in, exact\n  relationship_id: in, exact\n</code></pre></li> </ul> </li> <li> <p>Concept_synonym table: </p> <ul> <li>omop/conceptsynonyms/ Returns all records from the <code>concept_synonym</code> table</li> <li>omop/conceptsynonyms/2/ Returns all records from the <code>concept_synonym</code> table with <code>concept_id=2</code></li> </ul> </li> <li> <p>Domain table: </p> <ul> <li>omop/domains Returns all records from the <code>domain</code> table</li> <li>omop/domains/Condition/   Returns all records from the <code>domain</code> table with <code>domain_id=Condition</code></li> </ul> </li> <li> <p>Drug_strength table: </p> <ul> <li>omop/drugstrengths/   Returns all records from the <code>drug_strength</code> table   <pre><code>filter_fields:\n  drug_concept_id: in, exact\n  ingredient_concept_id: in, exact\n</code></pre></li> </ul> </li> <li> <p>Vocabulary table: </p> <ul> <li>omop/vocabularies/ Returns all records from the <code>vocabulary</code> table</li> <li>omop/vocabularies/Cost/   Returns a record from a <code>vocabulary</code> table with <code>vocabulary_id=Cost</code></li> </ul> </li> </ol>"},{"location":"Carrot-Mapper/API/#carrot","title":"Carrot","text":"<p>Note that many of these endpoints implement user permissions checks, and may restrict or filter results based upon the  user rights associated to the Token provided.</p> <ol> <li> <p>User table</p> <ul> <li>users/ Returns all records from the <code>auth_users</code> table. </li> <li>users/&lt;User_id&gt;/  Returns user details from the <code>auth_users</code> table by <code>id</code>.</li> <li>usersfilter/ <pre><code>filter_fields:\n  id: in, exact\n</code></pre></li> </ul> </li> <li> <p>mapping_scanreport table</p> <ul> <li>scanreports/  Returns all records from the <code>mapping_scanreport</code> table. </li> <li>For this endpoint, making a GET/POST request shows the results. Making a PUT/PATCH/DELETE request allows      for editing.    <pre><code>filter_fields:\n  parent_dataset: exact\n</code></pre></li> <li>scanreports/&lt;ScanReport_id&gt;/  Returns all records from the <code>mapping_scanreport</code> table by <code>id</code>.</li> </ul> </li> <li> <p>mapping_scanreporttable table</p> <ul> <li>scanreporttables/  Returns all records from the <code>mapping_scanreporttables</code> table.   <pre><code>filter_fields:\n  id: in, exact\n  name: in, exact\n  scan_report: in, exact\n</code></pre></li> <li>For this endpoint, making a GET/POST request shows the results. Making a PUT/PATCH/DELETE request allows      for editing.</li> <li>scanreporttables/&lt;ScanReportTable_id&gt;/  Returns all records from the <code>mapping_scanreporttable</code> table by <code>id</code>.</li> </ul> </li> <li> <p>mapping_scanreportfield table</p> <ul> <li>scanreportfields/  Returns all records from the <code>mapping_scanreportfield</code> table.   <pre><code>filter_fields:\n  id: in, exact\n  name: in, exact\n  scan_report_table: in, exact\n</code></pre></li> <li>For this endpoint, making a GET/POST request shows the results. Making a PUT/PATCH/DELETE request allows      for editing.</li> <li>scanreportfields/&lt;ScanReportField_id&gt;/  Returns all records in the <code>mapping_scanreportfields</code> table by <code>id</code>.</li> </ul> </li> <li> <p>mapping_scanreportvalue table</p> <ul> <li>scanreportvalues/  Returns all records from the <code>mapping_scanreportvalues</code> table.    <pre><code>filter_fields:\n  id: in, exact\n  name: in, exact\n  scan_report_field: in, exact\n</code></pre></li> <li>For this endpoint, making a GET/POST request shows the results. Making a PUT/PATCH/DELETE request allows      for editing.</li> <li> <p>scanreportvalues/&lt;ScanReportValue_id&gt;/  Returns all records from the <code>mapping_scanreportvalues</code> table by <code>id</code>.</p> </li> <li> <p>scanreportvaluepks/?scan_report=&lt;ScanReport_id&gt; Returns all records from the <code>mapping_scanreportvalues</code> table    which are linked to the scan report by <code>id</code> through the chain of <code>ScanReportValue -&gt; ScanReportField -&gt; ScanReportTable -&gt; ScanReport</code>, and which    additionally do not have <code>conceptID=-1</code> (which is the default for those without an associated ScanReportConcept).</p> </li> <li>scanreportvaluesfilterscanreporttable  Returns all ScanReportValues associated to the supplied <code>ScanReportTable</code> id.   <pre><code>filter_fields:\n  scan_report_table: exact\n</code></pre></li> </ul> </li> <li> <p>mapping_scanreportconcept table  </p> <ul> <li>scanreportconcepts/  Returns all records from the <code>mapping_scanreportconcepts</code> table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. </li> <li>scanreportconcepts/&lt;ScanReportConcept_id&gt;/  Returns all records from the <code>mapping_scanreportconcepts</code> table by <code>id</code>.</li> <li>scanreportconceptsfilter/  Returns all records from the <code>mapping_scanreportconcepts</code> table, with additional filtering capabilities on <code>concept_id</code>.   <pre><code>filter_fields:\n  id: in, exact\n  object_id: in, exact\n  content_type: in, exact\n  concept_id: in, exact\n</code></pre></li> <li>scanreportactiveconceptfilter/  Returns all records from the <code>mapping_scanreportconcepts</code> table   which (1) are associated to an object with the given <code>content_type</code> (<code>15</code> for <code>ScanReportField</code>, <code>17</code> for <code>ScanReportValue</code> - any other value will return None);   and (2) are associated to a <code>ScanReport</code> which is both active (not hidden) and has Status <code>Mapping Complete</code>.   This endpoint is only available to the Azure Function superuser.   <pre><code>filter_fields:\n  content_type: exact\n</code></pre></li> </ul> </li> <li> <p>mapping_classificationsystem table   </p> <ul> <li>UNUSED! classificationsystems/  Returns all records from the <code>mapping_classificationsystem</code> table.</li> <li>classificationsystems/&lt;ClassificationSystem_id&gt;/  Returns all records from the <code>mapping_classificationsystem</code> table by <code>id</code>.</li> </ul> </li> <li> <p>mapping_datadictionary table </p> <ul> <li>UNUSED! datadictionaries/  Returns all records from the <code>mapping_datadictionary</code> table.</li> <li>datadictionaries/&lt;DataDictionary_id&gt;/  Returns all records from the <code>mapping_datadictionary</code> table by <code>id</code>.</li> </ul> </li> <li> <p>datapartner table    </p> <ul> <li>datapartners/  Returns all records from the <code>datapartner</code> table.  </li> <li>datapartners/&lt;DataPartner_id&gt;/  Returns all records from the <code>datapartner</code> table by <code>id</code>.</li> <li>datapartnersfilter/ <pre><code>filter_fields:\n  name: exact\n</code></pre></li> </ul> </li> <li> <p>mapping_omoptable table </p> <ul> <li>omoptables/ Returns all records from the <code>mapping_omoptable</code> table.</li> <li>omoptables/&lt;OmopTable_id&gt;/ Returns all records from the <code>mapping_omoptable</code> table by <code>id</code>.</li> <li>UNUSED! omoptablesfilter/ <pre><code>filter_fields:\n  id: in, exact\n</code></pre></li> </ul> </li> <li> <p>mapping_omopfield table </p> <ul> <li>omopfields/  Returns all records from the <code>mapping_omopfield</code> table.</li> <li>omopfields/&lt;OmopField_id&gt;/  Returns all records from the <code>mapping_omopfield</code> table by <code>id</code>.</li> <li>UNUSED! omopfieldsfilter/ <pre><code>filter_fields:\n  id: in, exact\n</code></pre></li> </ul> </li> <li> <p>mapping_mappingrule table   </p> <ul> <li>mappingrules/  Returns all records from the <code>mapping_mappingrule</code> table.</li> <li>mappingrules/&lt;MappingRule_id&gt;  Returns all records from the <code>mapping_mappingrule</code> table by <code>id</code>.</li> <li>mappingrulesfilter/ <pre><code>filter_fields:\n  scan_report: in, exact\n  concept: in, exact\n</code></pre></li> <li>mappingruleslist/?id=&lt;ScanReport_id&gt; This returns all mapping rules associated to the <code>ScanReport</code> with <code>id</code>. This requires looking up a number of foreign keys and may be rather slow.</li> <li>This also supports pagination, using the page number parameter <code>p</code> and page size parameter <code>page_size</code>, e.g. <code>mappingruleslist/?id=56&amp;p=1&amp;page_size=30</code> </li> </ul> </li> <li> <p>mapping_dataset table</p> <ul> <li>datasets/  Returns all <code>Datasets</code> which the user is able to view.   <pre><code>filter_fields:\n  id: in, exact\n  data_partner: in, exact\n  hidden: in, exact \n</code></pre></li> <li>datasets/&lt;Dataset_id&gt;  Return a single <code>Dataset</code> by <code>id</code>.</li> <li>datasets/update/&lt;Dataset_id&gt;  Update a single <code>Dataset</code> by <code>id</code>.</li> <li>datasets/delete/&lt;Dataset_id&gt;  Delete a single <code>Dataset</code> by <code>id</code>.</li> <li>datasets/create Create a single <code>Dataset</code>.</li> <li>datasets_data_partners/  Returns all <code>Datasets</code> which the user is able to view, while providing a pagination option, and pre-fetching (for performance) Data Partner information associated to each.   <pre><code>filter_fields:\n  id: in, exact\n  data_partner: in, exact\n  hidden: in, exact \n</code></pre></li> <li>This also supports pagination, using the page number parameter <code>p</code> and page size parameter <code>page_size</code>, e.g. <code>datasets_data_partners/?id=56&amp;p=1&amp;page_size=30</code> </li> </ul> </li> <li> <p>mapping_projects table</p> <ul> <li>projects/  Returns all <code>Projects</code> which the user is able to view.   <pre><code>filter_fields:\n  name: in, exact\n  dataset: exact \n</code></pre></li> <li>projects/&lt;Project_id&gt;  Returns a single <code>Project</code> by <code>id</code>.</li> <li>projects/update/&lt;Project_id&gt;  Update a single <code>Project</code> by <code>id</code>.</li> </ul> </li> </ol>"},{"location":"Carrot-Mapper/API/#count-stats","title":"Count Stats","text":"<ul> <li>countstats/ Returns the total number of <code>ScanReports</code>, <code>ScanReportTables</code>, <code>ScanReportFields</code>, <code>ScanReportValues</code> &amp; <code>MappingRules</code> in the database.</li> <li>countstatsscanreport/?scan_report=&lt;ScanReport_id&gt; Returns the total number of tables, fields, values and mapping rules in a <code>ScanReport</code> by <code>id</code>.</li> <li>countstatsscanreporttable/?scan_report_table=&lt;ScanReportTable_id&gt; Returns the total number of <code>ScanReportFields</code> and <code>ScanReportValues</code> in a <code>ScanReportTable</code> by <code>id</code>.</li> <li>countstatsscanreporttablefield/?scan_report_field=&lt;ScanReportField_id&gt; Returns the total number of <code>ScanReportValues</code> in a <code>ScanReportField</code> by <code>id</code>.</li> </ul>"},{"location":"Carrot-Mapper/API/#assorted-others","title":"Assorted others","text":"<ul> <li> <p>analyse/&lt;ScanReport_id&gt;/  Returns an analysis of the <code>ScanReportConcepts</code> in the <code>ScanReport</code> with <code>id</code>. This analysis compares to all other <code>ScanReports</code>,    finding ancestor and descendants of each, and reporting the results (specifically, any mismatched <code>Concepts</code> found between this <code>ScanReport</code> and any other)</p> </li> <li> <p>countprojects/&lt;Dataset_id&gt;/ Returns the count of distinct projects to which the provided <code>Dataset</code> <code>id</code> is associated.</p> </li> <li> <p>scanreports/&lt;ScanReport_id&gt;/download Returns the <code>ScanReport</code> file in <code>.xlsx</code> format with <code>id</code> from Azure blob storage as an HTTP Response.</p> </li> </ul>"},{"location":"Carrot-Mapper/Adding-a-vocabulary/","title":"Adding a new vocabulary to the database","text":"<p>The <code>omop.vocabulary</code> table holds the recognised vocabularies for use in automated mapping. </p> <p>To add a new vocabulary to the database, 3 tables need updating: <code>omop.vocabulary</code>, <code>omop.concept</code> and <code>omop.concept_relationship</code>.</p> <p>Download the required files from Athena, which should give a file structure like this:</p> <p></p> <p>The only required fiels are <code>CONCEPT_RELATIONSHIP.csv</code>, <code>CONCEPT.csv</code> and <code>VOCABULARY.csv</code>.</p> <p>Open the <code>VOCABULARY.csv</code> file (better to use a text editor than Excel, which will often reformat the file unhelpfully), and remove everything except the header and the line for the required vocabulary, then save the file.</p> <p>In PGAdmin, right-click on the <code>omop.vocabulary</code> table &gt; Import/Export Data. Set the Import/Export toggle to \"Import\", and select the <code>VOCABULARY.csv</code> file. Set Header to \"Yes\", Delimiter to \"Tab\", then press OK. The import should proceed successfully. Check for success by querying the table contents.</p> <p>Open the <code>CONCEPT.csv</code>, and remove all lines which do not relate to the given vocabulary - check the <code>vocabulary_id</code> column. Import this file in the same way to the <code>omop.concepts</code> table.</p> <p>Finally, import <code>CONCEPT_RELATIONSHIP.csv</code> to <code>omop.concept_relationship</code> in the same way - the file may require editing,  as it may contain duplicate entries already present in the database, which will cause the import to fail. the only rows required are those which relate to concepts present in <code>CONCEPT.csv</code>. Ideally, an UPSERT script should be used for  this import, though one is not available at the time of writing.</p>"},{"location":"Carrot-Mapper/downloading-rules/","title":"Downloading rules","text":"<p>How to download the automatically generated mapping rules...</p>"},{"location":"Carrot-Mapper/internal-data-structures/","title":"Internal data structures","text":"<p>Overview of the data structures and nomenclature </p>"},{"location":"Carrot-Mapper/internal-data-structures/#django-orm","title":"Django ORM","text":"<p>The Django ORM has access to the models displayed in the diagram below, which also illustrates the  ForeignKey or GenericRelation links between the objects.</p> <p>All models under the heading \"mapping\" are available in Postgresql under the <code>public</code> schema, e.g. <code>public.mapping_omopfield</code>.  Models associated with OMOP are available in Postgresql under the <code>omop</code> schema e.g. <code>omop.person</code>.</p>"},{"location":"Carrot-Mapper/internal-data-structures/#diagram","title":"Diagram","text":""},{"location":"Carrot-Mapper/internal-data-structures/#lookup-table","title":"Lookup Table","text":"Name Description Example(s) Django Model Destination Field Output OMOP column/field name in the CDM <code>person_id</code>, <code>condition_source_value</code> <code>OmopField</code> Destination Table Output OMOP table name in the CDM <code>person</code>, <code>condition_occurrence</code> <code>OmopTable</code> Source Value Input value of given row/cell <code>M</code>, <code>FEMALE</code>, <code>YES</code> <code>ScanReportValue</code> Source Field Input column/field name GOSH::sex, GOSH::ethnicity <code>ScanReportField</code> Source Table Input table name GOSH:: 2_costar. CoConnect_db_serology_ <code>ScanReportTable</code> Source Report Input Scan Report GOSH::CO-STARS <code>ScanReport</code> OMOP Concept OMOP object defining a code and concept_id with a name and domain 8507 (MALE) [Gender] <code>Concept</code> Term Mapping Association between a <code>Concept</code> and a so-called <code>content_object</code> (<code>ScanReportField</code> or <code>ScanReportValue</code>) \"M\" -&gt; 8507 <code>ScanReportConcept</code>"},{"location":"Carrot-Mapper/mapping-rules/","title":"Mapping rules","text":"<p>Mapping Rules determine how to link (and potentially modify) between <code>source_fields</code>  (input data) and <code>destination_fields</code> (output data) when building <code>CDM</code> objects. </p>"},{"location":"Carrot-Mapper/mapping-rules/#prerequisites","title":"Prerequisites","text":"<p>Validation of <code>ScanReportConcept</code> occurs when they themselves are created. Their creation will fail if:</p> <ol> <li>The associated <code>ScanReportTable</code> does not have a <code>person_id</code> marked.  </li> <li>The associated <code>ScanReportTable</code> does not have a <code>date_event</code> marked.</li> </ol> <p>These are requirements for building the mapping rules</p>"},{"location":"Carrot-Mapper/mapping-rules/#building-rules","title":"Building Rules","text":"<p>For each <code>ScanReportConcept</code> that is created, the procedure to build Mapping-Rules proceeds as follows:</p> <ol> <li> <p>The destination_field (<code>OmopField</code> object) of the <code>&lt;domain&gt;_concept_id</code> is found using <code>ScanReportConcept.concept.domain_id</code>. The destination_table is linked to the object.</p> </li> <li> <p>From the source_field or source_value of the <code>ScanReportConcept.content_type</code> the source_table is found.  </p> </li> <li> <p>At least  five  common mapping rules are first created:</p> </li> </ol>"},{"location":"Carrot-Mapper/mapping-rules/#1-person-id","title":"1. Person ID","text":"<p>Is created for the <code>destination_field</code> of the (already found) <code>destination_table</code>:</p> <ul> <li>Every <code>CDM</code> object contains a <code>person_id</code> </li> <li>The rule is built with the current <code>source_table</code> and <code>source_value</code> or <code>source_field</code>, as well as with the <code>concept</code>, linking it to the <code>destination_table</code>.  </li> </ul>"},{"location":"Carrot-Mapper/mapping-rules/#2-date-events","title":"2. Date Events","text":"<p>At least one date based <code>destination_field</code> of the associated <code>destination_table</code> is created:</p> <ul> <li>Every <code>CDM</code> object must contain at least one <code>date_event</code> </li> <li>They are determined by the global variable <code>m_date_field_mapper</code> which is defined in <code>services_rules.py</code>:     <pre><code>m_date_field_mapper = {\n    'person': ['birth_datetime'],\n    'condition_occurrence': ['condition_start_datetime','condition_end_datetime'],\n    'measurement':['measurement_datetime'],\n    'observation':['observation_datetime']\n}\n</code></pre></li> <li>As can be seen, most CDM objects have one date event, however <code>condition_occurrence</code> is an example where two date events are/were needed</li> </ul> <p>Info</p> <p>This was a request from the data-team to also map <code>condition_end_datetime</code>, with the current implementation, the <code>condition_start_datetime == condition_end_datetime</code>.</p> <p>Attention</p> <p>OHDSI/OMOP say the standard is to set <code>condition_end_datetime = condition_start_datetime + 30 days</code> Previously this was handled automatically by the ETL-Tool, and via <code>operations</code>, which is now no-longer used.</p> <ul> <li>The rule is built with the current <code>source_table</code> from the <code>content_object</code> (<code>source_value</code> or <code>source_field</code>), as well as with the <code>concept</code>, linking it to the <code>destination_table</code>.</li> </ul>"},{"location":"Carrot-Mapper/mapping-rules/#3-source-value","title":"3. Source Value","text":"<ul> <li>A rule is created by finding the <code>&lt;domain&gt;_source_value</code> (<code>OmopField</code>) for the current <code>destination_table</code>. As with previous rules, the rule links this with the <code>concept</code> and the <code>source_table</code> and <code>source_field</code>.</li> </ul> <p>Attention</p> <p>This rule could be duplicated for the <code>destination_field</code> called <code>value_as_number</code> (or <code>value_as_float</code>). This appears in <code>measurement</code> and is a clone of <code>source_value</code>, with a different output format (FLOAT instead of CHAR). Formatting of rules is handled by the ETL-Tool, and therefore from the mapping-pipeline point of view, these rules are the same.</p>"},{"location":"Carrot-Mapper/mapping-rules/#4-concept-id","title":"4. Concept ID","text":"<ul> <li>A rule is created by finding the <code>&lt;domain&gt;_concept_id</code> (<code>OmopField</code>) for the current <code>destination_table</code>. As with previous rules, the rule links this with the <code>concept</code> and the <code>source_table</code> and <code>source_field</code>.</li> </ul>"},{"location":"Carrot-Mapper/mapping-rules/#5-source-concept-id","title":"5. Source Concept ID","text":"<ul> <li>A rule is created by finding the <code>&lt;domain&gt;_source_concept_id</code> (<code>OmopField</code>) for the current <code>destination_table</code>. As with previous rules, the rule links this with the <code>concept</code> and the <code>source_table</code> and <code>source_field</code>.</li> </ul> <p>Attention</p> <p>In the current implementation and validation, we force all concept IDs to be Standard. This means that always <code>&lt;domain&gt;_source_concept_id == &lt;domain&gt;_concept_id</code>. We may need to review this for the future and allow a <code>source_concept</code> and aswell as a <code>concept</code> object to be saved to a <code>ScanReportConcept</code>. The logic could be that if a <code>source_concept</code> is not <code>null</code> then <code>&lt;domain&gt;_source_concept_id != &lt;domain&gt;_concept_id</code>, which would not affect exisiting <code>ScanReportConcept</code> objects that are in the current database.</p>"},{"location":"Carrot-Mapper/mapping-rules/#schematic-diagram","title":"Schematic Diagram","text":""},{"location":"Carrot-Mapper/mapping-rules/#downloading-rules","title":"Downloading Rules","text":"<p>In <code>services_rules.py</code> the function <pre><code>def get_mapping_rules_json(qs:QuerySet) -&gt; dict:\n</code></pre> builds a dictionary that is converted to a <code>json</code> format before it is downloaded when the \"Download Mapping Rules\" button is clicked.</p> <p>The function works as follows: Given a query set of all <code>MappingRule</code>s associated to a given <code>ScanReport</code>:</p> <ol> <li>Group them based on the associated ScanReportConcept (object that spawned them)   </li> <li>Create a dictionary to contain each destination_table    </li> <li>Loop over all rules associated to each ScanReportConcept (&gt;=5 rules)   </li> <li>Retrieve the source_table, source_field, destination_field from the <code>MappingRule</code> </li> <li>If the <code>destination_field</code> is a <code>concept_id</code> add \"Term Mapping\" to the output json. Add this as        map or a scalar, depending on if it\u2019s a ScanReportValue or ScanReportField that is to be mapped   </li> </ol>"},{"location":"Carrot-Mapper/migrations/","title":"Migrations","text":"<p>Building and applying database migrations relies upon migration files stored in <code>api/mapping/migrations/</code>.</p> <p>The <code>makemigrations</code> command will compare the contents of <code>models.py</code> with the list of locally held migrations files in  <code>api/mapping/migrations/</code>. It will then create a migration file of the  operations required to convert the effect of the sum of all migrations  files into the same as <code>models.py</code>. Thus it is important that the locally held migrations files reflect the state of the database before  any new migration is applied. There are three possibilities: the local <code>migrations</code> match the current database state; <code>migrations</code> is behind  the current database state (because the previous migration of the  database was performed by another individual); or <code>migrations</code> is ahead  of the current database state (for example, if you have migrated  <code>model-test-db</code> but not yet migrated <code>ccnet-dev-db</code>)</p>"},{"location":"Carrot-Mapper/migrations/#choosing-the-correct-instructions","title":"Choosing the correct instructions","text":"<p>If the state of locally held migration files is behind the current  database state, then follow the instructions in the section  <code>Bringing the migrations/ directory up-to-date with the current database state</code>, before continuing to <code>Creating and applying the new migrations</code>.</p> <p>Alternatively, if the state of <code>migrations</code> matches the database state, then skip  to the section <code>Creating and applying the new migrations</code> below. </p> <p>Finally, if <code>migrations</code> is ahead of the database, then skip  straight to step 5 of <code>Creating and applying the new migrations</code> to apply  the migrations directly.</p>"},{"location":"Carrot-Mapper/migrations/#bringing-the-migrations-directory-up-to-date-with-the-current-database-state","title":"Bringing the <code>migrations/</code> directory up-to-date with the current database state","text":"<p>In the more complex case where the local migrations files do not match the existing state of the database, we must first create a  migration file to bring the local migrations up to date with the current  state of the database. This is achieved by checking out the latest <code>devel</code>  branch (make sure you pull the latest version), and applying the steps  below. </p> <ol> <li> <p>Checkout the <code>devel</code> branch, ensuring that you <code>git pull</code> in order to     have the latest state of <code>devel</code> as is deployed.</p> </li> <li> <p>Build and run the mapping tool Docker locally, where the <code>.env</code> file should     contain the credentials for the relevant DB. Please be     extremely careful when applying changes to the production database,    which will require the credentials for that DB. The relevant     environment variables are <code>COCONNECT_DB_NAME</code>, <code>COCONNECT_DB_USER</code>,     and <code>COCONNECT_DB_PASSWORD</code>. <pre><code>docker build --tag ccom . &amp;&amp; docker run -it --volume $PWD/api:/api --env-file .env -p 8080:8000 ccom\n</code></pre></p> </li> <li> <p>In another terminal tab, run the following to get the name of the     running container and open a shell within it (be careful here if     you have more than one container running in docker! - It just grabs     the first container in the list) <pre><code>docker exec -it `docker container ls | awk 'NR==2 {print $1}'` /bin/bash\n</code></pre></p> </li> <li> <p>Inside the shell, to create the migrations: <pre><code>cd /api\npython manage.py makemigrations mapping\n</code></pre></p> </li> <li> <p>Now fake-apply the migrations files. This will mark the migrations files as having been applied, but not make any changes to the DB. <pre><code>python manage.py migrate --fake mapping\n</code></pre></p> </li> </ol> <p>In this way, the local migration files are brought up-to-date with the database state.</p> <p>Now continue with the below section to apply new migrations.</p>"},{"location":"Carrot-Mapper/migrations/#creating-and-applying-the-new-migrations","title":"Creating and applying the new migrations","text":"<ol> <li> <p>Checkout the feature branch which is ready to be merged.</p> </li> <li> <p>Build and run the mapping tool Docker locally, where the <code>.env</code> file should     contain the credentials for the relevant DB. Please be     extremely careful when applying changes to the production database,    which will require the credentials for that DB. The relevant     environment variables are <code>COCONNECT_DB_NAME</code>, <code>COCONNECT_DB_USER</code>,     and <code>COCONNECT_DB_PASSWORD</code>. <pre><code>docker build --tag ccom . &amp;&amp; docker run -it --volume $PWD/api:/api --env-file .env -p 8080:8000 ccom\n</code></pre></p> </li> <li> <p>In another terminal tab, run the following to get the name of the     running container and open a shell within it (be careful here if     you have more than one container running in docker! - It just grabs     the first container in the list) <pre><code>docker exec -it `docker container ls | awk 'NR==2 {print $1}'` /bin/bash\n</code></pre></p> </li> <li> <p>Inside the shell, to create the migrations: <pre><code>cd /api\npython manage.py makemigrations mapping\n</code></pre></p> </li> <li> <p>Now dry-run the migration. This will show the operations that will be applied. <pre><code>python manage.py migrate --plan mapping\n</code></pre></p> </li> <li> <p>To apply the changes: <pre><code>python manage.py migrate mapping\n</code></pre></p> </li> </ol>"},{"location":"Carrot-Mapper/migrations/#tldr","title":"TL;DR","text":"<p>Steps 1-4 in each section are identical, except that the first uses <code>devel</code> and the second the feature branch that is to be merged. The first section  can be skipped if the state of the <code>migrations/</code> directory is up-to-date  with the status of the database. This will only be the case if you are the person who applied the previous set of migrations.</p> <p>In section 1 we fake applying the migrations to bring the state of the  <code>migrations/</code> directory up-to-date with the database.</p> <p>In contrast, in section 2 we apply the new migrations to bring the database up-to-date with the <code>migrations/</code> directory, which reflects <code>models.py</code>.</p>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/","title":"Projects, Datasets, and Scan Reports","text":"<p>The system makes use of a 3-level hierarchy of objects to organise data and facilitate easy  administration of granular user access controls. These are <code>Project</code>s, <code>Dataset</code>s, and  <code>Scan Report</code>s.</p> <p>A <code>Project</code> is the highest-level object. A single <code>Dataset</code> may live in more than one <code>Project</code>. <code>Project</code>s are the simplest way to administrate access to <code>Datasets</code>, because a user can only have access to a <code>Dataset</code> if they are a member of at least one of the <code>Project</code>s containing  the <code>Dataset</code>.</p> <p>A <code>Dataset</code> is owned by a single <code>Data Partner</code>, and contains a number of <code>Scan Reports</code>. Each <code>Scan Report</code> corresponds to a single scan report file generated by White Rabbit or a  similar tool.</p> <p>A <code>Scan Report</code> lives within a single <code>Dataset</code>.</p> <p>The below diagram is a simple illustration of the relationships. A <code>Scan Report</code> sits within a  single <code>Dataset</code>, which may contain many <code>Scan Reports</code>. <code>Dataset</code>s and <code>Project</code>s exist in a  many-to-many relationship.</p> <p></p>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#access-controls","title":"Access controls","text":"<p>There are 3 levels of protection between a user and data. These are</p> <ol> <li><code>Project</code> membership</li> <li><code>Dataset</code> visibility and roles</li> <li><code>Scan Report</code> visibility and roles</li> </ol> <p>The following are the rules for access. For a user to be able to see a <code>Dataset</code> or <code>Scan Report</code>,</p> <ol> <li>they must be a <code>member</code> of at least one <code>Project</code> in which the <code>Dataset</code>  (or the <code>Scan Report</code>'s parent <code>Dataset</code>) sits.</li> <li>the <code>Dataset</code> must be <code>PUBLIC</code>, or they must be an <code>admin</code>/<code>editor</code>/<code>viewer</code> of the <code>Dataset</code>.</li> <li>To see the contents of the <code>Scan Report</code>, in addition, the <code>Scan Report</code> must be <code>PUBLIC</code>, or they must be an <code>author</code>/<code>editor</code>/<code>viewer</code> of the <code>Scan Report</code>.</li> </ol>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#scan-report-roles","title":"Scan Report roles","text":""},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#viewers","title":"<code>Viewers</code>","text":"<p><code>viewers</code> of a <code>Scan Report</code> can perform read-only actions:</p> <ul> <li>view <code>Scan Report</code> contents</li> <li>view <code>Scan Report</code> details including <code>editors</code>, <code>author</code>, etc</li> <li>regenerate and download mapping rule list</li> </ul>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#editors","title":"<code>Editors</code>","text":"<p><code>editors</code> of a <code>Scan Report</code> can additionally </p> <ul> <li>add/remove concepts</li> <li>set PersonID and DataEvent on tables</li> <li>set <code>is ignore</code> and <code>Pass from source</code> flags, and set description columns, on fields</li> </ul>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#authors","title":"<code>Authors</code>","text":"<p>The <code>author</code> of a <code>Scan Report</code> can additionally</p> <ul> <li>administrate access to the <code>Scan Report</code></li> <li>edit name and visibility</li> <li>change parent <code>Dataset</code></li> </ul>"},{"location":"Carrot-Mapper/projects-datasets-and-scanreports/#permissions-inheritance-between-datasets-and-scan-reports","title":"Permissions inheritance between <code>Dataset</code>s and <code>Scan Report</code>s","text":"<p><code>editors/admins</code> of a <code>Dataset</code> inherit <code>editor/author</code> access respectively to all <code>Scan Report</code>s in the <code>Dataset</code>.</p> <p><code>Dataset</code> access is explained in more detail in this diagram:</p> <p></p> <p><code>Scan Report</code> access is explained in further detail in this diagram:</p> <p></p>"},{"location":"Carrot-Mapper/quickstart-omop/","title":"Quickstart omop","text":""},{"location":"Carrot-Mapper/quickstart-omop/#getting-started","title":"Getting Started:","text":"<p>To run Carrot-Mapper you need a Postgres database running a schema called \"OMOP\", based on the OMOP CDM v5.4.0.</p>"},{"location":"Carrot-Mapper/quickstart-omop/#what-is-the-omop-cdm","title":"What is the OMOP CDM?","text":"<p>For an introduction to the OMOP CDM, see:</p> <ul> <li>OHDSI Website</li> <li>EHDEN Academy OMOP Course</li> </ul>"},{"location":"Carrot-Mapper/quickstart-omop/#run-locally","title":"Run locally","text":"<p>The CDM R package can be used to create the DDL scripts and instantiate the tables.</p> <p>If an OMOP vocabulary has not been supplied to populate the tables, you can download them from Athena.</p> <p>If you would rather not use the R package, here's a manual approach, each database step can also be replicated using a database tool such as PGAdmin or Datagrip.</p> <ol> <li>Download the pregenerated OMOP v5.4.0 DDL's for Postgres.</li> <li>Change the DDLs to use OMOP as a schema, for example using sed: <code>sed -i '' 's/@cdmDatabaseSchema/omop/g' OMOPCDM_postgresql_5.4_ddl.sql</code></li> <li>Copy the DDLSs, and vocabulary files to the postgres container: <code>docker cp &lt;PATH&gt; carrot-mapper-dev-db-1</code></li> <li>Access the database container's shell: <code>docker exec -it carrot-mapper-dev-db-1 bash</code></li> <li>Connect to the Postgres process: <code>psql -U postgres -d postgres</code></li> <li>Create the OMOP schema: <code>CREATE SCHEMA omop;</code></li> <li>Create the tables by running the first DDL: <code>\\i OMOPCDM_postgresql_5.4_ddl.sql</code></li> <li>Load in your vocabulary data to the created tables. For example the concept table: <code>\\copy concept FROM 'concept.csv' DELIMITER E'\\t' CSV HEADER;</code></li> <li> <p>Apply the primary keys, constraints, and indexes by running the remaining DDLs:</p> <ul> <li><code>\\i OMOPCDM_postgresql_5.4_primary_keys.sql</code></li> <li><code>\\i OMOPCDM_postgresql_5.4_constraints.sql</code></li> <li><code>\\i OMOPCDM_postgresql_5.4_indices.sql</code></li> </ul> </li> </ol>"},{"location":"Carrot-Mapper/quickstart-webapp/","title":"Quickstart webapp","text":""},{"location":"Carrot-Mapper/quickstart-webapp/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Python 3.11</li> <li>Poetry</li> <li>Pip</li> <li>Azure Functions Core Tools v4</li> <li>Azure CLI</li> </ul>"},{"location":"Carrot-Mapper/quickstart-webapp/#getting-started","title":"Getting Started","text":"<p>The most direct route to running the application locally is using the Docker quickstart.</p> <p>The repository contains a docker-compose for development, so after you have setup the configuration, just run <code>docker-compose up -d</code> to start the application stack. This runs the database, Azurite emulator, and will build and run the web app container.</p> <p>Docker will mount the <code>api</code> directory to the web app container, so any changes in the Python code will be reflected in the running application.</p> <p>You will need to run some commands using the web app entrypoint, to access the container run: <code>docker exec -it carrot-mapper-web-1 bash</code></p>"},{"location":"Carrot-Mapper/quickstart-webapp/#configuration","title":"Configuration","text":"<p>The application is configured through environment variables, and a <code>local.settings.json</code> file.</p> <p>Rename the existing <code>sample-env.txt</code> to <code>.env</code>, and <code>sample-local.settings.json</code> to <code>local.settings.json</code> to use the default values with the Docker setup.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#database-setup","title":"Database Setup","text":"<p>The application stack interacts with a PostgreSQL Server database, and uses code-first migrations for managing the database schema.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#omop-tables","title":"OMOP Tables","text":"<p>You need a pre-seeded OMOP CDM database, with the schema <code>omop</code>. See OMOP quickstart for how to get this running.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#web-app-tables","title":"Web App Tables","text":"<p>When setting up a new environment, or running a newer version of the codebase if there have been schema changes, you need to run the migrations against your web app database.</p> <p>Inside the web app container <code>api</code> directory, run: <code>python manage.py migrate</code>.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#seed-data","title":"Seed Data","text":"<p>You need to seed the web app database with the OMOP table and field names, inside the web app container <code>api</code> directory run: <code>python manage.py loaddata mapping</code>.  </p> <p>To add a new admin user run: <code>python manage.py createsuperuser</code>.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#azure-functions","title":"Azure Functions","text":"<p>Whilst the rest of the stack runs in containers, the worker functions run directly in your Python environment.</p> <p>To create the storage containers and queues, use the Azure CLI:</p> <ul> <li><code>az storage container create -n scan-reports --connection-string &lt;CONNECTIONSTRING&gt;</code></li> <li><code>az storage container create -m data-dictionaries --connection-string &lt;CONNECTIONSTRING&gt;</code></li> <li><code>az storage queue create -n nlpqueue-local --connection-string &lt;CONNECTIONSTRING&gt;</code></li> <li><code>az storage queue create -n uploadreports-local --connection-string &lt;CONNECTIONSTRING&gt;</code></li> <li><code>az storage queue create -n rules-local --connection-string &lt;CONNECTIONSTRING&gt;</code></li> </ul> <p>To run the functions, in the project root:</p> <ol> <li>In the web app Django <code>http://localhost:8000/admin</code>, generate a new <code>auth token</code> for the admin user.</li> <li>Add the token to <code>local.settings.json</code> : <code>AZ_FUNCTION_KEY</code></li> <li>Install the dependencies in <code>app/workers</code>: <code>poetry install</code></li> <li>Run the functions: <code>poetry run func start</code></li> </ol> <p>You should now be setup to run the user quickstart.</p>"},{"location":"Carrot-Mapper/quickstart-webapp/#python-environment","title":"Python Environment","text":"<p>You can also run the web app directly in a Python environment instead of the Docker image.</p> <p>Prequisites:</p> <ul> <li>A separate Poetry environment for the web app.</li> <li>graphviz package installed.</li> <li> <p>Node v12.18.3</p> </li> <li> <p>Change the environment configuration to point to the running docker containers, for example <code>localhost</code> instead of <code>azurite</code>.</p> </li> <li>Inside the <code>app/react-client-app</code> directory, install the npm dependencies: <code>npm i</code></li> <li>Change the <code>snowpack.config.js</code> to use a relative file path: <pre><code>{\n  buildOptions: {\n      out: '../api/static/javascript/react',\n    }\n}\n</code></pre></li> <li>Build the react app: <code>npm run build</code>.</li> <li>Inside the <code>app/api</code> directory, install the Python dependencies: <code>poetry install</code>.</li> <li>Collect the Django static files: <code>poetry run python manage.py collectstatic</code>.</li> <li>Run the app: <code>poetry run python manage.py runserver</code>.</li> </ul> <p>If you're using VSCode, you can use the Workspaces to manage your Python virtual environments, and the debugging tool to run the web app and functions.</p>"},{"location":"Carrot-Mapper/quickstart/","title":"Quickstart Guide","text":"<p>Warning</p> <p>This page is a work in progress, and should not be relied upon.</p> <p>This Quickstart guide will guide you through the usage of Carrot-Mapper. Specifically, it covers:</p> <ol> <li>Gaining access</li> <li>Uploading a new Scan Report</li> <li>Navigating within a Scan Report</li> <li>Creating Mapping Rules</li> <li>Downloading Mapping Rules</li> <li>Understanding Datasets and Projects</li> <li>The Home page</li> <li>Administrating Datasets and Scan Reports</li> <li>Managing Project access</li> <li>Analysing Mapping Rules</li> <li>Getting help</li> </ol> <p>This Quickstart guide is intended as a brief introduction to each of the areas.  Users requiring more details should follow the links or search the documentation for their specific query. Developers should use the Developer Guide for technical documentation.</p>"},{"location":"Carrot-Mapper/quickstart/#gaining-access","title":"Gaining Access","text":"<p>The deployed Carrot-Mapper is available at carrot.ac.uk.</p> <p>A user account is required to access the site; one can be requested from the site administrator. </p> <p>Users must be a member of at least one Project. Project membership is currently handled by the site administrator.</p> <p>Once logged in, the user is presented with the Home page.  This is a simple dashboard displaying statistics about the data the user can access.</p>"},{"location":"Carrot-Mapper/quickstart/#upload-a-first-scan-report","title":"Upload a first Scan Report","text":"<p>To begin, navigate to \"Scan Reports &gt; Upload New Scan Report\" using the navbar at the top of the screen.</p> <p>TODO: Provide a sample ScanReport file and Data Dictionary file.</p> <p>Fill in the details as required, and upload a valid Scan Report File.  You may optionally provide a Data Dictionary file. If an error message occurs, fix the error in the Scan Report File (or Data Dictionary file if provided) and try again to upload.</p> <p>Caution</p> <p>Not all possible errors in the files can be checked before upload. If you are directed to the Scan Reports page after upload, this indicates that all pre-checks have completed. However, other errors can be checked only during the processing of the file after upload. If these occur, the <code>Status</code> of the Scan Report may change to <code>Upload Failed</code>, or it may stay as <code>Upload in Progress</code> forever (over 30 minutes indicates an error). If this occurs, please check your files yourself for possible errors, and if that fails then contact the system administrator to check the logs.</p> <p>Once a valid Scan Report File is uploaded, the user will be redirected to the Scan Report list page. This should show the newly created Scan Report at the top of the table.</p> <p>After upload, an automated task is launched to ingest the contents of the file and perform a number of processing steps.  This process can take several minutes for large Scan Report Files. While this process is running, the \"Status\" visible to the right of the row will be \"Upload in Progress\". Once the process completes, the status will change (once the page is reloaded) to \"Upload Complete\". In the case of an unrecoverable error in processing, the status will be set to \"Upload Failed\".</p> <p>Caution</p> <p>Please note that sometimes errors in processing do not result in the status being set to \"Upload Failed\", with the status incorrectly remaining as \"Upload in Progress\". If this status remains for more than 30 minutes then you can be sure that the process has failed. Please report this to the development team with a copy of the Scan Report File being used, to help us improve the checks run.</p> <p>Wait for approximately one minute to ensure the example Scan Report has been fully processed. Refresh the Scan Reports page to check that the Status of your Scan Report has been set to \"Upload Complete\" before continuing.</p>"},{"location":"Carrot-Mapper/quickstart/#navigation","title":"Navigation","text":"<p>After upload, the user is directed to the Scan Reports page before the new Scan Report has been fully processed.  While the Scan Report continues to be processed, the contents visible on Carrot-Mapper will be populated over time.</p> <p>Click on the name of the Scan Report to navigate to the list of Tables (\"Tables List\") within the Scan Report. As Tables are processed, the user can click on the name of each Table to navigate to the list of Fields (\"Field List\") within that Table. Similarly, clicking on a Field name in a Table will display the list of Values (\"Value List\") within that Field. In this way, the structure of the Scan Report can be navigated.</p> <p>Sometimes a list is too large to be displayed all at once. In this case, the list will be displayed over a number of pages. Use the grey buttons at the top of the page to navigate through the pages.</p> <p>TODO: note somewhere how to navigate pagination URLs.</p> <p>Use the breadcrumbs at the top of the page to quickly navigate back up the Scan Report.</p>"},{"location":"Carrot-Mapper/quickstart/#creating-mapping-rules","title":"Creating Mapping Rules","text":"<p>The example Scan Report and Data Dictionary will have added some OMOP Concepts to fields and values in the Scan Report. For example, TODO(field) shows the OMOP code TODO, and TODO(value) shows the OMOP code TODO.</p> <p>You can hover over the Concepts to show more details about them (currently this just shows further details of how the Concept was added - in this case, they are added by vocab as defined in the Data Dictionary we supplied)</p>"},{"location":"Carrot-Mapper/quickstart/#setting-person-id-and-date-event-for-each-table","title":"Setting Person ID and Date Event for each table","text":"<p>The first task of creating Mapping Rules for a Scan Report is to set the Person ID and Date Event fields of each table.</p> <p>Navigate to the Tables view of your Scan Report by clicking on its name on the Scan Report list page. This page will show the list of all the tables in the Scan Report, while the <code>PERSON ID</code> and <code>EVENT DATE</code> columns are empty.  On the right-hand side, press the <code>Edit Table</code> link on the first table (<code>Demographics.csv</code>). In the first drop-down (Person ID), select <code>PersonID</code> which is one of the fields as reported by the Scan Report file. In the second drop-down (Date Event), select <code>Date_of_Birth</code>. Then press the <code>Update Demographics.csv</code> button.  You will be taken back to the Tables list, and the <code>PERSON ID</code> and <code>EVENT DATE</code> columns are now populated in the <code>Demographics.csv</code> row.</p> <p>Now do the same for <code>Covid.csv</code>, selecting <code>PersonID</code> and <code>Date</code>.</p> <p>The Tables list will now show both tables have filled <code>PERSON ID</code> and <code>EVENT DATE</code> columns. And we are ready to proceed with generating Mapping Rules</p>"},{"location":"Carrot-Mapper/quickstart/#generate-initial-mapping-rules","title":"Generate Initial Mapping Rules","text":"<p>Navigate to the Mapping Rules page associated to your Scan Report.  This can be either (a) from the Scan Reports page - press the <code>Rules</code> button; or (b) from any of the Tables, Fields or Values pages - press the <code>Go to Rules</code> button near the top of the page.</p> <p>Now that <code>Person ID</code> and <code>Date Event</code> are set for each of the tables, the Mapping Rules associated to those Concepts have been generated.</p> <p>After a few moments, the table will be populated by the Mapping Rules associated to the Concepts already mapped (in this example, there should be 16 Mapping Rules generated).</p> <p>Note</p> <p>If you don't see any Mapping Rules appear, this is usually an indication that you have not set the <code>Person ID</code> and <code>Date Event</code> on the tables.</p> <p>Note that there are 5 or 6 rows in the table associated to each Concept.  In the <code>Rule ID</code> column in the table, there are 5 or 6 with a given ID.  This fully describes how to map the source term to the OMOP standard.</p> <p>A Summary view is available using the <code>Show Summary view</code> button at the top of the page.  This shows only one of the Rules associated to each source-concept pair, which can be useful for reviewing.</p> <p>Note</p> <p>The Summary view only shows Rules that are shown on the current pagination view.  Often this means only 5 or 6 Rules are visible in Summary View at any one time. By editing the URL so that <code>page_size=300</code>, for example, one can increase the number of Rules visible in Summary View at one time.</p> <p>All the Mapping Rules associated to the Concepts, which were automatically added to our Scan Report by using a Data Dictionary file, are now available.</p>"},{"location":"Carrot-Mapper/quickstart/#add-further-concepts","title":"Add further Concepts","text":"<p>Further mappings can be added manually.</p> <p>Navigate to the <code>Demographics.csv</code> table, and the <code>Sex</code> field.  There will be a table with 3 rows: <code>F</code>, <code>M</code> and an empty row. Note that, thanks to our Data Dictionary file, the <code>Value description</code> column has been populated with helpful information. It indicates that <code>F</code> means \"Female\", and <code>M</code> \"Male\". To add the correct OMOP Concepts, click in the text box to the left of the <code>Add</code> button on the right-hand side of the screen, in the row associated with <code>F</code>. Type <code>8532</code> and press <code>Add</code>. After a moment, a green <code>Success: Mapping Rules created</code> banner appears, along with a blue Concept tag inside the table. Similarly, add <code>8507</code> for the <code>M</code> row.</p> <p>Now that these Concepts have been added, press the <code>Go to Rules</code> button. Note that new rows have been added associated to the <code>F</code> and <code>M</code> values. These were generated at the time of adding the Concepts on the previous page.</p> <p>You can also try removing a Concept by pressing the grey <code>x</code> on the right of any Concept tag, then return to the Rules page to see that the associated Mapping Rules have been deleted.</p>"},{"location":"Carrot-Mapper/quickstart/#downloading-mapping-rules","title":"Downloading Mapping Rules","text":"<p>The final task in the simplest end-to-end process in Carrot-Mapper is to download the Mapping Rules in a form that can be used as input to Carrot-CDM (which will perform the transformations on the data).</p> <p>Navigate to the Mapping Rules page, and press the <code>Download Mapping JSON</code> button. This will download to your machine a <code>.json</code> file containing the Mapping Rules.</p> <p>To view a more human-readable version, press the <code>Download Mapping CSV</code> for a <code>.csv</code> file instead. This however cannot be used as input to Carrot-CDM.</p> <p>Finally, a diagram of the Mapping Rules can be viewed and downloaded using the yellow <code>View Map Diagram</code> and red <code>Download Map Diagram</code> buttons.</p>"},{"location":"Carrot-Mapper/uploading-scan-report/","title":"Overview of process","text":"<p>A Scan Report is the basic unit representing a single data source.  In order to create a new Scan Report in the Carrot-Mapper system,  one must upload a Scan Report Form to the system. Once uploaded,  an ingestion process reads this file and populates the system with its contents. The documentation below explains this process and the options available to users.</p>"},{"location":"Carrot-Mapper/uploading-scan-report/#the-scan-report-upload-form","title":"The Scan Report Upload form","text":"<p>To upload a new Scan Report Form, navigate to the Scan Reports  page and then click \"New Scan Report\". This opens a new web form.</p> <p>Firstly select an existing Data Partner from the dropdown list. This will populate the Dataset field with existing Datasets linked to this Data Partner. These are grouped by Project that  the user has access to. Select the correct Dataset that the new Scan Report should belong to.</p> <p>Creating a new Dataset</p> <p>If there is no appropriate Dataset already existing, click the  Add New button to the right of the Dataset dropdown. This opens a panel.</p> <p>Enter a name for the Dataset (which must be globally unique - if  you receive an error then it is likely that this Dataset name is  already taken), and select at least one Project to which it will  be associated. Then select the visibility, editor and admin options as appropriate (see  Projects, Datasets, Scan Reports, and Permissions   for an explanation of these. The Dataset creator is automatically  assigned as an editor.</p> <p>Finally, click the \"Add new Dataset\" button  to create the new Dataset.</p>"},{"location":"Carrot-Mapper/uploading-scan-report/#setting-visibilityviewers-and-editors-permissions","title":"Setting visibility/viewers and editors - permissions","text":"<p>Once a Dataset has been selected, select the appropriate  permissions for the Scan Report. The user performing the upload is set as the Scan Report's Author, and has administrator rights  to the Scan Report. Additional users can be set as editors, and, if the Scan Report is set as RESTRICTED, viewers. Note that  if a user has a given level of access to the Dataset, then they  inherit that same access to the Scan Report automatically.</p>"},{"location":"Carrot-Mapper/uploading-scan-report/#upload-form","title":"Upload Form","text":"<p>Choose a Scan Report in the \"WhiteRabbit ScanReport\" field, and  optionally choose a dictionary file in the \"Data Dictionary\" field. The formats of these two files are as described below. Then click  the \"Submit\" button to begin the upload process. This will launch an instance of the ProcessQueue.</p>"},{"location":"Carrot-Mapper/uploading-scan-report/#the-scan-report-file-format","title":"The Scan Report file format","text":"<p>The Scan Report File should be in the format as output by the  White Rabbit tool. Broadly, this is an Excel .xlsx file with the  following non-exhaustive format. At upload, a series of checks are run upon the file to ensure it meets certain standards before it is further processed, and an error message is presented if these  checks fail.</p> <ol> <li><code>.xlsx</code> file format</li> <li>The first sheet is 'Field Overview' where:<ol> <li>There is no empty row or column at the start of the sheet.</li> <li>The first 10 column headers are:   \"Table\",    \"Field\",   \"Description\",   \"Type\",   \"Max length\",   \"N rows\",   \"N rows checked\",   \"Fraction empty\",   \"N unique values\",   \"Fraction unique\"</li> <li>A contiguous group of rows is associated to each table in the    dataset. Each group should be separated from the next by a    blank row. There is no blank between the header row and the    first row of the first group. Two contiguous blank rows will be    detected as the end of the sheet, and no further entries will be    added.</li> <li>Within each group, a single row corresponds to a single field    in the associated table.</li> <li>Each table name in the 'Field Overview' table must be associated    to a single sheet with identical name. This imposes a 30-character    limit on the table names including <code>.csv</code> ending.</li> </ol> </li> <li>2 additional sheets named 'Table Overview' and '_' must also be  present, although they are never used.</li> <li>In each of the sheets associated to a table from the 'Field Overview' tab:</li> <li>Each field in the table is represented by 2 columns in the sheet.    The first column has in its header row the name of the field, while    the second column is always headed 'Frequency'. Thus a typical sheet    might be headed with <code>\"Person\", \"Frequency\", \"DOB\", \"Frequency\",     \"Observation\", \"Frequency\"</code> etc.</li> <li>The fields in the sheet must match exactly with the field names    provided for this table in the 'Field Overview' sheet.</li> <li>Each pair of columns is populated by all values that appear in the     given field, with their corresponding frequency. Usually, this     frequency should be no less than 5 due to applying count thresholding.</li> <li>Pairs of columns associated to a given field can differ in length    from pairs of columns associated to other fields.</li> <li>An entry in a given pair of columns which is blank in both the field    name column and Frequency column will count as the end of the column.</li> <li>If all entries in a given field should be redacted, it can be helpful    to add a single entry in the field, wth value \"List truncated...\" to     show the intent.</li> <li>There is no empty row or column at the start of the sheet.</li> </ol> <p>These stipulations are summarised in the following two annotated screenshots.</p> <p>The 'Field Overview' sheet: </p> <p>Single sheet associated to a single table: </p>"},{"location":"Carrot-Mapper/uploading-scan-report/#the-data-dictionary-file-format","title":"The Data Dictionary file format","text":"<p>The optionally-supplied Data Dictionary file must be in .csv format, and  with the format as defined at  the CO-CONNECT data standards page </p>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/","title":"Pseudonymisation","text":"<p>A separate package, to help with pseudonymisation of columns in data files by passing a salt. co-connect-pseudonymise is available to be installed via pypi, or via the source code on GitHub.</p> <p>This is provided as a separate package for those who no not wish (e.g. for security reasons) or need to install the full package (co-connect-tools).</p> <p>Info</p> <p>co-connect-pseudonymise will be installed if the packge co-connect-tools is installed, as it is a dependancy of the latter. </p> <p>co-connect-pseudonymise contains:   </p> <ul> <li>a simple CLI (command line interface) to pseudonymise <code>csv</code> files   </li> <li>a python workbook showing how pseudonymisation can be performed manually   </li> <li>additional example code for C# and Java    </li> </ul>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#installation-via-pip","title":"Installation (via pip)","text":"<p>To install the package, it's recommended to use a python virtual environment, using <code>python &gt;= 3.6.8</code>. It's best to install via <code>pip</code> with a connection to the internet.</p>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#setup-a-virtual-environment","title":"Setup a virtual environment","text":"<p>Navigate to a clean or working directory and setup a virtual environment:</p> <pre><code>python3 -m venv  .\nsource bin/activate\n</code></pre>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#upgrade-pip","title":"Upgrade Pip","text":"<p>You'll first need to upgrade pip to make sure the latest packages can be found: <pre><code>pip install pip --upgrade\n</code></pre></p>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#install-the-package","title":"Install the package","text":"<pre><code>pip install co-connect-pseudonymise\n</code></pre>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#check-the-package","title":"Check the package","text":"<p><pre><code>pseudonymise --help\n</code></pre> Print the message: <pre><code>Usage: pseudonymise [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  csv  Command to pseudonymise csv files, given a salt, the name of the...\n</code></pre></p>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#run-for-pseudonymisation-of-a-csv-file","title":"Run for pseudonymisation of a CSV file","text":""},{"location":"Carrot-Pseudonymise/Pseudonymisation/#display-help","title":"Display help","text":"<p>To display the help message to see what options are available, run: <pre><code>pseudonymise csv --help \n</code></pre> Which shows: <pre><code>Usage: pseudonymise csv [OPTIONS] INPUT\n\n  Command to pseudonymise csv files, given a salt, the name of the columns to\n  pseudonymise and the input file.\n\nOptions:\n  -s, --salt TEXT           salt hash  [required]\n  -c, --column, --id TEXT   name of the identifier columns  [required]\n  -o, --output-folder TEXT  path of the output folder  [required]\n  --chunksize INTEGER       set the chunksize when loading data, useful for\n                            large files\n  --help                    Show this message and exit.\n</code></pre></p>"},{"location":"Carrot-Pseudonymise/Pseudonymisation/#execute","title":"Execute","text":"<p>Executing a command to pseudonymise the data in the column <code>PersonID</code> of the file <code>data/Demographics.csv</code> using the salt <code>beb2839bd78</code> and outputing the file to the folder <code>pseudonymised_data/</code>: <pre><code>pseudonymise csv --salt beb2839bd78 --column PersonID  data/Demographics.csv -o pseudonymised_data/\n</code></pre></p> <p>Outputs the following: <pre><code>2022-01-10 09:56:11.155 | INFO     | cli.cli:csv:16 - Working on file Demographics.csv, pseudonymising columns '['PersonID']' with salt 'beb2839bd78'\n2022-01-10 09:56:11.157 | INFO     | cli.cli:csv:22 - Saving new file to pseudonymised_data/Demographics.csv\n2022-01-10 09:56:11.173 | DEBUG    | cli.cli:csv:32 - 0      16dc368a89b428b2485484313ba67a3912ca03f2b2b424...\n1      37834f2f25762f23e1f74a531cbe445db73d6765ebe608...\n2      454f63ac30c8322997ef025edff6abd23e0dbe7b8a3d51...\n3      5ef6fdf32513aa7cd11f72beccf132b9224d33f271471f...\n4      1253e9373e781b7500266caa55150e08e210bc8cd8cc70...\n                             ...                        \n995    8352dd9eb8b64669e0a8347fd37ae6e5cd67c817f2b4b1...\n996    235aa062e6372588dbae00552abf36b8ff9c315e3da56c...\n997    4aec429ac0bfafdbb8dab14f41d1b7a98dacf1ce3478b7...\n998    330e14d4ae80612334d94c488d29eb469626b476864abd...\n999    ab9828ca390581b72629069049793ba3c99bb8e5e9e7b9...\nName: PersonID, Length: 1000, dtype: object\n2022-01-10 09:56:11.183 | INFO     | cli.cli:csv:52 - Done with pseudonymised_data/Demographics.csv!\n</code></pre></p>"},{"location":"Testing/django-testing-writing/","title":"Django testing writing","text":""},{"location":"Testing/django-testing-writing/#creating-a-unit-test","title":"Creating a Unit Test","text":"<ol> <li>Create a file called <code>test_&lt;something&gt;.py</code>. The file name must begin with <code>test_</code>.</li> <li>Inside <code>test_&lt;something&gt;.py</code>, create a <code>class</code> which inherits from <code>django.test.TestCase</code> <pre><code>from django.test import TestCase\n\n\nclass TestSomething(TestCase):\n    pass\n</code></pre></li> <li>(Optional) If you have data that can be reused in multiple tests, you can write a <code>setUp</code> method that will be ran before each test in the <code>class</code>. Make sure to prepend <code>self.</code> to your variable. <pre><code>from django.test import TestCase\n\n\nclass TestSomething(TestCase):\n    def setUp(self):\n        self.var1 = 1\n        self.var2 = 2\n        ...\n        # Add more reusable variables\n</code></pre></li> <li>Write a method to test a functionality of your code. The name must start with <code>test_</code>. <pre><code>from django.test import TestCase\nfrom .my_module import add\n\n\nclass TestSomething(TestCase):\n    def setUp(self):\n        self.var1 = 1\n        self.var2 = 2\n        ...\n        # Add more reusable variables\n\n    def test_add(self):\n        expected_answer = 3\n        self.assertEqual(\n            add(self.var1, self.var2),\n            expected_answer\n        )\n</code></pre></li> </ol>"},{"location":"Testing/django-testing-writing/#useful-information","title":"Useful information","text":"<p><code>django.test.TestCase</code> is a subclass of <code>unittest.TestCase</code> from the Python standard library. As such, it has all the <code>assert*</code> functions available in <code>unittest.TestCase</code>. See here</p> <p>For more general information on <code>unittest</code>, see here.</p> <p>For information about testing specific to Django, see here.</p> <p>For information about testing specific to Django REST Framework, see here.</p>"},{"location":"Testing/django-testing/","title":"Django testing","text":"<p>To run unit tests locally, we can do the following:</p> <ol> <li> <p>Build and run the mapping tool locally, where the <code>.env</code> file should     contain the credentials for the dev DB. <pre><code>docker build --tag ccom . &amp;&amp; docker run -it --volume $PWD/api:/api \n   --env-file .env -p 8080:8000 ccom\n</code></pre></p> </li> <li> <p>In another terminal tab, run the following the get the name of the     running container and open a shell within it (be careful here if     you have more than one container running in docker! - It just grabs     the first container in the list) <pre><code>docker exec -it `docker container ls | awk 'NR==2 {print $1}'` /bin/bash\n</code></pre></p> </li> <li> <p>Then run the following commands in the container: <pre><code>cd ../api\npython manage.py test mapping\n</code></pre></p> </li> </ol>"}]}